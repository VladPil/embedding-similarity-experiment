"""
Tension Analysis Strategy.
Analyzes tension timeline using selective chunk processing.
"""

import json
import re
from typing import Dict, Any, List
from loguru import logger

from server.core.analysis.base import IAnalysisStrategy, AnalysisContext, AnalysisType
from server.core.analysis.prompt_templates import PromptTemplates
from server.core.analysis.chunk_indexer import ChunkIndexer
from server.core.analysis.llm_manager import get_llm_manager


class TensionAnalysisStrategy(IAnalysisStrategy):
    """
    Strategy for tension analysis.

    - Finds high-tension chunks using indexer
    - LLM analyzes only tension peaks
    - Returns tension timeline with descriptions
    """

    def __init__(self, tension_threshold: float = 6.0):
        """
        Initialize tension analysis strategy.

        Args:
            tension_threshold: Minimum tension score to analyze (0-10)
        """
        self.prompt_templates = PromptTemplates()
        self.indexer = ChunkIndexer()
        self.tension_threshold = tension_threshold

    def get_type(self) -> AnalysisType:
        """Get analysis type identifier."""
        return AnalysisType.TENSION

    def requires_llm(self) -> bool:
        """Tension analysis requires LLM for peak descriptions."""
        return True

    def requires_embeddings(self) -> bool:
        """Tension analysis can use embeddings but doesn't require them."""
        return False

    def get_estimated_time(self, chunk_count: int) -> float:
        """Estimate time: ~1.5 sec per tension peak."""
        # Approximately 10-15% of chunks are high-tension
        peak_count = int(chunk_count * 0.15)
        return max(peak_count * 1.5, 10.0)  # Min 10 seconds

    async def analyze(self, context: AnalysisContext) -> Dict[str, Any]:
        """
        Execute tension analysis.

        Args:
            context: Analysis context with chunks

        Returns:
            {
                "average_tension": float,
                "timeline": List[TensionPoint],
                "peaks": List[int],
                "analysis_coverage": float
            }
        """
        try:
            logger.info("Starting tension analysis...")

            # Build tension index
            tension_index = self.indexer.build_tension_index(
                context.chunks,
                threshold=self.tension_threshold
            )

            logger.info(
                f"Tension index built: {len(tension_index.chunk_indices)} "
                f"high-tension chunks ({tension_index.coverage:.1%})"
            )

            # Calculate average tension from all chunks
            all_scores = [
                self.indexer._calculate_tension_from_keywords(chunk.text)
                for chunk in context.chunks
            ]
            average_tension = sum(all_scores) / len(all_scores) if all_scores else 0.0

            # Get high-tension chunks
            tension_chunks = self.indexer.get_chunk_subset(
                context.chunks,
                'tension'
            )

            if not tension_chunks:
                logger.info("No high-tension chunks found")
                return {
                    "average_tension": average_tension,
                    "timeline": [],
                    "peaks": [],
                    "analysis_coverage": 0.0
                }

            # Analyze tension peaks with LLM
            llm_manager = await get_llm_manager()
            tension_points = []

            # Limit to top 15 peaks
            max_peaks = min(len(tension_chunks), 15)

            for i, chunk in enumerate(tension_chunks[:max_peaks]):
                try:
                    # Create prompt
                    prompt = self.prompt_templates.format_tension_prompt(chunk.text)

                    # LLM analysis
                    result = await llm_manager.execute_task(
                        task_type='custom',
                        text1=prompt,
                        text2=None
                    )

                    # Parse tension data
                    tension_data = self._parse_tension_response(result)

                    if tension_data:
                        # Get score from index
                        chunk_idx_in_index = tension_index.chunk_indices.index(chunk.index)
                        score = tension_index.scores[chunk_idx_in_index]

                        tension_point = {
                            "position": chunk.position_ratio,
                            "score": score,
                            "source": tension_data.get('source', 'unknown'),
                            "description": tension_data.get('description', ''),
                            "excerpt": tension_data.get('excerpt', chunk.text[:200])
                        }

                        tension_points.append(tension_point)

                except Exception as e:
                    logger.warning(f"Failed to analyze tension chunk {chunk.index}: {e}")
                    continue

            # Sort by position
            tension_points.sort(key=lambda x: x['position'])

            logger.info(f"Tension analysis complete: {len(tension_points)} peaks analyzed")

            return {
                "average_tension": round(average_tension, 2),
                "timeline": tension_points,
                "peaks": [p['position'] for p in tension_points],
                "peak_count": len(tension_points),
                "analysis_coverage": tension_index.coverage
            }

        except Exception as e:
            logger.error(f"Tension analysis failed: {e}")
            return {
                "average_tension": 0.0,
                "timeline": [],
                "peaks": [],
                "peak_count": 0,
                "error": str(e)
            }

    def _parse_tension_response(self, llm_response: Dict) -> Dict[str, Any]:
        """Parse LLM response for tension data."""
        try:
            response_text = self._extract_response_text(llm_response)

            # Extract JSON
            json_match = re.search(r'\{[^{}]*\}', response_text, re.DOTALL)

            if json_match:
                parsed = json.loads(json_match.group(0))
                return parsed

            return {}

        except Exception as e:
            logger.warning(f"Failed to parse tension response: {e}")
            return {}

    def _extract_response_text(self, response: Any) -> str:
        """Extract text from LLM response."""
        if isinstance(response, dict):
            for key in ['response', 'text', 'generated_text', 'content']:
                if key in response:
                    return response[key]
            return str(response)
        return str(response)

    def interpret_results(self, results: Dict[str, Any]) -> str:
        """
        Interpret tension analysis results for UI display.

        Args:
            results: Tension analysis results

        Returns:
            Human-readable interpretation
        """
        if 'error' in results:
            return f"‚ùå –û—à–∏–±–∫–∞ –∞–Ω–∞–ª–∏–∑–∞ –Ω–∞–ø—Ä—è–∂–µ–Ω–∏—è: {results['error']}"

        avg_tension = results.get('average_tension', 0)
        timeline = results.get('timeline', [])
        peaks = results.get('peaks', [])
        peak_count = results.get('peak_count', 0)

        # Tension emoji based on average
        if avg_tension > 7:
            tension_emoji = 'üî•'
            tension_level = '–æ—á–µ–Ω—å –≤—ã—Å–æ–∫–æ–µ'
        elif avg_tension > 5:
            tension_emoji = '‚ö°'
            tension_level = '–≤—ã—Å–æ–∫–æ–µ'
        elif avg_tension > 3:
            tension_emoji = 'üìä'
            tension_level = '—É–º–µ—Ä–µ–Ω–Ω–æ–µ'
        else:
            tension_emoji = 'üåä'
            tension_level = '–Ω–∏–∑–∫–æ–µ'

        # Build interpretation
        lines = [
            f"üé≠ **–ê–Ω–∞–ª–∏–∑ –Ω–∞–ø—Ä—è–∂–µ–Ω–∏—è –≤ –ø—Ä–æ–∏–∑–≤–µ–¥–µ–Ω–∏–∏**\n",
            f"{tension_emoji} **–°—Ä–µ–¥–Ω–µ–µ –Ω–∞–ø—Ä—è–∂–µ–Ω–∏–µ**: {tension_level} ({avg_tension:.1f}/10)",
            f"üìà **–û–±–Ω–∞—Ä—É–∂–µ–Ω–æ –ø–∏–∫–æ–≤ –Ω–∞–ø—Ä—è–∂–µ–Ω–∏—è**: {peak_count}\n"
        ]

        # Interpretation based on tension level
        if avg_tension > 7:
            interpretation = (
                "–í—ã—Å–æ–∫–æ–Ω–∞–ø—Ä—è–∂–µ–Ω–Ω–æ–µ –ø–æ–≤–µ—Å—Ç–≤–æ–≤–∞–Ω–∏–µ. –¢–µ–∫—Å—Ç –Ω–∞—Å—ã—â–µ–Ω –∫–æ–Ω—Ñ–ª–∏–∫—Ç–∞–º–∏, "
                "–æ–ø–∞—Å–Ω–æ—Å—Ç—è–º–∏ –∏ —ç–º–æ—Ü–∏–æ–Ω–∞–ª—å–Ω—ã–º–∏ –º–æ–º–µ–Ω—Ç–∞–º–∏. –•–∞—Ä–∞–∫—Ç–µ—Ä–Ω–æ –¥–ª—è —Ç—Ä–∏–ª–ª–µ—Ä–æ–≤ –∏ —ç–∫—à–Ω-–∂–∞–Ω—Ä–æ–≤."
            )
        elif avg_tension > 5:
            interpretation = (
                "–£–º–µ—Ä–µ–Ω–Ω–æ –Ω–∞–ø—Ä—è–∂–µ–Ω–Ω–æ–µ –ø–æ–≤–µ—Å—Ç–≤–æ–≤–∞–Ω–∏–µ. –•–æ—Ä–æ—à–∏–π –±–∞–ª–∞–Ω—Å —Å–ø–æ–∫–æ–π–Ω—ã—Ö "
                "–∏ –¥—Ä–∞–º–∞—Ç–∏—á–Ω—ã—Ö –º–æ–º–µ–Ω—Ç–æ–≤, –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞—é—â–∏–π –∏–Ω—Ç–µ—Ä–µ—Å —á–∏—Ç–∞—Ç–µ–ª—è."
            )
        elif avg_tension > 3:
            interpretation = (
                "–°–ø–æ–∫–æ–π–Ω–æ–µ –ø–æ–≤–µ—Å—Ç–≤–æ–≤–∞–Ω–∏–µ —Å —Ä–µ–¥–∫–∏–º–∏ –≤—Å–ø–ª–µ—Å–∫–∞–º–∏ –Ω–∞–ø—Ä—è–∂–µ–Ω–∏—è. "
                "–§–æ–∫—É—Å –Ω–∞ –∞—Ç–º–æ—Å—Ñ–µ—Ä–µ, —Ä–∞–∑–º—ã—à–ª–µ–Ω–∏—è—Ö –∏ —Ä–∞–∑–≤–∏—Ç–∏–∏ –ø–µ—Ä—Å–æ–Ω–∞–∂–µ–π."
            )
        else:
            interpretation = (
                "–û—á–µ–Ω—å —Å–ø–æ–∫–æ–π–Ω–æ–µ –ø–æ–≤–µ—Å—Ç–≤–æ–≤–∞–Ω–∏–µ. –ú–∏–Ω–∏–º—É–º –∫–æ–Ω—Ñ–ª–∏–∫—Ç–æ–≤ –∏ –¥—Ä–∞–º–∞—Ç–∏—á–µ—Å–∫–∏—Ö —Å–∏—Ç—É–∞—Ü–∏–π. "
                "–•–∞—Ä–∞–∫—Ç–µ—Ä–Ω–æ –¥–ª—è –º–µ–¥–∏—Ç–∞—Ç–∏–≤–Ω–æ–π, —Ñ–∏–ª–æ—Å–æ—Ñ—Å–∫–æ–π –ø—Ä–æ–∑—ã."
            )

        lines.append(f"üí° **–ò–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏—è**: {interpretation}\n")

        # Timeline analysis
        if timeline:
            lines.append(f"üìä **–î–∏–Ω–∞–º–∏–∫–∞ –Ω–∞–ø—Ä—è–∂–µ–Ω–∏—è –ø–æ —Ö–æ–¥—É –ø–æ–≤–µ—Å—Ç–≤–æ–≤–∞–Ω–∏—è**:\n")

            # Group peaks by position
            if len(timeline) > 5:
                lines.append("   –û—Å–Ω–æ–≤–Ω—ã–µ –ø–∏–∫–∏ –Ω–∞–ø—Ä—è–∂–µ–Ω–∏—è:")
                for i, point in enumerate(timeline[:5], 1):
                    position = point.get('position', 0) * 100
                    score = point.get('score', 0)
                    source = point.get('source', 'unknown')
                    description = point.get('description', '')[:80]

                    lines.append(
                        f"\n   {i}. –ù–∞ {position:.0f}% —Ç–µ–∫—Å—Ç–∞ (–Ω–∞–ø—Ä—è–∂–µ–Ω–∏–µ: {score:.1f}/10)",
                        f"      –ò—Å—Ç–æ—á–Ω–∏–∫: {source}",
                        f"      {description}..."
                    )
            else:
                for point in timeline:
                    position = point.get('position', 0) * 100
                    score = point.get('score', 0)
                    source = point.get('source', 'unknown')

                    lines.append(
                        f"   ‚Ä¢ {position:.0f}%: {source} ({score:.1f}/10)"
                    )

            lines.append("")

        # Pacing analysis
        if peaks and len(peaks) > 2:
            # Calculate distribution
            first_half_peaks = sum(1 for p in peaks if p < 0.5)
            second_half_peaks = len(peaks) - first_half_peaks

            lines.append(f"üìå **–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø–∏–∫–æ–≤**:")
            lines.append(f"   ‚Ä¢ –ü–µ—Ä–≤–∞—è –ø–æ–ª–æ–≤–∏–Ω–∞: {first_half_peaks} –ø–∏–∫–æ–≤")
            lines.append(f"   ‚Ä¢ –í—Ç–æ—Ä–∞—è –ø–æ–ª–æ–≤–∏–Ω–∞: {second_half_peaks} –ø–∏–∫–æ–≤")

            if second_half_peaks > first_half_peaks * 1.5:
                lines.append("   üí° –ù–∞–ø—Ä—è–∂–µ–Ω–∏–µ –Ω–∞—Ä–∞—Å—Ç–∞–µ—Ç –∫ —Ñ–∏–Ω–∞–ª—É")
            elif first_half_peaks > second_half_peaks * 1.5:
                lines.append("   üí° –û—Å–Ω–æ–≤–Ω—ã–µ –∫–æ–Ω—Ñ–ª–∏–∫—Ç—ã –≤ –Ω–∞—á–∞–ª–µ –ø—Ä–æ–∏–∑–≤–µ–¥–µ–Ω–∏—è")
            else:
                lines.append("   üí° –†–∞–≤–Ω–æ–º–µ—Ä–Ω–æ–µ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –Ω–∞–ø—Ä—è–∂–µ–Ω–∏—è")

        return '\n'.join(lines)
