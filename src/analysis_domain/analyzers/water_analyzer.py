"""
–ê–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä "–≤–æ–¥—ã" –≤ —Ç–µ–∫—Å—Ç–µ (water level)
"""
import numpy as np
from typing import Dict, Any, List, Optional
from loguru import logger

from ..entities.base_analyzer import BaseAnalyzer
from ..entities.analysis_result import AnalysisResult
from src.text_domain.entities.base_text import BaseText
from src.common.types import AnalysisMode
from src.common.exceptions import AnalysisError


class WaterAnalyzer(BaseAnalyzer):
    """
    –ê–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä "–≤–æ–¥—ã" (–∏–∑–±—ã—Ç–æ—á–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–Ω—Ç–∞)

    –ù–ï –¢–†–ï–ë–£–ï–¢ LLM - –æ—á–µ–Ω—å –±—ã—Å—Ç—Ä—ã–π —Å—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫–∏–π –∞–Ω–∞–ª–∏–∑!
    –ù–ï –¢–†–ï–ë–£–ï–¢ embeddings - –∏—Å–ø–æ–ª—å–∑—É–µ—Ç —Å—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫–∏–µ –º–µ—Ç—Ä–∏–∫–∏

    –ò—Å–ø–æ–ª—å–∑—É–µ—Ç:
    - –ü–æ–≤—Ç–æ—Ä—ã —Å–ª–æ–≤ –∏ —Ñ—Ä–∞–∑ (–≤—ã—Å–æ–∫–∞—è —á–∞—Å—Ç–æ—Ç–∞ = –≤–æ–¥–∞)
    - –î–ª–∏–Ω—É –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π (–æ—á–µ–Ω—å –¥–ª–∏–Ω–Ω—ã–µ/–∫–æ—Ä–æ—Ç–∫–∏–µ = –≤–æ–¥–∞)
    - –ü–ª–æ—Ç–Ω–æ—Å—Ç—å —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Å–ª–æ–≤
    - –°–æ–æ—Ç–Ω–æ—à–µ–Ω–∏–µ —Å–ª—É–∂–µ–±–Ω—ã—Ö –∫ –∑–Ω–∞—á–∏–º—ã–º —Å–ª–æ–≤–∞–º

    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç:
    - –ü—Ä–æ—Ü–µ–Ω—Ç "–≤–æ–¥—ã" (0-100%)
    - –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏–æ–Ω–Ω—É—é –ø–ª–æ—Ç–Ω–æ—Å—Ç—å
    - –†–µ–π—Ç–∏–Ω–≥ (concise/balanced/verbose)
    - –°—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –ø–æ –ø–æ–≤—Ç–æ—Ä–∞–º
    """

    def __init__(self):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä–∞ –≤–æ–¥—ã"""
        pass

    @property
    def name(self) -> str:
        """–£–Ω–∏–∫–∞–ª—å–Ω–æ–µ –∏–º—è –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä–∞"""
        return "water"

    @property
    def display_name(self) -> str:
        """–ß–µ–ª–æ–≤–µ–∫–æ—á–∏—Ç–∞–µ–º–æ–µ –Ω–∞–∑–≤–∞–Ω–∏–µ"""
        return "–ê–Ω–∞–ª–∏–∑ –≤–æ–¥—ã –≤ —Ç–µ–∫—Å—Ç–µ"

    @property
    def description(self) -> str:
        """–û–ø–∏—Å–∞–Ω–∏–µ –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä–∞"""
        return "–û–ø—Ä–µ–¥–µ–ª—è–µ—Ç —É—Ä–æ–≤–µ–Ω—å '–≤–æ–¥—ã' (–∏–∑–±—ã—Ç–æ—á–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–Ω—Ç–∞) –≤ —Ç–µ–∫—Å—Ç–µ –Ω–∞ –æ—Å–Ω–æ–≤–µ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–π –∏–∑–±—ã—Ç–æ—á–Ω–æ—Å—Ç–∏"

    @property
    def requires_llm(self) -> bool:
        """–ù–ï —Ç—Ä–µ–±—É–µ—Ç—Å—è LLM"""
        return False

    @property
    def requires_embeddings(self) -> bool:
        """–ù–ï —Ç—Ä–µ–±—É—é—Ç—Å—è embeddings - —Å—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫–∏–π –∞–Ω–∞–ª–∏–∑"""
        return False

    async def analyze(
        self,
        text: BaseText,
        mode: AnalysisMode,
        **kwargs
    ) -> AnalysisResult:
        """
        –í—ã–ø–æ–ª–Ω–∏—Ç—å –∞–Ω–∞–ª–∏–∑ –≤–æ–¥—ã

        Args:
            text: –¢–µ–∫—Å—Ç –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
            mode: –†–µ–∂–∏–º –∞–Ω–∞–ª–∏–∑–∞
            **kwargs: –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã (–¥–æ–ª–∂–Ω—ã —Å–æ–¥–µ—Ä–∂–∞—Ç—å embeddings)

        Returns:
            AnalysisResult: –†–µ–∑—É–ª—å—Ç–∞—Ç –∞–Ω–∞–ª–∏–∑–∞
        """
        try:
            logger.info(f"–ù–∞—á–∞–ª–æ –∞–Ω–∞–ª–∏–∑–∞ –≤–æ–¥—ã: {text.title}")

            # –ü–æ–ª—É—á–∞–µ–º —á–∞–Ω–∫–∏ –∏ embeddings
            chunks = kwargs.get('chunks', [])
            embeddings = kwargs.get('embeddings', [])

            if not chunks:
                raise AnalysisError(
                    "–ê–Ω–∞–ª–∏–∑ –≤–æ–¥—ã —Ç—Ä–µ–±—É–µ—Ç —á–∞–Ω–∫–∏ —Ç–µ–∫—Å—Ç–∞",
                    details={"text_id": text.id}
                )

            if embeddings and len(embeddings) > 0:
                # –ò—Å–ø–æ–ª—å–∑—É–µ–º embeddings –¥–ª—è —Ç–æ—á–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞
                water_data = self._analyze_with_embeddings(chunks, embeddings)
            else:
                # Fallback –Ω–∞ —ç–≤—Ä–∏—Å—Ç–∏–∫—É
                logger.warning("Embeddings –Ω–µ–¥–æ—Å—Ç—É–ø–Ω—ã, –∏—Å–ø–æ–ª—å–∑—É–µ–º —ç–≤—Ä–∏—Å—Ç–∏–∫—É")
                water_data = self._analyze_with_heuristics(chunks)

            logger.info(f"–ê–Ω–∞–ª–∏–∑ –≤–æ–¥—ã –∑–∞–≤–µ—Ä—à—ë–Ω: {water_data['water_percentage']:.1f}%")

            return AnalysisResult(
                text_id=text.id,
                analyzer_name=self.name,
                mode=mode,
                data=water_data
            )

        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –∞–Ω–∞–ª–∏–∑–∞ –≤–æ–¥—ã: {e}")
            raise AnalysisError(
                f"–ù–µ —É–¥–∞–ª–æ—Å—å –ø—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å –≤–æ–¥—É: {str(e)}",
                details={"text_id": text.id}
            )

    def _analyze_with_embeddings(
        self,
        chunks: List[Any],
        embeddings: List[np.ndarray]
    ) -> Dict[str, Any]:
        """
        –ê–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å –≤–æ–¥—É –∏—Å–ø–æ–ª—å–∑—É—è semantic embeddings

        Args:
            chunks: –°–ø–∏—Å–æ–∫ —á–∞–Ω–∫–æ–≤
            embeddings: –°–ø–∏—Å–æ–∫ –≤–µ–∫—Ç–æ—Ä–æ–≤

        Returns:
            Dict: –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –∞–Ω–∞–ª–∏–∑–∞
        """
        if len(embeddings) != len(chunks):
            logger.warning("–ù–µ—Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ embeddings, fallback")
            return self._analyze_with_heuristics(chunks)

        # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –≤ numpy array
        emb_matrix = np.array([emb for emb in embeddings])

        # 1. –í—ã—á–∏—Å–ª—è–µ–º —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫—É—é –≤–∞—Ä–∏–∞—Ç–∏–≤–Ω–æ—Å—Ç—å –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —á–∞–Ω–∫–∞
        # –ù–∏–∑–∫–∞—è –≤–∞—Ä–∏–∞—Ç–∏–≤–Ω–æ—Å—Ç—å = –Ω–∏–∑–∫–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–æ–Ω–Ω–∞—è –ø–ª–æ—Ç–Ω–æ—Å—Ç—å
        chunk_variances = []
        for emb in emb_matrix:
            variance = np.var(emb)
            chunk_variances.append(variance)

        # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º –≤–∞—Ä–∏–∞—Ç–∏–≤–Ω–æ—Å—Ç—å
        max_var = max(chunk_variances) if chunk_variances else 1.0
        normalized_variances = [v / max_var for v in chunk_variances]

        # 2. –í—ã—á–∏—Å–ª—è–µ–º –∏–∑–±—ã—Ç–æ—á–Ω–æ—Å—Ç—å (—Å—Ö–æ–∂–µ—Å—Ç—å —Å–æ—Å–µ–¥–Ω–∏—Ö —á–∞–Ω–∫–æ–≤)
        redundancy_scores = []
        for i in range(len(emb_matrix) - 1):
            similarity = self._cosine_similarity(emb_matrix[i], emb_matrix[i+1])
            redundancy_scores.append(similarity)

        avg_redundancy = sum(redundancy_scores) / len(redundancy_scores) if redundancy_scores else 0.0

        # 3. –í—ã—á–∏—Å–ª—è–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–æ–Ω–Ω—É—é –ø–ª–æ—Ç–Ω–æ—Å—Ç—å
        # –í—ã—Å–æ–∫–∞—è –≤–∞—Ä–∏–∞—Ç–∏–≤–Ω–æ—Å—Ç—å + –Ω–∏–∑–∫–∞—è –∏–∑–±—ã—Ç–æ—á–Ω–æ—Å—Ç—å = –≤—ã—Å–æ–∫–∞—è –ø–ª–æ—Ç–Ω–æ—Å—Ç—å
        info_density_scores = []
        for i, variance in enumerate(normalized_variances):
            if i < len(redundancy_scores):
                # –ù–∏–∑–∫–∞—è –∏–∑–±—ã—Ç–æ—á–Ω–æ—Å—Ç—å —Å —Å–æ—Å–µ–¥–Ω–∏–º = –±–æ–ª–µ–µ —É–Ω–∏–∫–∞–ª—å–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è
                uniqueness = 1.0 - redundancy_scores[i]
                density = (variance + uniqueness) / 2.0
            else:
                density = variance

            info_density_scores.append(density)

        # –°—Ä–µ–¥–Ω—è—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–æ–Ω–Ω–∞—è –ø–ª–æ—Ç–Ω–æ—Å—Ç—å
        avg_info_density = sum(info_density_scores) / len(info_density_scores)

        # –ü—Ä–æ—Ü–µ–Ω—Ç –≤–æ–¥—ã = –æ–±—Ä–∞—Ç–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–æ–Ω–Ω–∞—è –ø–ª–æ—Ç–Ω–æ—Å—Ç—å
        water_percentage = (1.0 - avg_info_density) * 100

        # –ù–∞—Ö–æ–¥–∏–º "–≤–æ–¥—è–Ω–∏—Å—Ç—ã–µ" —á–∞–Ω–∫–∏ (–Ω–∏–∂–Ω–∏–µ 20%)
        sorted_indices = sorted(
            range(len(info_density_scores)),
            key=lambda i: info_density_scores[i]
        )
        verbose_count = max(int(len(chunks) * 0.2), 1)
        verbose_chunks = sorted_indices[:verbose_count]

        # –†–µ–π—Ç–∏–Ω–≥
        if water_percentage < 20:
            rating = "concise"
            rating_ru = "–ª–∞–∫–æ–Ω–∏—á–Ω—ã–π"
            rating_emoji = "‚ú®"
        elif water_percentage < 50:
            rating = "balanced"
            rating_ru = "—Å–±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∞–Ω–Ω—ã–π"
            rating_emoji = "‚úÖ"
        else:
            rating = "verbose"
            rating_ru = "–º–Ω–æ–≥–æ—Å–ª–æ–≤–Ω—ã–π"
            rating_emoji = "üíß"

        return {
            "water_percentage": round(water_percentage, 2),
            "info_density": round(avg_info_density, 2),
            "rating": rating,
            "rating_ru": rating_ru,
            "rating_emoji": rating_emoji,
            "verbose_chunks": verbose_chunks,
            "avg_redundancy": round(avg_redundancy, 2)
        }

    def _analyze_with_heuristics(self, chunks: List[Any]) -> Dict[str, Any]:
        """
        –ê–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å –≤–æ–¥—É –∏—Å–ø–æ–ª—å–∑—É—è —ç–≤—Ä–∏—Å—Ç–∏–∫—É

        Args:
            chunks: –°–ø–∏—Å–æ–∫ —á–∞–Ω–∫–æ–≤

        Returns:
            Dict: –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –∞–Ω–∞–ª–∏–∑–∞
        """

        # –û–±–æ–≥–∞—â–µ–Ω–Ω—ã–µ —Å–ª–æ–≤–∞—Ä–∏ –¥–ª—è –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏—è "–≤–æ–¥–Ω–æ—Å—Ç–∏"
        water_patterns = {
            "—Å–ª–æ–≤–∞_–ø–∞—Ä–∞–∑–∏—Ç—ã": {
                "–∫–∞–∫ –±—ã", "—Ç–∏–ø–∞", "–∫–æ—Ä–æ—á–µ", "–≤–æ–æ–±—â–µ", "–≤ –æ–±—â–µ–º", "—Ç–∞–∫ —Å–∫–∞–∑–∞—Ç—å", "–º–æ–∂–Ω–æ —Å–∫–∞–∑–∞—Ç—å",
                "–µ—Å–ª–∏ —á–µ—Å—Ç–Ω–æ", "—á–µ—Å—Ç–Ω–æ –≥–æ–≤–æ—Ä—è", "–ø—Ä—è–º–æ —Å–∫–∞–∂–µ–º", "–Ω–∞–¥–æ —Å–∫–∞–∑–∞—Ç—å", "—Å–∫–∞–∂–µ–º —Ç–∞–∫",
                "–≤ –ø—Ä–∏–Ω—Ü–∏–ø–µ", "–≤ –æ—Å–Ω–æ–≤–Ω–æ–º", "–ø–æ –±–æ–ª—å—à–æ–º—É —Å—á–µ—Ç—É", "–µ—Å–ª–∏ –º–æ–∂–Ω–æ —Ç–∞–∫ –≤—ã—Ä–∞–∑–∏—Ç—å—Å—è",
                "—Å–æ–±—Å—Ç–≤–µ–Ω–Ω–æ –≥–æ–≤–æ—Ä—è", "–µ—Å–ª–∏ —Ä–∞–∑–æ–±—Ä–∞—Ç—å—Å—è", "–∫–∞–∫ –≥–æ–≤–æ—Ä–∏—Ç—Å—è", "–º–µ–∂–¥—É –ø—Ä–æ—á–∏–º",
                "–≤ –Ω–µ–∫–æ—Ç–æ—Ä–æ–º —Å–º—ã—Å–ª–µ", "–≤ –∫–∞–∫–æ–º-—Ç–æ —Å–º—ã—Å–ª–µ", "–¥–æ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω–æ–π —Å—Ç–µ–ø–µ–Ω–∏"
            },
            "–∫–∞–Ω—Ü–µ–ª—è—Ä–∏–∑–º—ã": {
                "–≤ –Ω–∞—Å—Ç–æ—è—â–µ–µ –≤—Ä–µ–º—è", "–Ω–∞ —Å–µ–≥–æ–¥–Ω—è—à–Ω–∏–π –¥–µ–Ω—å", "–≤ –¥–∞–Ω–Ω—ã–π –º–æ–º–µ–Ω—Ç –≤—Ä–µ–º–µ–Ω–∏",
                "–ø–æ —Å–æ—Å—Ç–æ—è–Ω–∏—é –Ω–∞", "–≤ —Ç–µ—á–µ–Ω–∏–µ –¥–ª–∏—Ç–µ–ª—å–Ω–æ–≥–æ –≤—Ä–µ–º–µ–Ω–∏", "–≤ —Ö–æ–¥–µ –ø—Ä–æ–≤–µ–¥–µ–Ω–∏—è",
                "—Å —Ü–µ–ª—å—é –æ—Å—É—â–µ—Å—Ç–≤–ª–µ–Ω–∏—è", "–¥–ª—è –æ–±–µ—Å–ø–µ—á–µ–Ω–∏—è –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏", "–≤ –ø—Ä–æ—Ü–µ—Å—Å–µ —Ä–µ–∞–ª–∏–∑–∞—Ü–∏–∏",
                "–ø—Ä–∏ —É—Å–ª–æ–≤–∏–∏ —Å–æ–±–ª—é–¥–µ–Ω–∏—è", "–≤ —Ä–∞–º–∫–∞—Ö –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è", "—Å–æ–≥–ª–∞—Å–Ω–æ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–Ω—ã–º —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è–º"
            },
            "–∏–∑–±—ã—Ç–æ—á–Ω—ã–µ_–∫–æ–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏": {
                "–¥–µ–ª–æ –≤ —Ç–æ–º —á—Ç–æ", "—Å—É—Ç—å –¥–µ–ª–∞ –∑–∞–∫–ª—é—á–∞–µ—Ç—Å—è –≤ —Ç–æ–º", "—Ö–æ—á–µ—Ç—Å—è –æ—Ç–º–µ—Ç–∏—Ç—å —á—Ç–æ",
                "—Å–ª–µ–¥—É–µ—Ç –ø–æ–¥—á–µ—Ä–∫–Ω—É—Ç—å —á—Ç–æ", "–≤–∞–∂–Ω–æ –ø–æ–Ω–∏–º–∞—Ç—å —á—Ç–æ", "–Ω–µ–ª—å–∑—è –Ω–µ —Å–∫–∞–∑–∞—Ç—å —á—Ç–æ",
                "—Å—Ç–æ–∏—Ç –æ–±—Ä–∞—Ç–∏—Ç—å –≤–Ω–∏–º–∞–Ω–∏–µ –Ω–∞ —Ç–æ —á—Ç–æ", "–Ω–µ–æ–±—Ö–æ–¥–∏–º–æ –æ—Ç–º–µ—Ç–∏—Ç—å —á—Ç–æ",
                "—Ö–æ—Ç–µ–ª–æ—Å—å –±—ã –ø–æ–¥—á–µ—Ä–∫–Ω—É—Ç—å —á—Ç–æ", "–Ω–µ –º–æ–≥—É –Ω–µ –æ—Ç–º–µ—Ç–∏—Ç—å —á—Ç–æ"
            },
            "—Ç–∞–≤—Ç–æ–ª–æ–≥–∏–∏": {
                "–≥–ª–∞–≤–Ω–∞—è —Å—É—Ç—å", "—Å–≤–æ–±–æ–¥–Ω–∞—è –≤–∞–∫–∞–Ω—Å–∏—è", "–ø—Ä–µ–π—Å–∫—É—Ä–∞–Ω—Ç —Ü–µ–Ω", "–ø–∞–º—è—Ç–Ω—ã–π —Å—É–≤–µ–Ω–∏—Ä",
                "—Å–æ–≤–º–µ—Å—Ç–Ω–æ–µ —Å–æ—Ç—Ä—É–¥–Ω–∏—á–µ—Å—Ç–≤–æ", "–≤–∑–∞–∏–º–Ω–æ–µ —Å–æ—Ç—Ä—É–¥–Ω–∏—á–µ—Å—Ç–≤–æ", "–ø–µ—Ä–≤–∞—è –ø—Ä–µ–º—å–µ—Ä–∞",
                "—Å—Ç–∞—Ä—ã–π –≤–µ—Ç–µ—Ä–∞–Ω", "–º–æ–ª–æ–¥–æ–π —é–Ω–æ—à–∞", "–Ω–∞—Ä–æ–¥–Ω–∞—è –¥–µ–º–æ–∫—Ä–∞—Ç–∏—è", "–±–µ—Å–ø–ª–∞—Ç–Ω—ã–π –ø–æ–¥–∞—Ä–æ–∫",
                "–æ–∫—Ä—É–∂–∞—é—â–∞—è —Å—Ä–µ–¥–∞ –≤–æ–∫—Ä—É–≥", "–ø–µ—Ä–∏–æ–¥ –≤—Ä–µ–º–µ–Ω–∏", "–æ–≥—Ä–æ–º–Ω–∞—è –º–∞—Ö–∏–Ω–∞"
            },
            "—à—Ç–∞–º–ø—ã_–∫–ª–∏—à–µ": {
                "—Å–∞–º–æ —Å–æ–±–æ–π —Ä–∞–∑—É–º–µ–µ—Ç—Å—è", "–±–µ–∑ –≤—Å—è–∫–æ–≥–æ —Å–æ–º–Ω–µ–Ω–∏—è", "–∫–∞–∫ –∏–∑–≤–µ—Å—Ç–Ω–æ",
                "–Ω–µ —Å–µ–∫—Ä–µ—Ç —á—Ç–æ", "–≤—Å–µ–º –∏–∑–≤–µ—Å—Ç–Ω–æ —á—Ç–æ", "–∏–≥—Ä–∞–µ—Ç –≤–∞–∂–Ω—É—é —Ä–æ–ª—å",
                "–∏–º–µ–µ—Ç –±–æ–ª—å—à–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ", "–Ω–µ—Ç—Ä—É–¥–Ω–æ –¥–æ–≥–∞–¥–∞—Ç—å—Å—è", "–ª–µ–≥–∫–æ –ø–æ–Ω—è—Ç—å",
                "–æ—á–µ–≤–∏–¥–Ω–æ —á—Ç–æ", "–ø–æ–Ω—è—Ç–Ω–æ —á—Ç–æ", "—è—Å–Ω–æ —á—Ç–æ"
            }
        }

        # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º —Ç–µ–∫—Å—Ç –ø–æ —á–∞–Ω–∫–∞–º
        total_words = 0
        total_water_words = 0
        water_examples = {category: [] for category in water_patterns.keys()}
        word_frequencies = {}

        full_text = " ".join(chunk.content for chunk in chunks).lower()

        # –ü–æ–¥—Å—á–∏—Ç—ã–≤–∞–µ–º —Å–ª–æ–≤–∞
        words = full_text.split()
        total_words = len(words)

        # –°—á–∏—Ç–∞–µ–º —á–∞—Å—Ç–æ—Ç—ã —Å–ª–æ–≤ –¥–ª—è –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏—è –ø–æ–≤—Ç–æ—Ä–æ–≤
        for word in words:
            if len(word) > 3:  # –ò–≥–Ω–æ—Ä–∏—Ä—É–µ–º –∫–æ—Ä–æ—Ç–∫–∏–µ —Å–ª–æ–≤–∞
                word_frequencies[word] = word_frequencies.get(word, 0) + 1

        # –ò—â–µ–º –ø–∞—Ç—Ç–µ—Ä–Ω—ã –≤–æ–¥–Ω–æ—Å—Ç–∏
        for category, patterns in water_patterns.items():
            for pattern in patterns:
                count = full_text.count(pattern)
                if count > 0:
                    water_examples[category].append((pattern, count))
                    total_water_words += count * len(pattern.split())

        # –°—á–∏—Ç–∞–µ–º –ø–æ–≤—Ç–æ—Ä—è—é—â–∏–µ—Å—è —Å–ª–æ–≤–∞ (>5 —Ä–∞–∑ = –≤–æ–¥–∞)
        repetitive_words = sum(
            count for count in word_frequencies.values() if count > 5
        )

        # –û–±–Ω–∞—Ä—É–∂–µ–Ω–∏–µ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏—Ö –ø–æ–≤—Ç–æ—Ä–æ–≤ (–æ–ø–∏—Å–∞–Ω–∏–µ –æ–¥–Ω–æ–≥–æ –∏ —Ç–æ–≥–æ –∂–µ —Ä–∞–∑–Ω—ã–º–∏ —Å–ª–æ–≤–∞–º–∏)
        semantic_repeats = self._detect_semantic_repetitions(chunks)
        semantic_water_percentage = (len(semantic_repeats) / max(len(chunks), 1)) * 100

        # –†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ–º –ø—Ä–æ—Ü–µ–Ω—Ç –≤–æ–¥–Ω–æ—Å—Ç–∏
        if total_words == 0:
            water_percentage = 50.0
        else:
            # –ö–æ–º–±–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –æ—Ü–µ–Ω–∫–∞: –ø–∞—Ç—Ç–µ—Ä–Ω—ã + –ø–æ–≤—Ç–æ—Ä—ã + —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–µ –ø–æ–≤—Ç–æ—Ä—ã
            pattern_water = (total_water_words / total_words) * 100
            repetition_water = (repetitive_words / total_words) * 100
            water_percentage = min(pattern_water + repetition_water + semantic_water_percentage, 100)

        # –ù–∞—Ö–æ–¥–∏–º —Ç–æ–ø –ø–æ–≤—Ç–æ—Ä—è—é—â–∏—Ö—Å—è —Å–ª–æ–≤
        top_repetitive = sorted(
            [(word, count) for word, count in word_frequencies.items() if count > 3],
            key=lambda x: x[1], reverse=True
        )[:10]

        # –†–µ–π—Ç–∏–Ω–≥
        if water_percentage < 20:
            rating = "concise"
            rating_ru = "–ª–∞–∫–æ–Ω–∏—á–Ω—ã–π"
            rating_emoji = "‚ú®"
        elif water_percentage < 50:
            rating = "balanced"
            rating_ru = "—Å–±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∞–Ω–Ω—ã–π"
            rating_emoji = "‚úÖ"
        else:
            rating = "verbose"
            rating_ru = "–º–Ω–æ–≥–æ—Å–ª–æ–≤–Ω—ã–π"
            rating_emoji = "üíß"

        return {
            "water_percentage": round(water_percentage, 2),
            "info_density": round(1.0 - (water_percentage / 100), 2),
            "rating": rating,
            "rating_ru": rating_ru,
            "rating_emoji": rating_emoji,
            "verbose_chunks": [],
            "avg_redundancy": 0.0,
            "method": "enhanced_heuristic",
            "water_examples": water_examples,
            "top_repetitive_words": top_repetitive,
            "semantic_repeats": semantic_repeats,
            "pattern_water_percentage": round((total_water_words / max(total_words, 1)) * 100, 2),
            "repetition_water_percentage": round((repetitive_words / max(total_words, 1)) * 100, 2),
            "semantic_water_percentage": round(semantic_water_percentage, 2)
        }

    def _detect_semantic_repetitions(self, chunks: List[Any]) -> List[Dict[str, Any]]:
        """
        –û–±–Ω–∞—Ä—É–∂–∏—Ç—å —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–µ –ø–æ–≤—Ç–æ—Ä—ã - –∫–æ–≥–¥–∞ –æ–¥–Ω–æ –∏ —Ç–æ –∂–µ –æ–ø–∏—Å—ã–≤–∞–µ—Ç—Å—è —Ä–∞–∑–Ω—ã–º–∏ —Å–ª–æ–≤–∞–º–∏

        Args:
            chunks: –°–ø–∏—Å–æ–∫ —á–∞–Ω–∫–æ–≤

        Returns:
            List: –°–ø–∏—Å–æ–∫ –Ω–∞–π–¥–µ–Ω–Ω—ã—Ö —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏—Ö –ø–æ–≤—Ç–æ—Ä–æ–≤
        """
        semantic_repeats = []

        # –ü–∞—Ç—Ç–µ—Ä–Ω—ã –¥–ª—è –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏—è —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏—Ö –ø–æ–≤—Ç–æ—Ä–æ–≤
        repetition_patterns = {
            "–ø–æ–≤—Ç–æ—Ä_–∏–¥–µ–∏": [
                # –ü–∞—Ç—Ç–µ—Ä–Ω—ã, —É–∫–∞–∑—ã–≤–∞—é—â–∏–µ –Ω–∞ –ø–µ—Ä–µ—Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–∫—É
                "–¥—Ä—É–≥–∏–º–∏ —Å–ª–æ–≤–∞–º–∏", "–∏–Ω—ã–º–∏ —Å–ª–æ–≤–∞–º–∏", "—Ç–æ –µ—Å—Ç—å", "–∞ –∏–º–µ–Ω–Ω–æ",
                "–ø—Ä–æ—â–µ –≥–æ–≤–æ—Ä—è", "–≥–æ–≤–æ—Ä—è –∏–Ω–∞—á–µ", "–º–æ–∂–Ω–æ —Å–∫–∞–∑–∞—Ç—å", "–∏–Ω–∞—á–µ –≥–æ–≤–æ—Ä—è"
            ],
            "–∏–∑–±—ã—Ç–æ—á–Ω—ã–µ_–æ–±—ä—è—Å–Ω–µ–Ω–∏—è": [
                # –õ–∏—à–Ω–∏–µ –æ–±—ä—è—Å–Ω–µ–Ω–∏—è –æ—á–µ–≤–∏–¥–Ω–æ–≥–æ
                "–∫–∞–∫ –∏–∑–≤–µ—Å—Ç–Ω–æ", "—Å–∞–º–æ —Å–æ–±–æ–π —Ä–∞–∑—É–º–µ–µ—Ç—Å—è", "–æ—á–µ–≤–∏–¥–Ω–æ —á—Ç–æ", "–ø–æ–Ω—è—Ç–Ω–æ —á—Ç–æ",
                "–Ω–µ –Ω—É–∂–Ω–æ –æ–±—ä—è—Å–Ω—è—Ç—å", "–≤—Å–µ–º —è—Å–Ω–æ", "–Ω–µ —Å–µ–∫—Ä–µ—Ç", "–Ω–∏ –¥–ª—è –∫–æ–≥–æ –Ω–µ —Å–µ–∫—Ä–µ—Ç"
            ],
            "–¥—É–±–ª–∏—Ä–æ–≤–∞–Ω–∏–µ_–¥–µ–π—Å—Ç–≤–∏–π": [
                # –û–ø–∏—Å–∞–Ω–∏–µ –æ–¥–Ω–æ–≥–æ –¥–µ–π—Å—Ç–≤–∏—è –Ω–µ—Å–∫–æ–ª—å–∫–æ —Ä–∞–∑
                "–æ–ø—è—Ç—å", "—Å–Ω–æ–≤–∞", "–µ—â–µ —Ä–∞–∑", "–ø–æ–≤—Ç–æ—Ä–Ω–æ", "–æ–ø—è—Ç—å –∂–µ", "–ø–æ-–ø—Ä–µ–∂–Ω–µ–º—É",
                "–∫–∞–∫ –∏ –ø—Ä–µ–∂–¥–µ", "–∫–∞–∫ –æ–±—ã—á–Ω–æ", "–∫–∞–∫ –≤—Å–µ–≥–¥–∞"
            ]
        }

        # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º —Å–æ—Å–µ–¥–Ω–∏–µ —á–∞–Ω–∫–∏ –Ω–∞ –ø—Ä–µ–¥–º–µ—Ç —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏—Ö –ø–æ–≤—Ç–æ—Ä–æ–≤
        for i in range(len(chunks) - 1):
            chunk1 = chunks[i].content.lower()
            chunk2 = chunks[i + 1].content.lower()

            # –ò—â–µ–º –ø–∞—Ç—Ç–µ—Ä–Ω—ã –ø–µ—Ä–µ—Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–∫–∏
            for category, patterns in repetition_patterns.items():
                for pattern in patterns:
                    if pattern in chunk1 or pattern in chunk2:
                        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å—Ö–æ–∂–µ—Å—Ç—å –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤
                        similarity_score = self._calculate_keyword_similarity(chunk1, chunk2)
                        if similarity_score > 0.3:  # –ü–æ—Ä–æ–≥ —Å—Ö–æ–∂–µ—Å—Ç–∏
                            semantic_repeats.append({
                                "type": category,
                                "chunks": [i, i + 1],
                                "pattern": pattern,
                                "similarity": similarity_score,
                                "description": f"–í–æ–∑–º–æ–∂–Ω–æ–µ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–µ –ø–æ–≤—Ç–æ—Ä–µ–Ω–∏–µ —á–µ—Ä–µ–∑ '{pattern}'"
                            })

            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –ø–æ–≤—Ç–æ—Ä–µ–Ω–∏–µ –∫–ª—é—á–µ–≤—ã—Ö —Å—É—â–µ—Å—Ç–≤–∏—Ç–µ–ª—å–Ω—ã—Ö –∏ –≥–ª–∞–≥–æ–ª–æ–≤
            keywords1 = self._extract_keywords(chunk1)
            keywords2 = self._extract_keywords(chunk2)

            common_keywords = set(keywords1) & set(keywords2)
            if len(common_keywords) >= 3:  # –ï—Å–ª–∏ 3+ –æ–±—â–∏—Ö –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤–∞
                semantic_repeats.append({
                    "type": "keyword_repetition",
                    "chunks": [i, i + 1],
                    "common_keywords": list(common_keywords),
                    "description": f"–ü–æ–≤—Ç–æ—Ä–µ–Ω–∏–µ –∫–ª—é—á–µ–≤—ã—Ö –ø–æ–Ω—è—Ç–∏–π: {', '.join(list(common_keywords)[:3])}"
                })

        return semantic_repeats

    def _calculate_keyword_similarity(self, text1: str, text2: str) -> float:
        """
        –í—ã—á–∏—Å–ª–∏—Ç—å —Å—Ö–æ–∂–µ—Å—Ç—å —Ç–µ–∫—Å—Ç–æ–≤ –ø–æ –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–∞–º

        Args:
            text1: –ü–µ—Ä–≤—ã–π —Ç–µ–∫—Å—Ç
            text2: –í—Ç–æ—Ä–æ–π —Ç–µ–∫—Å—Ç

        Returns:
            float: –ö–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç —Å—Ö–æ–∂–µ—Å—Ç–∏ 0-1
        """
        keywords1 = set(self._extract_keywords(text1))
        keywords2 = set(self._extract_keywords(text2))

        if not keywords1 or not keywords2:
            return 0.0

        intersection = len(keywords1 & keywords2)
        union = len(keywords1 | keywords2)

        return intersection / union if union > 0 else 0.0

    def _extract_keywords(self, text: str) -> List[str]:
        """
        –ò–∑–≤–ª–µ—á—å –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –∏–∑ —Ç–µ–∫—Å—Ç–∞ (—Å—É—â–µ—Å—Ç–≤–∏—Ç–µ–ª—å–Ω—ã–µ, –≥–ª–∞–≥–æ–ª—ã, –ø—Ä–∏–ª–∞–≥–∞—Ç–µ–ª—å–Ω—ã–µ)

        Args:
            text: –¢–µ–∫—Å—Ç –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞

        Returns:
            List: –°–ø–∏—Å–æ–∫ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤
        """
        # –ü—Ä–æ—Å—Ç–∞—è —ç–≤—Ä–∏—Å—Ç–∏–∫–∞: —Å–ª–æ–≤–∞ –¥–ª–∏–Ω–Ω–µ–µ 4 —Å–∏–º–≤–æ–ª–æ–≤, –∏—Å–∫–ª—é—á–∞—è —Å–ª—É–∂–µ–±–Ω—ã–µ
        stop_words = {
            '—á—Ç–æ', '–∫–∞–∫', '—ç—Ç–æ', '–≤—Å–µ', '–¥–ª—è', '–æ–Ω–∞', '–æ–Ω–∏', '–µ–≥–æ', '–±—ã–ª', '–±—ã–ª–∞',
            '–±—ã–ª–∏', '–µ—Å—Ç—å', '–º–æ–∂–µ—Ç', '–±—ã—Ç—å', '—Å–∫–∞–∑–∞–ª', '—Å–∫–∞–∑–∞–ª–∞', '–æ–¥–∏–Ω', '–æ–¥–Ω–∞',
            '—Ç–æ—Ç', '—Ç–æ–º', '—Ç–æ–π', '—Ç–µ–º', '–ø—Ä–∏', '–Ω–∞–¥', '–ø–æ–¥', '–±–µ–∑', '—á–µ—Ä–µ–∑'
        }

        words = text.split()
        keywords = []

        for word in words:
            # –£–±–∏—Ä–∞–µ–º –ø—É–Ω–∫—Ç—É–∞—Ü–∏—é
            clean_word = ''.join(c for c in word if c.isalpha()).lower()
            if (len(clean_word) > 4 and
                clean_word not in stop_words and
                clean_word.isalpha()):
                keywords.append(clean_word)

        return keywords

    def _cosine_similarity(self, vec1: np.ndarray, vec2: np.ndarray) -> float:
        """
        –í—ã—á–∏—Å–ª–∏—Ç—å –∫–æ—Å–∏–Ω—É—Å–Ω–æ–µ —Å—Ö–æ–¥—Å—Ç–≤–æ

        Args:
            vec1: –ü–µ—Ä–≤—ã–π –≤–µ–∫—Ç–æ—Ä
            vec2: –í—Ç–æ—Ä–æ–π –≤–µ–∫—Ç–æ—Ä

        Returns:
            float: –ö–æ—Å–∏–Ω—É—Å–Ω–æ–µ —Å—Ö–æ–¥—Å—Ç–≤–æ (0-1)
        """
        dot_product = np.dot(vec1, vec2)
        norm1 = np.linalg.norm(vec1)
        norm2 = np.linalg.norm(vec2)

        if norm1 == 0 or norm2 == 0:
            return 0.0

        return dot_product / (norm1 * norm2)

    def interpret_results(self, result: AnalysisResult) -> str:
        """
        –ò–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∏—Ä–æ–≤–∞—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –∞–Ω–∞–ª–∏–∑–∞

        Args:
            result: –†–µ–∑—É–ª—å—Ç–∞—Ç –∞–Ω–∞–ª–∏–∑–∞

        Returns:
            str: –ß–µ–ª–æ–≤–µ–∫–æ—á–∏—Ç–∞–µ–º–∞—è –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏—è
        """
        data = result.data
        water_percent = data.get('water_percentage', 0)
        info_density = data.get('info_density', 0)
        rating_ru = data.get('rating_ru', '–Ω–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–π')
        rating_emoji = data.get('rating_emoji', 'üíß')

        lines = [
            f'üíß **–ê–Ω–∞–ª–∏–∑ "–≤–æ–¥—ã" –≤ —Ç–µ–∫—Å—Ç–µ**\n',
            f"{rating_emoji} **–£—Ä–æ–≤–µ–Ω—å –≤–æ–¥—ã**: {water_percent:.1f}%",
            f"üìä **–ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏–æ–Ω–Ω–∞—è –ø–ª–æ—Ç–Ω–æ—Å—Ç—å**: {info_density:.2f}",
            f"‚≠ê **–†–µ–π—Ç–∏–Ω–≥**: {rating_ru}\n"
        ]

        # –ò–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏—è
        if water_percent < 20:
            interpretation = (
                "–û—á–µ–Ω—å –ª–∞–∫–æ–Ω–∏—á–Ω—ã–π —Ç–µ–∫—Å—Ç. –í—ã—Å–æ–∫–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–æ–Ω–Ω–∞—è –ø–ª–æ—Ç–Ω–æ—Å—Ç—å. "
                "–ö–∞–∂–¥–æ–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ –Ω–µ—Å—ë—Ç —Å–º—ã—Å–ª. –û—Ç–ª–∏—á–Ω—ã–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç!"
            )
        elif water_percent < 35:
            interpretation = (
                "–•–æ—Ä–æ—à–∏–π –±–∞–ª–∞–Ω—Å –º–µ–∂–¥—É –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–µ–π –∏ —á–∏—Ç–∞–±–µ–ª—å–Ω–æ—Å—Ç—å—é. "
                "–£–º–µ—Ä–µ–Ω–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–æ–≤—Ç–æ—Ä–æ–≤ –∏ –æ–ø–∏—Å–∞–Ω–∏–π."
            )
        elif water_percent < 50:
            interpretation = (
                "–°—Ä–µ–¥–Ω–∏–π —É—Ä–æ–≤–µ–Ω—å –≤–æ–¥—ã. –ï—Å—Ç—å –ø–æ–≤—Ç–æ—Ä—ã –∏ –∏–∑–±—ã—Ç–æ—á–Ω—ã–µ –æ–ø–∏—Å–∞–Ω–∏—è, "
                "–Ω–æ –æ–Ω–∏ –Ω–µ –∫—Ä–∏—Ç–∏—á–Ω—ã."
            )
        elif water_percent < 70:
            interpretation = (
                '–ú–Ω–æ–≥–æ "–≤–æ–¥—ã". –ó–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–æ–≤—Ç–æ—Ä–æ–≤, –∏–∑–±—ã—Ç–æ—á–Ω—ã—Ö '
                "–æ–ø–∏—Å–∞–Ω–∏–π –∏ –º–∞–ª–æ–∏–Ω—Ñ–æ—Ä–º–∞—Ç–∏–≤–Ω—ã—Ö —Ñ—Ä–∞–≥–º–µ–Ω—Ç–æ–≤. –†–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è —Ä–µ–¥–∞–∫—Ç—É—Ä–∞."
            )
        else:
            interpretation = (
                "–û—á–µ–Ω—å –≤—ã—Å–æ–∫–∏–π —É—Ä–æ–≤–µ–Ω—å –≤–æ–¥—ã. –¢–µ–∫—Å—Ç –ø–µ—Ä–µ–≥—Ä—É–∂–µ–Ω –ø–æ–≤—Ç–æ—Ä–∞–º–∏ "
                "–∏ –∏–∑–±—ã—Ç–æ—á–Ω—ã–º–∏ –æ–ø–∏—Å–∞–Ω–∏—è–º–∏. –¢—Ä–µ–±—É–µ—Ç—Å—è —Å–µ—Ä—å—ë–∑–Ω–∞—è —Ä–µ–¥–∞–∫—Ç—É—Ä–∞."
            )

        lines.append(f"üí° **–ò–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏—è**: {interpretation}")

        return '\n'.join(lines)
