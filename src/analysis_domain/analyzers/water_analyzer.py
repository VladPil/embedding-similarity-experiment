"""
–ê–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä "–≤–æ–¥—ã" –≤ —Ç–µ–∫—Å—Ç–µ (water level)
"""
import numpy as np
from typing import Dict, Any, List, Optional
from loguru import logger

from ..entities.base_analyzer import BaseAnalyzer
from ..entities.analysis_result import AnalysisResult
from src.text_domain.entities.base_text import BaseText
from src.common.types import AnalysisMode
from src.common.exceptions import AnalysisError


class WaterAnalyzer(BaseAnalyzer):
    """
    –ê–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä "–≤–æ–¥—ã" (–∏–∑–±—ã—Ç–æ—á–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–Ω—Ç–∞)

    –ù–ï –¢–†–ï–ë–£–ï–¢ LLM - –æ—á–µ–Ω—å –±—ã—Å—Ç—Ä—ã–π!
    –¢–†–ï–ë–£–ï–¢ embeddings –¥–ª—è —Ç–æ—á–Ω–æ—Å—Ç–∏

    –ò—Å–ø–æ–ª—å–∑—É–µ—Ç:
    - –°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫—É—é –≤–∞—Ä–∏–∞—Ç–∏–≤–Ω–æ—Å—Ç—å (–Ω–∏–∑–∫–∞—è = –ø–æ–≤—Ç–æ—Ä = –≤–æ–¥–∞)
    - –ò–∑–±—ã—Ç–æ—á–Ω–æ—Å—Ç—å (—Å—Ö–æ–∂–µ—Å—Ç—å —Å–æ—Å–µ–¥–Ω–∏—Ö —á–∞–Ω–∫–æ–≤ = –≤–æ–¥–∞)
    - –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏–æ–Ω–Ω—É—é –ø–ª–æ—Ç–Ω–æ—Å—Ç—å

    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç:
    - –ü—Ä–æ—Ü–µ–Ω—Ç "–≤–æ–¥—ã" (0-100%)
    - –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏–æ–Ω–Ω—É—é –ø–ª–æ—Ç–Ω–æ—Å—Ç—å
    - –†–µ–π—Ç–∏–Ω–≥ (concise/balanced/verbose)
    - –ò–Ω–¥–µ–∫—Å—ã "–≤–æ–¥—è–Ω–∏—Å—Ç—ã—Ö" —á–∞–Ω–∫–æ–≤
    """

    def __init__(self):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä–∞ –≤–æ–¥—ã"""
        pass

    @property
    def requires_llm(self) -> bool:
        """–ù–ï —Ç—Ä–µ–±—É–µ—Ç—Å—è LLM"""
        return False

    @property
    def requires_embeddings(self) -> bool:
        """–¢–†–ï–ë–£–Æ–¢–°–Ø embeddings"""
        return True

    async def analyze(
        self,
        text: BaseText,
        mode: AnalysisMode,
        **kwargs
    ) -> AnalysisResult:
        """
        –í—ã–ø–æ–ª–Ω–∏—Ç—å –∞–Ω–∞–ª–∏–∑ –≤–æ–¥—ã

        Args:
            text: –¢–µ–∫—Å—Ç –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
            mode: –†–µ–∂–∏–º –∞–Ω–∞–ª–∏–∑–∞
            **kwargs: –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã (–¥–æ–ª–∂–Ω—ã —Å–æ–¥–µ—Ä–∂–∞—Ç—å embeddings)

        Returns:
            AnalysisResult: –†–µ–∑—É–ª—å—Ç–∞—Ç –∞–Ω–∞–ª–∏–∑–∞
        """
        try:
            logger.info(f"–ù–∞—á–∞–ª–æ –∞–Ω–∞–ª–∏–∑–∞ –≤–æ–¥—ã: {text.title}")

            # –ü–æ–ª—É—á–∞–µ–º —á–∞–Ω–∫–∏ –∏ embeddings
            chunks = kwargs.get('chunks', [])
            embeddings = kwargs.get('embeddings', [])

            if not chunks:
                raise AnalysisError(
                    "–ê–Ω–∞–ª–∏–∑ –≤–æ–¥—ã —Ç—Ä–µ–±—É–µ—Ç —á–∞–Ω–∫–∏ —Ç–µ–∫—Å—Ç–∞",
                    details={"text_id": text.id}
                )

            if embeddings and len(embeddings) > 0:
                # –ò—Å–ø–æ–ª—å–∑—É–µ–º embeddings –¥–ª—è —Ç–æ—á–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞
                water_data = self._analyze_with_embeddings(chunks, embeddings)
            else:
                # Fallback –Ω–∞ —ç–≤—Ä–∏—Å—Ç–∏–∫—É
                logger.warning("Embeddings –Ω–µ–¥–æ—Å—Ç—É–ø–Ω—ã, –∏—Å–ø–æ–ª—å–∑—É–µ–º —ç–≤—Ä–∏—Å—Ç–∏–∫—É")
                water_data = self._analyze_with_heuristics(chunks)

            logger.info(f"–ê–Ω–∞–ª–∏–∑ –≤–æ–¥—ã –∑–∞–≤–µ—Ä—à—ë–Ω: {water_data['water_percentage']:.1f}%")

            return AnalysisResult(
                analyzer_type=self.__class__.__name__,
                text_id=text.id,
                mode=mode,
                data=water_data
            )

        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –∞–Ω–∞–ª–∏–∑–∞ –≤–æ–¥—ã: {e}")
            raise AnalysisError(
                f"–ù–µ —É–¥–∞–ª–æ—Å—å –ø—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å –≤–æ–¥—É: {str(e)}",
                details={"text_id": text.id}
            )

    def _analyze_with_embeddings(
        self,
        chunks: List[Any],
        embeddings: List[np.ndarray]
    ) -> Dict[str, Any]:
        """
        –ê–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å –≤–æ–¥—É –∏—Å–ø–æ–ª—å–∑—É—è semantic embeddings

        Args:
            chunks: –°–ø–∏—Å–æ–∫ —á–∞–Ω–∫–æ–≤
            embeddings: –°–ø–∏—Å–æ–∫ –≤–µ–∫—Ç–æ—Ä–æ–≤

        Returns:
            Dict: –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –∞–Ω–∞–ª–∏–∑–∞
        """
        if len(embeddings) != len(chunks):
            logger.warning("–ù–µ—Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ embeddings, fallback")
            return self._analyze_with_heuristics(chunks)

        # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –≤ numpy array
        emb_matrix = np.array([emb for emb in embeddings])

        # 1. –í—ã—á–∏—Å–ª—è–µ–º —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫—É—é –≤–∞—Ä–∏–∞—Ç–∏–≤–Ω–æ—Å—Ç—å –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —á–∞–Ω–∫–∞
        # –ù–∏–∑–∫–∞—è –≤–∞—Ä–∏–∞—Ç–∏–≤–Ω–æ—Å—Ç—å = –Ω–∏–∑–∫–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–æ–Ω–Ω–∞—è –ø–ª–æ—Ç–Ω–æ—Å—Ç—å
        chunk_variances = []
        for emb in emb_matrix:
            variance = np.var(emb)
            chunk_variances.append(variance)

        # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º –≤–∞—Ä–∏–∞—Ç–∏–≤–Ω–æ—Å—Ç—å
        max_var = max(chunk_variances) if chunk_variances else 1.0
        normalized_variances = [v / max_var for v in chunk_variances]

        # 2. –í—ã—á–∏—Å–ª—è–µ–º –∏–∑–±—ã—Ç–æ—á–Ω–æ—Å—Ç—å (—Å—Ö–æ–∂–µ—Å—Ç—å —Å–æ—Å–µ–¥–Ω–∏—Ö —á–∞–Ω–∫–æ–≤)
        redundancy_scores = []
        for i in range(len(emb_matrix) - 1):
            similarity = self._cosine_similarity(emb_matrix[i], emb_matrix[i+1])
            redundancy_scores.append(similarity)

        avg_redundancy = sum(redundancy_scores) / len(redundancy_scores) if redundancy_scores else 0.0

        # 3. –í—ã—á–∏—Å–ª—è–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–æ–Ω–Ω—É—é –ø–ª–æ—Ç–Ω–æ—Å—Ç—å
        # –í—ã—Å–æ–∫–∞—è –≤–∞—Ä–∏–∞—Ç–∏–≤–Ω–æ—Å—Ç—å + –Ω–∏–∑–∫–∞—è –∏–∑–±—ã—Ç–æ—á–Ω–æ—Å—Ç—å = –≤—ã—Å–æ–∫–∞—è –ø–ª–æ—Ç–Ω–æ—Å—Ç—å
        info_density_scores = []
        for i, variance in enumerate(normalized_variances):
            if i < len(redundancy_scores):
                # –ù–∏–∑–∫–∞—è –∏–∑–±—ã—Ç–æ—á–Ω–æ—Å—Ç—å —Å —Å–æ—Å–µ–¥–Ω–∏–º = –±–æ–ª–µ–µ —É–Ω–∏–∫–∞–ª—å–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è
                uniqueness = 1.0 - redundancy_scores[i]
                density = (variance + uniqueness) / 2.0
            else:
                density = variance

            info_density_scores.append(density)

        # –°—Ä–µ–¥–Ω—è—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–æ–Ω–Ω–∞—è –ø–ª–æ—Ç–Ω–æ—Å—Ç—å
        avg_info_density = sum(info_density_scores) / len(info_density_scores)

        # –ü—Ä–æ—Ü–µ–Ω—Ç –≤–æ–¥—ã = –æ–±—Ä–∞—Ç–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–æ–Ω–Ω–∞—è –ø–ª–æ—Ç–Ω–æ—Å—Ç—å
        water_percentage = (1.0 - avg_info_density) * 100

        # –ù–∞—Ö–æ–¥–∏–º "–≤–æ–¥—è–Ω–∏—Å—Ç—ã–µ" —á–∞–Ω–∫–∏ (–Ω–∏–∂–Ω–∏–µ 20%)
        sorted_indices = sorted(
            range(len(info_density_scores)),
            key=lambda i: info_density_scores[i]
        )
        verbose_count = max(int(len(chunks) * 0.2), 1)
        verbose_chunks = sorted_indices[:verbose_count]

        # –†–µ–π—Ç–∏–Ω–≥
        if water_percentage < 20:
            rating = "concise"
            rating_ru = "–ª–∞–∫–æ–Ω–∏—á–Ω—ã–π"
            rating_emoji = "‚ú®"
        elif water_percentage < 50:
            rating = "balanced"
            rating_ru = "—Å–±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∞–Ω–Ω—ã–π"
            rating_emoji = "‚úÖ"
        else:
            rating = "verbose"
            rating_ru = "–º–Ω–æ–≥–æ—Å–ª–æ–≤–Ω—ã–π"
            rating_emoji = "üíß"

        return {
            "water_percentage": round(water_percentage, 2),
            "info_density": round(avg_info_density, 2),
            "rating": rating,
            "rating_ru": rating_ru,
            "rating_emoji": rating_emoji,
            "verbose_chunks": verbose_chunks,
            "avg_redundancy": round(avg_redundancy, 2)
        }

    def _analyze_with_heuristics(self, chunks: List[Any]) -> Dict[str, Any]:
        """
        –ê–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å –≤–æ–¥—É –∏—Å–ø–æ–ª—å–∑—É—è —ç–≤—Ä–∏—Å—Ç–∏–∫—É

        Args:
            chunks: –°–ø–∏—Å–æ–∫ —á–∞–Ω–∫–æ–≤

        Returns:
            Dict: –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –∞–Ω–∞–ª–∏–∑–∞
        """
        # –ü—Ä–æ—Å—Ç–∞—è —ç–≤—Ä–∏—Å—Ç–∏–∫–∞: –ø–æ–≤—Ç–æ—Ä—è—é—â–∏–µ—Å—è —Å–ª–æ–≤–∞
        word_frequencies = {}
        total_words = 0

        for chunk in chunks:
            words = chunk.content.lower().split()
            total_words += len(words)

            for word in words:
                if len(word) > 3:  # –ò–≥–Ω–æ—Ä–∏—Ä—É–µ–º –∫–æ—Ä–æ—Ç–∫–∏–µ —Å–ª–æ–≤–∞
                    word_frequencies[word] = word_frequencies.get(word, 0) + 1

        # –í—ã—á–∏—Å–ª—è–µ–º –ø–æ–≤—Ç–æ—Ä—è–µ–º–æ—Å—Ç—å
        if total_words == 0:
            water_percentage = 50.0  # –î–µ—Ñ–æ–ª—Ç
        else:
            # –°–ª–æ–≤–∞, –ø–æ–≤—Ç–æ—Ä—è—é—â–∏–µ—Å—è >5 —Ä–∞–∑ = "–≤–æ–¥–∞"
            repetitive_words = sum(
                count for count in word_frequencies.values() if count > 5
            )
            water_percentage = min((repetitive_words / total_words) * 200, 100)

        # –†–µ–π—Ç–∏–Ω–≥
        if water_percentage < 20:
            rating = "concise"
            rating_ru = "–ª–∞–∫–æ–Ω–∏—á–Ω—ã–π"
            rating_emoji = "‚ú®"
        elif water_percentage < 50:
            rating = "balanced"
            rating_ru = "—Å–±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∞–Ω–Ω—ã–π"
            rating_emoji = "‚úÖ"
        else:
            rating = "verbose"
            rating_ru = "–º–Ω–æ–≥–æ—Å–ª–æ–≤–Ω—ã–π"
            rating_emoji = "üíß"

        return {
            "water_percentage": round(water_percentage, 2),
            "info_density": round(1.0 - (water_percentage / 100), 2),
            "rating": rating,
            "rating_ru": rating_ru,
            "rating_emoji": rating_emoji,
            "verbose_chunks": [],
            "avg_redundancy": 0.0,
            "method": "heuristic"
        }

    def _cosine_similarity(self, vec1: np.ndarray, vec2: np.ndarray) -> float:
        """
        –í—ã—á–∏—Å–ª–∏—Ç—å –∫–æ—Å–∏–Ω—É—Å–Ω–æ–µ —Å—Ö–æ–¥—Å—Ç–≤–æ

        Args:
            vec1: –ü–µ—Ä–≤—ã–π –≤–µ–∫—Ç–æ—Ä
            vec2: –í—Ç–æ—Ä–æ–π –≤–µ–∫—Ç–æ—Ä

        Returns:
            float: –ö–æ—Å–∏–Ω—É—Å–Ω–æ–µ —Å—Ö–æ–¥—Å—Ç–≤–æ (0-1)
        """
        dot_product = np.dot(vec1, vec2)
        norm1 = np.linalg.norm(vec1)
        norm2 = np.linalg.norm(vec2)

        if norm1 == 0 or norm2 == 0:
            return 0.0

        return dot_product / (norm1 * norm2)

    def interpret_results(self, result: AnalysisResult) -> str:
        """
        –ò–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∏—Ä–æ–≤–∞—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –∞–Ω–∞–ª–∏–∑–∞

        Args:
            result: –†–µ–∑—É–ª—å—Ç–∞—Ç –∞–Ω–∞–ª–∏–∑–∞

        Returns:
            str: –ß–µ–ª–æ–≤–µ–∫–æ—á–∏—Ç–∞–µ–º–∞—è –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏—è
        """
        data = result.data
        water_percent = data.get('water_percentage', 0)
        info_density = data.get('info_density', 0)
        rating_ru = data.get('rating_ru', '–Ω–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–π')
        rating_emoji = data.get('rating_emoji', 'üíß')

        lines = [
            f"üíß **–ê–Ω–∞–ª–∏–∑ "–≤–æ–¥—ã" –≤ —Ç–µ–∫—Å—Ç–µ**\n",
            f"{rating_emoji} **–£—Ä–æ–≤–µ–Ω—å –≤–æ–¥—ã**: {water_percent:.1f}%",
            f"üìä **–ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏–æ–Ω–Ω–∞—è –ø–ª–æ—Ç–Ω–æ—Å—Ç—å**: {info_density:.2f}",
            f"‚≠ê **–†–µ–π—Ç–∏–Ω–≥**: {rating_ru}\n"
        ]

        # –ò–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏—è
        if water_percent < 20:
            interpretation = (
                "–û—á–µ–Ω—å –ª–∞–∫–æ–Ω–∏—á–Ω—ã–π —Ç–µ–∫—Å—Ç. –í—ã—Å–æ–∫–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–æ–Ω–Ω–∞—è –ø–ª–æ—Ç–Ω–æ—Å—Ç—å. "
                "–ö–∞–∂–¥–æ–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ –Ω–µ—Å—ë—Ç —Å–º—ã—Å–ª. –û—Ç–ª–∏—á–Ω—ã–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç!"
            )
        elif water_percent < 35:
            interpretation = (
                "–•–æ—Ä–æ—à–∏–π –±–∞–ª–∞–Ω—Å –º–µ–∂–¥—É –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–µ–π –∏ —á–∏—Ç–∞–±–µ–ª—å–Ω–æ—Å—Ç—å—é. "
                "–£–º–µ—Ä–µ–Ω–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–æ–≤—Ç–æ—Ä–æ–≤ –∏ –æ–ø–∏—Å–∞–Ω–∏–π."
            )
        elif water_percent < 50:
            interpretation = (
                "–°—Ä–µ–¥–Ω–∏–π —É—Ä–æ–≤–µ–Ω—å –≤–æ–¥—ã. –ï—Å—Ç—å –ø–æ–≤—Ç–æ—Ä—ã –∏ –∏–∑–±—ã—Ç–æ—á–Ω—ã–µ –æ–ø–∏—Å–∞–Ω–∏—è, "
                "–Ω–æ –æ–Ω–∏ –Ω–µ –∫—Ä–∏—Ç–∏—á–Ω—ã."
            )
        elif water_percent < 70:
            interpretation = (
                "–ú–Ω–æ–≥–æ "–≤–æ–¥—ã". –ó–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–æ–≤—Ç–æ—Ä–æ–≤, –∏–∑–±—ã—Ç–æ—á–Ω—ã—Ö "
                "–æ–ø–∏—Å–∞–Ω–∏–π –∏ –º–∞–ª–æ–∏–Ω—Ñ–æ—Ä–º–∞—Ç–∏–≤–Ω—ã—Ö —Ñ—Ä–∞–≥–º–µ–Ω—Ç–æ–≤. –†–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è —Ä–µ–¥–∞–∫—Ç—É—Ä–∞."
            )
        else:
            interpretation = (
                "–û—á–µ–Ω—å –≤—ã—Å–æ–∫–∏–π —É—Ä–æ–≤–µ–Ω—å –≤–æ–¥—ã. –¢–µ–∫—Å—Ç –ø–µ—Ä–µ–≥—Ä—É–∂–µ–Ω –ø–æ–≤—Ç–æ—Ä–∞–º–∏ "
                "–∏ –∏–∑–±—ã—Ç–æ—á–Ω—ã–º–∏ –æ–ø–∏—Å–∞–Ω–∏—è–º–∏. –¢—Ä–µ–±—É–µ—Ç—Å—è —Å–µ—Ä—å—ë–∑–Ω–∞—è —Ä–µ–¥–∞–∫—Ç—É—Ä–∞."
            )

        lines.append(f"üí° **–ò–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏—è**: {interpretation}")

        return '\n'.join(lines)
