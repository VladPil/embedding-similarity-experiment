"""
ÐÐ½Ð°Ð»Ð¸Ð·Ð°Ñ‚Ð¾Ñ€ Ð¿ÐµÑ€ÑÐ¾Ð½Ð°Ð¶ÐµÐ¹
"""
import json
import re
from typing import Dict, Any, List, Optional
from loguru import logger

from ..entities.base_analyzer import BaseAnalyzer
from ..entities.analysis_result import AnalysisResult
from ..entities.prompt_template import PromptTemplate
from ..helpers.chunk_indexer import ChunkIndexer
from src.text_domain.entities.base_text import BaseText
from src.common.types import AnalysisMode
from src.common.exceptions import AnalysisError


class CharacterAnalyzer(BaseAnalyzer):
    """
    ÐÐ½Ð°Ð»Ð¸Ð·Ð°Ñ‚Ð¾Ñ€ Ð¿ÐµÑ€ÑÐ¾Ð½Ð°Ð¶ÐµÐ¹ Ð¿Ñ€Ð¾Ð¸Ð·Ð²ÐµÐ´ÐµÐ½Ð¸Ñ

    Ð˜ÑÐ¿Ð¾Ð»ÑŒÐ·ÑƒÐµÑ‚:
    - ChunkIndexer Ð´Ð»Ñ Ð¾Ñ‚Ð±Ð¾Ñ€Ð° Ñ€ÐµÐ»ÐµÐ²Ð°Ð½Ñ‚Ð½Ñ‹Ñ… Ñ‡Ð°Ð½ÐºÐ¾Ð² (Ð´Ð¸Ð°Ð»Ð¾Ð³Ð¸, Ð¸Ð¼ÐµÐ½Ð°)
    - LLM Ð´Ð»Ñ Ð¸Ð·Ð²Ð»ÐµÑ‡ÐµÐ½Ð¸Ñ Ð¿ÐµÑ€ÑÐ¾Ð½Ð°Ð¶ÐµÐ¹ Ð¸ Ð¸Ñ… Ñ‡ÐµÑ€Ñ‚
    - ÐÐ³Ñ€ÐµÐ³Ð°Ñ†Ð¸ÑŽ ÑƒÐ¿Ð¾Ð¼Ð¸Ð½Ð°Ð½Ð¸Ð¹ Ð¿ÐµÑ€ÑÐ¾Ð½Ð°Ð¶ÐµÐ¹

    Ð’Ð¾Ð·Ð²Ñ€Ð°Ñ‰Ð°ÐµÑ‚:
    - Ð¡Ð¿Ð¸ÑÐ¾Ðº Ð¿ÐµÑ€ÑÐ¾Ð½Ð°Ð¶ÐµÐ¹ Ñ Ñ€Ð¾Ð»ÑÐ¼Ð¸ (main/secondary/episodic)
    - Ð§ÐµÑ€Ñ‚Ñ‹ Ñ…Ð°Ñ€Ð°ÐºÑ‚ÐµÑ€Ð°
    - Timeline Ð¿Ð¾ÑÐ²Ð»ÐµÐ½Ð¸Ð¹
    - Development timeline
    """

    def __init__(
        self,
        llm_service: Optional[Any] = None,
        prompt_template: Optional[PromptTemplate] = None,
        max_chunks_to_analyze: int = 30
    ):
        """
        Ð˜Ð½Ð¸Ñ†Ð¸Ð°Ð»Ð¸Ð·Ð°Ñ†Ð¸Ñ Ð°Ð½Ð°Ð»Ð¸Ð·Ð°Ñ‚Ð¾Ñ€Ð° Ð¿ÐµÑ€ÑÐ¾Ð½Ð°Ð¶ÐµÐ¹

        Args:
            llm_service: Ð¡ÐµÑ€Ð²Ð¸Ñ Ð´Ð»Ñ Ñ€Ð°Ð±Ð¾Ñ‚Ñ‹ Ñ LLM
            prompt_template: Ð¨Ð°Ð±Ð»Ð¾Ð½ Ð¿Ñ€Ð¾Ð¼Ð¿Ñ‚Ð°
            max_chunks_to_analyze: ÐœÐ°ÐºÑÐ¸Ð¼Ð°Ð»ÑŒÐ½Ð¾Ðµ ÐºÐ¾Ð»Ð¸Ñ‡ÐµÑÑ‚Ð²Ð¾ Ñ‡Ð°Ð½ÐºÐ¾Ð² Ð´Ð»Ñ Ð°Ð½Ð°Ð»Ð¸Ð·Ð°
        """
        self.llm_service = llm_service
        self.prompt_template = prompt_template or self._create_default_prompt()
        self.max_chunks_to_analyze = max_chunks_to_analyze
        self.indexer = ChunkIndexer()

    @property
    def requires_llm(self) -> bool:
        """Ð¢Ñ€ÐµÐ±ÑƒÐµÑ‚ÑÑ LLM"""
        return True

    @property
    def requires_embeddings(self) -> bool:
        """ÐÐµ Ñ‚Ñ€ÐµÐ±ÑƒÑŽÑ‚ÑÑ embeddings"""
        return False

    async def analyze(
        self,
        text: BaseText,
        mode: AnalysisMode,
        **kwargs
    ) -> AnalysisResult:
        """
        Ð’Ñ‹Ð¿Ð¾Ð»Ð½Ð¸Ñ‚ÑŒ Ð°Ð½Ð°Ð»Ð¸Ð· Ð¿ÐµÑ€ÑÐ¾Ð½Ð°Ð¶ÐµÐ¹

        Args:
            text: Ð¢ÐµÐºÑÑ‚ Ð´Ð»Ñ Ð°Ð½Ð°Ð»Ð¸Ð·Ð°
            mode: Ð ÐµÐ¶Ð¸Ð¼ Ð°Ð½Ð°Ð»Ð¸Ð·Ð°
            **kwargs: Ð”Ð¾Ð¿Ð¾Ð»Ð½Ð¸Ñ‚ÐµÐ»ÑŒÐ½Ñ‹Ðµ Ð¿Ð°Ñ€Ð°Ð¼ÐµÑ‚Ñ€Ñ‹

        Returns:
            AnalysisResult: Ð ÐµÐ·ÑƒÐ»ÑŒÑ‚Ð°Ñ‚ Ð°Ð½Ð°Ð»Ð¸Ð·Ð°
        """
        try:
            logger.info(f"ÐÐ°Ñ‡Ð°Ð»Ð¾ Ð°Ð½Ð°Ð»Ð¸Ð·Ð° Ð¿ÐµÑ€ÑÐ¾Ð½Ð°Ð¶ÐµÐ¹: {text.title}")

            # ÐŸÐ¾Ð»ÑƒÑ‡Ð°ÐµÐ¼ Ñ‡Ð°Ð½ÐºÐ¸
            chunks = kwargs.get('chunks', [])
            if not chunks:
                raise AnalysisError(
                    "ÐÐ½Ð°Ð»Ð¸Ð· Ð¿ÐµÑ€ÑÐ¾Ð½Ð°Ð¶ÐµÐ¹ Ñ‚Ñ€ÐµÐ±ÑƒÐµÑ‚ Ñ‡Ð°Ð½ÐºÐ¸ Ñ‚ÐµÐºÑÑ‚Ð°",
                    details={"text_id": text.id}
                )

            # Ð¡Ñ‚Ñ€Ð¾Ð¸Ð¼ Ð¸Ð½Ð´ÐµÐºÑ Ð¿ÐµÑ€ÑÐ¾Ð½Ð°Ð¶Ð½Ñ‹Ñ… Ñ‡Ð°Ð½ÐºÐ¾Ð²
            char_index = self.indexer.build_character_index(chunks)

            logger.info(
                f"Ð˜Ð½Ð´ÐµÐºÑ Ð¿ÐµÑ€ÑÐ¾Ð½Ð°Ð¶ÐµÐ¹: {len(char_index.chunk_indices)} Ñ‡Ð°Ð½ÐºÐ¾Ð² "
                f"({char_index.coverage*100:.1f}% Ð¾Ñ…Ð²Ð°Ñ‚)"
            )

            # ÐŸÐ¾Ð»ÑƒÑ‡Ð°ÐµÐ¼ Ñ€ÐµÐ»ÐµÐ²Ð°Ð½Ñ‚Ð½Ñ‹Ðµ Ñ‡Ð°Ð½ÐºÐ¸
            relevant_chunks = self.indexer.get_chunk_subset(chunks, 'characters')

            if not relevant_chunks:
                logger.warning("ÐÐµ Ð½Ð°Ð¹Ð´ÐµÐ½Ð¾ Ð¿ÐµÑ€ÑÐ¾Ð½Ð°Ð¶Ð½Ñ‹Ñ… Ñ‡Ð°Ð½ÐºÐ¾Ð²")
                return AnalysisResult(
                    analyzer_type=self.__class__.__name__,
                    text_id=text.id,
                    mode=mode,
                    data={
                        "characters": [],
                        "total_characters": 0,
                        "chunks_analyzed": 0,
                        "coverage": 0.0
                    }
                )

            # ÐÐ½Ð°Ð»Ð¸Ð·Ð¸Ñ€ÑƒÐµÐ¼ Ñ‡Ð°Ð½ÐºÐ¸ Ñ LLM
            character_mentions = await self._analyze_chunks_with_llm(relevant_chunks)

            # ÐÐ³Ñ€ÐµÐ³Ð¸Ñ€ÑƒÐµÐ¼ Ð¿ÐµÑ€ÑÐ¾Ð½Ð°Ð¶ÐµÐ¹
            characters = self._aggregate_characters(character_mentions)

            logger.info(f"ÐÐ°Ð¹Ð´ÐµÐ½Ð¾ {len(characters)} Ð¿ÐµÑ€ÑÐ¾Ð½Ð°Ð¶ÐµÐ¹")

            return AnalysisResult(
                analyzer_type=self.__class__.__name__,
                text_id=text.id,
                mode=mode,
                data={
                    "characters": characters,
                    "total_characters": len(characters),
                    "chunks_analyzed": min(len(relevant_chunks), self.max_chunks_to_analyze),
                    "coverage": char_index.coverage,
                    "character_mentions": len(character_mentions)
                }
            )

        except Exception as e:
            logger.error(f"ÐžÑˆÐ¸Ð±ÐºÐ° Ð°Ð½Ð°Ð»Ð¸Ð·Ð° Ð¿ÐµÑ€ÑÐ¾Ð½Ð°Ð¶ÐµÐ¹: {e}")
            raise AnalysisError(
                f"ÐÐµ ÑƒÐ´Ð°Ð»Ð¾ÑÑŒ Ð¿Ñ€Ð¾Ð°Ð½Ð°Ð»Ð¸Ð·Ð¸Ñ€Ð¾Ð²Ð°Ñ‚ÑŒ Ð¿ÐµÑ€ÑÐ¾Ð½Ð°Ð¶ÐµÐ¹: {str(e)}",
                details={"text_id": text.id}
            )

    async def _analyze_chunks_with_llm(
        self,
        chunks: List[Any]
    ) -> List[Dict[str, Any]]:
        """
        ÐÐ½Ð°Ð»Ð¸Ð·Ð¸Ñ€Ð¾Ð²Ð°Ñ‚ÑŒ Ñ‡Ð°Ð½ÐºÐ¸ Ñ Ð¿Ð¾Ð¼Ð¾Ñ‰ÑŒÑŽ LLM

        Args:
            chunks: Ð¡Ð¿Ð¸ÑÐ¾Ðº Ñ‡Ð°Ð½ÐºÐ¾Ð²

        Returns:
            List[Dict]: Ð¡Ð¿Ð¸ÑÐ¾Ðº ÑƒÐ¿Ð¾Ð¼Ð¸Ð½Ð°Ð½Ð¸Ð¹ Ð¿ÐµÑ€ÑÐ¾Ð½Ð°Ð¶ÐµÐ¹
        """
        character_mentions = []

        # ÐžÐ³Ñ€Ð°Ð½Ð¸Ñ‡Ð¸Ð²Ð°ÐµÐ¼ ÐºÐ¾Ð»Ð¸Ñ‡ÐµÑÑ‚Ð²Ð¾ Ñ‡Ð°Ð½ÐºÐ¾Ð²
        chunks_to_process = chunks[:self.max_chunks_to_analyze]

        for i, chunk in enumerate(chunks_to_process):
            try:
                # ÐšÐ¾Ð½Ñ‚ÐµÐºÑÑ‚ (Ð¿Ñ€ÐµÐ´Ñ‹Ð´ÑƒÑ‰Ð¸Ð¹ Ñ‡Ð°Ð½Ðº)
                context_text = ""
                if i > 0:
                    context_text = chunks_to_process[i-1].content[:500]

                # Ð¤Ð¾Ñ€Ð¼Ð¸Ñ€ÑƒÐµÐ¼ Ð¿Ñ€Ð¾Ð¼Ð¿Ñ‚
                prompt = self._format_character_prompt(chunk.content, context_text)

                # LLM Ð·Ð°Ð¿Ñ€Ð¾Ñ
                result = await self.llm_service.generate(
                    prompt=prompt,
                    max_tokens=512,
                    temperature=0.3
                )

                # ÐŸÐ°Ñ€ÑÐ¸Ð¼ Ð¾Ñ‚Ð²ÐµÑ‚
                char_data = self._parse_character_response(result)

                if char_data and char_data.get('name'):
                    char_data['chunk_index'] = chunk.index
                    char_data['position'] = chunk.metadata.get('position_ratio', 0.0)
                    character_mentions.append(char_data)

            except Exception as e:
                logger.warning(f"ÐžÑˆÐ¸Ð±ÐºÐ° Ð°Ð½Ð°Ð»Ð¸Ð·Ð° Ñ‡Ð°Ð½ÐºÐ° {chunk.index}: {e}")
                continue

        return character_mentions

    def _format_character_prompt(self, chunk_text: str, context: str = "") -> str:
        """
        Ð¤Ð¾Ñ€Ð¼Ð°Ñ‚Ð¸Ñ€Ð¾Ð²Ð°Ñ‚ÑŒ Ð¿Ñ€Ð¾Ð¼Ð¿Ñ‚ Ð´Ð»Ñ Ð°Ð½Ð°Ð»Ð¸Ð·Ð° Ð¿ÐµÑ€ÑÐ¾Ð½Ð°Ð¶ÐµÐ¹

        Args:
            chunk_text: Ð¢ÐµÐºÑÑ‚ Ñ‡Ð°Ð½ÐºÐ°
            context: ÐšÐ¾Ð½Ñ‚ÐµÐºÑÑ‚ (Ð¿Ñ€ÐµÐ´Ñ‹Ð´ÑƒÑ‰Ð¸Ð¹ Ñ‡Ð°Ð½Ðº)

        Returns:
            str: ÐžÑ‚Ñ„Ð¾Ñ€Ð¼Ð°Ñ‚Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð½Ñ‹Ð¹ Ð¿Ñ€Ð¾Ð¼Ð¿Ñ‚
        """
        prompt = f"""ÐŸÑ€Ð¾Ð°Ð½Ð°Ð»Ð¸Ð·Ð¸Ñ€ÑƒÐ¹ Ñ„Ñ€Ð°Ð³Ð¼ÐµÐ½Ñ‚ Ñ‚ÐµÐºÑÑ‚Ð° Ð¸ Ð¸Ð·Ð²Ð»ÐµÐºÐ¸ Ð¸Ð½Ñ„Ð¾Ñ€Ð¼Ð°Ñ†Ð¸ÑŽ Ð¾ Ð¿ÐµÑ€ÑÐ¾Ð½Ð°Ð¶Ð°Ñ….

ÐšÐ¾Ð½Ñ‚ÐµÐºÑÑ‚ (Ð¿Ñ€ÐµÐ´Ñ‹Ð´ÑƒÑ‰Ð¸Ð¹ Ñ„Ñ€Ð°Ð³Ð¼ÐµÐ½Ñ‚):
{context if context else "ÐÐµÑ‚ ÐºÐ¾Ð½Ñ‚ÐµÐºÑÑ‚Ð°"}

Ð¤Ñ€Ð°Ð³Ð¼ÐµÐ½Ñ‚ Ð´Ð»Ñ Ð°Ð½Ð°Ð»Ð¸Ð·Ð°:
{chunk_text[:1500]}

Ð—Ð°Ð´Ð°Ñ‡Ð°:
1. ÐÐ°Ð¹Ð´Ð¸ ÐžÐ”ÐÐžÐ“Ðž Ð½Ð°Ð¸Ð±Ð¾Ð»ÐµÐµ Ð²Ð°Ð¶Ð½Ð¾Ð³Ð¾ Ð¿ÐµÑ€ÑÐ¾Ð½Ð°Ð¶Ð° Ð² ÑÑ‚Ð¾Ð¼ Ñ„Ñ€Ð°Ð³Ð¼ÐµÐ½Ñ‚Ðµ
2. ÐžÐ¿Ñ€ÐµÐ´ÐµÐ»Ð¸ ÐµÐ³Ð¾ Ñ€Ð¾Ð»ÑŒ: main (Ð³Ð»Ð°Ð²Ð½Ñ‹Ð¹), secondary (Ð²Ñ‚Ð¾Ñ€Ð¾ÑÑ‚ÐµÐ¿ÐµÐ½Ð½Ñ‹Ð¹), episodic (ÑÐ¿Ð¸Ð·Ð¾Ð´Ð¸Ñ‡ÐµÑÐºÐ¸Ð¹)
3. Ð˜Ð·Ð²Ð»ÐµÐºÐ¸ 2-3 Ñ‡ÐµÑ€Ñ‚Ñ‹ Ñ…Ð°Ñ€Ð°ÐºÑ‚ÐµÑ€Ð° Ñ Ð´Ð¾ÐºÐ°Ð·Ð°Ñ‚ÐµÐ»ÑŒÑÑ‚Ð²Ð°Ð¼Ð¸ Ð¸Ð· Ñ‚ÐµÐºÑÑ‚Ð°

ÐžÑ‚Ð²ÐµÑ‚ÑŒ Ð¡Ð¢Ð ÐžÐ“Ðž Ð² Ñ„Ð¾Ñ€Ð¼Ð°Ñ‚Ðµ JSON:
{{
    "name": "Ð˜Ð¼Ñ ÐŸÐµÑ€ÑÐ¾Ð½Ð°Ð¶Ð°",
    "role": "main|secondary|episodic",
    "traits": [
        {{"trait": "Ñ‡ÐµÑ€Ñ‚Ð° Ñ…Ð°Ñ€Ð°ÐºÑ‚ÐµÑ€Ð°", "evidence": "Ñ†Ð¸Ñ‚Ð°Ñ‚Ð° Ð¸Ð· Ñ‚ÐµÐºÑÑ‚Ð°"}},
        ...
    ]
}}

JSON:"""

        return prompt

    def _parse_character_response(self, llm_response: str) -> Dict[str, Any]:
        """
        Ð Ð°ÑÐ¿Ð°Ñ€ÑÐ¸Ñ‚ÑŒ Ð¾Ñ‚Ð²ÐµÑ‚ LLM

        Args:
            llm_response: ÐžÑ‚Ð²ÐµÑ‚ LLM

        Returns:
            Dict: Ð”Ð°Ð½Ð½Ñ‹Ðµ Ð¿ÐµÑ€ÑÐ¾Ð½Ð°Ð¶Ð°
        """
        try:
            # Ð˜Ð·Ð²Ð»ÐµÐºÐ°ÐµÐ¼ JSON
            json_match = re.search(
                r'\{[^{}]*(?:\{[^{}]*\}[^{}]*)*\}',
                llm_response,
                re.DOTALL
            )

            if json_match:
                parsed = json.loads(json_match.group(0))
                return parsed

            return {}

        except Exception as e:
            logger.warning(f"ÐžÑˆÐ¸Ð±ÐºÐ° Ð¿Ð°Ñ€ÑÐ¸Ð½Ð³Ð° Ð¾Ñ‚Ð²ÐµÑ‚Ð° Ð¿ÐµÑ€ÑÐ¾Ð½Ð°Ð¶Ð°: {e}")
            return {}

    def _aggregate_characters(self, mentions: List[Dict]) -> List[Dict]:
        """
        ÐÐ³Ñ€ÐµÐ³Ð¸Ñ€Ð¾Ð²Ð°Ñ‚ÑŒ ÑƒÐ¿Ð¾Ð¼Ð¸Ð½Ð°Ð½Ð¸Ñ Ð¿ÐµÑ€ÑÐ¾Ð½Ð°Ð¶ÐµÐ¹

        ÐžÐ±ÑŠÐµÐ´Ð¸Ð½ÑÐµÑ‚ Ð½ÐµÑÐºÐ¾Ð»ÑŒÐºÐ¾ ÑƒÐ¿Ð¾Ð¼Ð¸Ð½Ð°Ð½Ð¸Ð¹ Ð¾Ð´Ð½Ð¾Ð³Ð¾ Ð¿ÐµÑ€ÑÐ¾Ð½Ð°Ð¶Ð° Ð² ÐµÐ´Ð¸Ð½Ñ‹Ð¹ Ð¿Ñ€Ð¾Ñ„Ð¸Ð»ÑŒ

        Args:
            mentions: Ð¡Ð¿Ð¸ÑÐ¾Ðº ÑƒÐ¿Ð¾Ð¼Ð¸Ð½Ð°Ð½Ð¸Ð¹

        Returns:
            List[Dict]: Ð¡Ð¿Ð¸ÑÐ¾Ðº Ð°Ð³Ñ€ÐµÐ³Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð½Ñ‹Ñ… Ð¿ÐµÑ€ÑÐ¾Ð½Ð°Ð¶ÐµÐ¹
        """
        character_map = {}

        for mention in mentions:
            name = mention.get('name', '').strip()
            if not name or name in ["Ð˜Ð¼Ñ ÐŸÐµÑ€ÑÐ¾Ð½Ð°Ð¶Ð°", "ÐÐµÐ¸Ð·Ð²ÐµÑÑ‚Ð½Ñ‹Ð¹", "Unknown"]:
                continue

            # ÐÐ¾Ñ€Ð¼Ð°Ð»Ð¸Ð·ÑƒÐµÐ¼ Ð¸Ð¼Ñ
            name_key = name.lower()

            if name_key not in character_map:
                character_map[name_key] = {
                    "name": name,
                    "traits": [],
                    "role": mention.get('role', 'episodic'),
                    "appearances": [],
                    "first_appearance": mention.get('position', 0.0)
                }

            char = character_map[name_key]

            # Ð”Ð¾Ð±Ð°Ð²Ð»ÑÐµÐ¼ Ñ‡ÐµÑ€Ñ‚Ñ‹
            if 'traits' in mention:
                for trait in mention['traits']:
                    if isinstance(trait, dict):
                        char['traits'].append(trait)
                    else:
                        char['traits'].append({"trait": str(trait), "evidence": ""})

            # Ð”Ð¾Ð±Ð°Ð²Ð»ÑÐµÐ¼ Ð¿Ð¾ÑÐ²Ð»ÐµÐ½Ð¸Ðµ
            char['appearances'].append({
                "position": mention.get('position', 0.0),
                "chunk_index": mention.get('chunk_index', 0)
            })

            # ÐžÐ±Ð½Ð¾Ð²Ð»ÑÐµÐ¼ Ñ€Ð¾Ð»ÑŒ (main > secondary > episodic)
            if mention.get('role') == 'main':
                char['role'] = 'main'
            elif mention.get('role') == 'secondary' and char['role'] != 'main':
                char['role'] = 'secondary'

        # ÐšÐ¾Ð½Ð²ÐµÑ€Ñ‚Ð¸Ñ€ÑƒÐµÐ¼ Ð² ÑÐ¿Ð¸ÑÐ¾Ðº
        characters = []
        for char in character_map.values():
            # Ð”ÐµÐ´ÑƒÐ¿Ð»Ð¸ÐºÐ°Ñ†Ð¸Ñ Ñ‡ÐµÑ€Ñ‚
            unique_traits = {}
            for trait_dict in char['traits']:
                trait_name = trait_dict.get('trait', '').lower()
                if trait_name and trait_name not in unique_traits:
                    unique_traits[trait_name] = trait_dict

            char['traits'] = list(unique_traits.values())[:5]  # Ð¢Ð¾Ð¿-5 Ñ‡ÐµÑ€Ñ‚

            # Ð¡Ð¾Ñ€Ñ‚Ð¸Ñ€ÑƒÐµÐ¼ Ð¿Ð¾ÑÐ²Ð»ÐµÐ½Ð¸Ñ
            char['appearances'].sort(key=lambda x: x['position'])

            # Timeline Ñ€Ð°Ð·Ð²Ð¸Ñ‚Ð¸Ñ
            if len(char['appearances']) > 1:
                char['development_timeline'] = [
                    {
                        "position": char['first_appearance'],
                        "description": "ÐŸÐµÑ€Ð²Ð¾Ðµ Ð¿Ð¾ÑÐ²Ð»ÐµÐ½Ð¸Ðµ"
                    },
                    {
                        "position": char['appearances'][-1]['position'],
                        "description": "ÐŸÐ¾ÑÐ»ÐµÐ´Ð½ÐµÐµ Ð¿Ð¾ÑÐ²Ð»ÐµÐ½Ð¸Ðµ"
                    }
                ]
            else:
                char['development_timeline'] = []

            characters.append(char)

        # Ð¡Ð¾Ñ€Ñ‚Ð¸Ñ€ÑƒÐµÐ¼ Ð¿Ð¾ Ð²Ð°Ð¶Ð½Ð¾ÑÑ‚Ð¸
        role_order = {'main': 0, 'secondary': 1, 'episodic': 2}
        characters.sort(key=lambda x: (
            role_order.get(x['role'], 3),
            -len(x['appearances'])
        ))

        return characters

    def interpret_results(self, result: AnalysisResult) -> str:
        """
        Ð˜Ð½Ñ‚ÐµÑ€Ð¿Ñ€ÐµÑ‚Ð¸Ñ€Ð¾Ð²Ð°Ñ‚ÑŒ Ñ€ÐµÐ·ÑƒÐ»ÑŒÑ‚Ð°Ñ‚Ñ‹ Ð°Ð½Ð°Ð»Ð¸Ð·Ð°

        Args:
            result: Ð ÐµÐ·ÑƒÐ»ÑŒÑ‚Ð°Ñ‚ Ð°Ð½Ð°Ð»Ð¸Ð·Ð°

        Returns:
            str: Ð§ÐµÐ»Ð¾Ð²ÐµÐºÐ¾Ñ‡Ð¸Ñ‚Ð°ÐµÐ¼Ð°Ñ Ð¸Ð½Ñ‚ÐµÑ€Ð¿Ñ€ÐµÑ‚Ð°Ñ†Ð¸Ñ
        """
        data = result.data
        characters = data.get('characters', [])
        total = data.get('total_characters', 0)
        coverage = data.get('coverage', 0)

        if not characters:
            return "ðŸ‘¥ ÐŸÐµÑ€ÑÐ¾Ð½Ð°Ð¶Ð¸ Ð½Ðµ Ð¾Ð±Ð½Ð°Ñ€ÑƒÐ¶ÐµÐ½Ñ‹ Ð² Ñ‚ÐµÐºÑÑ‚Ðµ."

        # Ð“Ñ€ÑƒÐ¿Ð¿Ð¸Ñ€ÑƒÐµÐ¼ Ð¿Ð¾ Ñ€Ð¾Ð»ÑÐ¼
        main_chars = [c for c in characters if c.get('role') == 'main']
        secondary_chars = [c for c in characters if c.get('role') == 'secondary']
        episodic_chars = [c for c in characters if c.get('role') == 'episodic']

        lines = [
            f"ðŸ‘¥ **ÐÐ½Ð°Ð»Ð¸Ð· Ð¿ÐµÑ€ÑÐ¾Ð½Ð°Ð¶ÐµÐ¹ Ð¿Ñ€Ð¾Ð¸Ð·Ð²ÐµÐ´ÐµÐ½Ð¸Ñ**\n",
            f"ðŸ“Š ÐžÐ±Ð½Ð°Ñ€ÑƒÐ¶ÐµÐ½Ð¾ Ð¿ÐµÑ€ÑÐ¾Ð½Ð°Ð¶ÐµÐ¹: {total} (Ð¾Ñ…Ð²Ð°Ñ‚: {coverage*100:.1f}%)\n"
        ]

        # Ð“Ð»Ð°Ð²Ð½Ñ‹Ðµ Ð¿ÐµÑ€ÑÐ¾Ð½Ð°Ð¶Ð¸
        if main_chars:
            lines.append(f"â­ **Ð“Ð»Ð°Ð²Ð½Ñ‹Ðµ Ð¿ÐµÑ€ÑÐ¾Ð½Ð°Ð¶Ð¸** ({len(main_chars)}):")
            for char in main_chars[:5]:
                name = char.get('name', 'ÐÐµÐ¸Ð·Ð²ÐµÑÑ‚Ð½Ñ‹Ð¹')
                traits = char.get('traits', [])
                appearances = len(char.get('appearances', []))

                trait_names = [t.get('trait', '') for t in traits[:3]]
                trait_text = ', '.join(trait_names) if trait_names else 'Ð½ÐµÑ‚ Ð´Ð°Ð½Ð½Ñ‹Ñ…'

                lines.append(
                    f"\n   **{name}**\n"
                    f"   â€¢ ÐŸÐ¾ÑÐ²Ð»ÐµÐ½Ð¸Ð¹: {appearances}\n"
                    f"   â€¢ Ð§ÐµÑ€Ñ‚Ñ‹: {trait_text}"
                )

        # Ð’Ñ‚Ð¾Ñ€Ð¾ÑÑ‚ÐµÐ¿ÐµÐ½Ð½Ñ‹Ðµ
        if secondary_chars:
            lines.append(f"\nðŸ‘¤ **Ð’Ñ‚Ð¾Ñ€Ð¾ÑÑ‚ÐµÐ¿ÐµÐ½Ð½Ñ‹Ðµ Ð¿ÐµÑ€ÑÐ¾Ð½Ð°Ð¶Ð¸** ({len(secondary_chars)}):")
            for char in secondary_chars[:3]:
                name = char.get('name', 'ÐÐµÐ¸Ð·Ð²ÐµÑÑ‚Ð½Ñ‹Ð¹')
                appearances = len(char.get('appearances', []))
                lines.append(f"   â€¢ {name} ({appearances} Ð¿Ð¾ÑÐ²Ð»ÐµÐ½Ð¸Ð¹)")

        # Ð­Ð¿Ð¸Ð·Ð¾Ð´Ð¸Ñ‡ÐµÑÐºÐ¸Ðµ
        if episodic_chars:
            lines.append(f"\nðŸ‘¥ **Ð­Ð¿Ð¸Ð·Ð¾Ð´Ð¸Ñ‡ÐµÑÐºÐ¸Ðµ Ð¿ÐµÑ€ÑÐ¾Ð½Ð°Ð¶Ð¸**: {len(episodic_chars)}")

        return '\n'.join(lines)

    def _create_default_prompt(self) -> PromptTemplate:
        """Ð¡Ð¾Ð·Ð´Ð°Ñ‚ÑŒ Ð´ÐµÑ„Ð¾Ð»Ñ‚Ð½Ñ‹Ð¹ ÑˆÐ°Ð±Ð»Ð¾Ð½ Ð¿Ñ€Ð¾Ð¼Ð¿Ñ‚Ð°"""
        return PromptTemplate.create_default_character_prompt()
