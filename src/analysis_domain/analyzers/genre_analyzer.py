"""
–ê–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä –∂–∞–Ω—Ä–∞ —Ç–µ–∫—Å—Ç–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ LLM
"""
import json
from typing import Optional, Dict, Any
from loguru import logger

from src.text_domain.entities.base_text import BaseText
from src.text_domain.entities.chunking_strategy import ChunkingStrategy
from ..entities.base_analyzer import BaseAnalyzer
from ..entities.analysis_result import AnalysisResult
from ..entities.prompt_template import PromptTemplate
from src.common.types import AnalysisMode
from src.common.exceptions import AnalysisError


class GenreAnalyzer(BaseAnalyzer):
    """
    –ê–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä –∂–∞–Ω—Ä–∞ —Ç–µ–∫—Å—Ç–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ LLM

    –û–ø—Ä–µ–¥–µ–ª—è–µ—Ç –ª–∏—Ç–µ—Ä–∞—Ç—É—Ä–Ω—ã–π –∂–∞–Ω—Ä —Ç–µ–∫—Å—Ç–∞ –∏—Å–ø–æ–ª—å–∑—É—è —è–∑—ã–∫–æ–≤—É—é –º–æ–¥–µ–ª—å
    """

    def __init__(
        self,
        llm_service=None,
        prompt_template: Optional[PromptTemplate] = None
    ):
        """
        –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä–∞

        Args:
            llm_service: –°–µ—Ä–≤–∏—Å –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å LLM
            prompt_template: –®–∞–±–ª–æ–Ω –ø—Ä–æ–º–ø—Ç–∞ (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
        """
        self.llm_service = llm_service
        self.prompt_template = prompt_template or PromptTemplate.create_default_genre_prompt()

    @property
    def name(self) -> str:
        return "genre"

    @property
    def display_name(self) -> str:
        return "–ê–Ω–∞–ª–∏–∑ –∂–∞–Ω—Ä–∞"

    @property
    def description(self) -> str:
        return """–û–ø—Ä–µ–¥–µ–ª—è–µ—Ç –ª–∏—Ç–µ—Ä–∞—Ç—É—Ä–Ω—ã–π –∂–∞–Ω—Ä —Ç–µ–∫—Å—Ç–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ –µ–≥–æ —Å–æ–¥–µ—Ä–∂–∞–Ω–∏—è,
—Å—Ç–∏–ª—è, —Ç–µ–º–∞—Ç–∏–∫–∏ –∏ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã. –ò—Å–ø–æ–ª—å–∑—É–µ—Ç —è–∑—ã–∫–æ–≤—É—é –º–æ–¥–µ–ª—å –¥–ª—è –≥–ª—É–±–æ–∫–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞."""

    @property
    def requires_llm(self) -> bool:
        return True

    @property
    def supports_chunked_mode(self) -> bool:
        # –î–ª—è –∂–∞–Ω—Ä–∞ –ª—É—á—à–µ –∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å –≤–µ—Å—å —Ç–µ–∫—Å—Ç
        return False

    async def analyze(
        self,
        text: BaseText,
        mode: AnalysisMode = AnalysisMode.FULL_TEXT,
        chunking_strategy: Optional[ChunkingStrategy] = None,
        context: Optional[Dict[str, Any]] = None
    ) -> AnalysisResult:
        """
        –ê–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å –∂–∞–Ω—Ä —Ç–µ–∫—Å—Ç–∞

        Args:
            text: –¢–µ–∫—Å—Ç –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
            mode: –†–µ–∂–∏–º –∞–Ω–∞–ª–∏–∑–∞ (—Ç–æ–ª—å–∫–æ FULL_TEXT)
            chunking_strategy: –ù–µ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è
            context: –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–π –∫–æ–Ω—Ç–µ–∫—Å—Ç

        Returns:
            AnalysisResult: –†–µ–∑—É–ª—å—Ç–∞—Ç –∞–Ω–∞–ª–∏–∑–∞
        """
        try:
            import time
            start_time = time.time()

            # –í–∞–ª–∏–¥–∞—Ü–∏—è —Ä–µ–∂–∏–º–∞
            self.validate_mode(mode)

            # –ü–æ–ª—É—á–µ–Ω–∏–µ –∫–æ–Ω—Ç–µ–Ω—Ç–∞
            content = await text.get_content()

            # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –¥–ª–∏–Ω—É —Ç–µ–∫—Å—Ç–∞ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ (–ø–µ—Ä–≤—ã–µ 4000 —Å–∏–º–≤–æ–ª–æ–≤)
            # –î–ª—è –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –∂–∞–Ω—Ä–∞ –æ–±—ã—á–Ω–æ –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –Ω–∞—á–∞–ª–∞
            analysis_text = content[:4000] if len(content) > 4000 else content

            # –§–æ—Ä–º–∏—Ä—É–µ–º –ø—Ä–æ–º–ø—Ç
            user_prompt = self.prompt_template.format_user_prompt(text=analysis_text)

            # –í—ã–∑–æ–≤ LLM (–±—É–¥–µ—Ç —Ä–µ–∞–ª–∏–∑–æ–≤–∞–Ω –ø–æ–∑–∂–µ –≤ model_management)
            if self.llm_service:
                llm_response = await self._call_llm(
                    system_prompt=self.prompt_template.system_prompt,
                    user_prompt=user_prompt
                )
            else:
                # –ó–∞–≥–ª—É—à–∫–∞ –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –±–µ–∑ LLM
                llm_response = self._get_mock_response()

            # –ü–∞—Ä—Å–∏–º –æ—Ç–≤–µ—Ç LLM
            result_data = await self._parse_llm_response(llm_response)

            # –î–æ–±–∞–≤–ª—è–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
            result_data["text_length_analyzed"] = len(analysis_text)
            result_data["full_text_length"] = len(content)

            execution_time = (time.time() - start_time) * 1000

            # –°–æ–∑–¥–∞–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞
            result = AnalysisResult(
                text_id=text.id,
                analyzer_name=self.name,
                data=result_data,
                execution_time_ms=execution_time,
                mode=mode.value,
            )

            # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏–∏
            result.interpretation = self.interpret_results(result)

            logger.debug(
                f"–ê–Ω–∞–ª–∏–∑ –∂–∞–Ω—Ä–∞ –∑–∞–≤–µ—Ä—à—ë–Ω –¥–ª—è —Ç–µ–∫—Å—Ç–∞ {text.id} "
                f"–∑–∞ {execution_time:.0f}ms"
            )

            return result

        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –∞–Ω–∞–ª–∏–∑–∞ –∂–∞–Ω—Ä–∞: {e}")
            raise AnalysisError(
                message=f"Genre analysis failed: {e}",
                details={"text_id": text.id, "error": str(e)}
            )

    async def _call_llm(self, system_prompt: str, user_prompt: str) -> str:
        """
        –í—ã–∑–æ–≤ LLM —Å–µ—Ä–≤–∏—Å–∞

        Args:
            system_prompt: –°–∏—Å—Ç–µ–º–Ω—ã–π –ø—Ä–æ–º–ø—Ç
            user_prompt: –ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–∏–π –ø—Ä–æ–º–ø—Ç

        Returns:
            str: –û—Ç–≤–µ—Ç LLM
        """
        # TODO: –†–µ–∞–ª–∏–∑–æ–≤–∞—Ç—å –ø–æ—Å–ª–µ —Å–æ–∑–¥–∞–Ω–∏—è model_management
        # return await self.llm_service.generate(
        #     system_prompt=system_prompt,
        #     user_prompt=user_prompt,
        #     temperature=self.prompt_template.temperature,
        #     max_tokens=self.prompt_template.max_tokens,
        # )
        return self._get_mock_response()

    def _get_mock_response(self) -> str:
        """
        –ó–∞–≥–ª—É—à–∫–∞ –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –±–µ–∑ LLM

        Returns:
            str: –ú–æ–∫-–æ—Ç–≤–µ—Ç
        """
        return json.dumps({
            "main_genre": "—Ö—É–¥–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω–∞—è –ª–∏—Ç–µ—Ä–∞—Ç—É—Ä–∞",
            "sub_genres": ["—Ñ–∞–Ω—Ç–∞—Å—Ç–∏–∫–∞", "–ø—Ä–∏–∫–ª—é—á–µ–Ω–∏—è"],
            "confidence": 0.85,
            "reasoning": "–¢–µ–∫—Å—Ç —Å–æ–¥–µ—Ä–∂–∏—Ç —ç–ª–µ–º–µ–Ω—Ç—ã —Ñ–∞–Ω—Ç–∞—Å—Ç–∏–∫–∏ –∏ –ø—Ä–∏–∫–ª—é—á–µ–Ω—á–µ—Å–∫–æ–≥–æ –∂–∞–Ω—Ä–∞."
        }, ensure_ascii=False)

    async def _parse_llm_response(self, response: str) -> Dict[str, Any]:
        """
        –ü–∞—Ä—Å–∏–Ω–≥ –æ—Ç–≤–µ—Ç–∞ LLM

        Args:
            response: –û—Ç–≤–µ—Ç –æ—Ç LLM

        Returns:
            Dict: –†–∞—Å–ø–∞—Ä—Å–µ–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ
        """
        try:
            # –ü—ã—Ç–∞–µ–º—Å—è —Ä–∞—Å–ø–∞—Ä—Å–∏—Ç—å JSON
            data = json.loads(response)

            # –í–∞–ª–∏–¥–∞—Ü–∏—è –æ–±—è–∑–∞—Ç–µ–ª—å–Ω—ã—Ö –ø–æ–ª–µ–π
            if "main_genre" not in data:
                raise ValueError("Missing 'main_genre' in response")

            if "confidence" not in data:
                data["confidence"] = 0.5

            # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è confidence
            data["confidence"] = max(0.0, min(1.0, float(data["confidence"])))

            # –ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ sub_genres –≤ —Å–ø–∏—Å–æ–∫ –µ—Å–ª–∏ –Ω—É–∂–Ω–æ
            if "sub_genres" not in data:
                data["sub_genres"] = []
            elif isinstance(data["sub_genres"], str):
                data["sub_genres"] = [data["sub_genres"]]

            return data

        except json.JSONDecodeError:
            # –ï—Å–ª–∏ –Ω–µ JSON, –ø—ã—Ç–∞–µ–º—Å—è –∏–∑–≤–ª–µ—á—å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –∏–∑ —Ç–µ–∫—Å—Ç–∞
            logger.warning("LLM response is not valid JSON, parsing as text")
            return {
                "main_genre": "–Ω–µ–∏–∑–≤–µ—Å—Ç–Ω–æ",
                "sub_genres": [],
                "confidence": 0.3,
                "reasoning": response[:200],
                "raw_response": response,
            }

    def interpret_results(self, result: AnalysisResult) -> str:
        """
        –ò–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∏—Ä–æ–≤–∞—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –∞–Ω–∞–ª–∏–∑–∞ –∂–∞–Ω—Ä–∞

        Args:
            result: –†–µ–∑—É–ª—å—Ç–∞—Ç –∞–Ω–∞–ª–∏–∑–∞

        Returns:
            str: –¢–µ–∫—Å—Ç–æ–≤–∞—è –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏—è
        """
        data = result.data

        main_genre = data.get("main_genre", "–Ω–µ–∏–∑–≤–µ—Å—Ç–Ω–æ")
        sub_genres = data.get("sub_genres", [])
        confidence = data.get("confidence", 0.0)
        reasoning = data.get("reasoning", "")

        lines = [f"üé≠ –ñ–∞–Ω—Ä: {main_genre}"]

        # –ü–æ–¥–∑–∞–Ω—Ä—ã
        if sub_genres:
            sub_genres_str = ", ".join(sub_genres)
            lines.append(f"–ü–æ–¥–∑–∞–Ω—Ä—ã: {sub_genres_str}")

        # –£–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å
        confidence_pct = confidence * 100
        confidence_level = "–≤—ã—Å–æ–∫–∞—è" if confidence >= 0.7 else "—Å—Ä–µ–¥–Ω—è—è" if confidence >= 0.5 else "–Ω–∏–∑–∫–∞—è"
        lines.append(f"–£–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å: {confidence_pct:.0f}% ({confidence_level})")

        # –û–±–æ—Å–Ω–æ–≤–∞–Ω–∏–µ
        if reasoning:
            lines.append(f"\n–û–±–æ—Å–Ω–æ–≤–∞–Ω–∏–µ: {reasoning}")

        return "\n".join(lines)

    def get_estimated_time(
        self,
        text_length: int,
        mode: AnalysisMode = AnalysisMode.FULL_TEXT
    ) -> float:
        """
        –û—Ü–µ–Ω–∫–∞ –≤—Ä–µ–º–µ–Ω–∏ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è

        Args:
            text_length: –î–ª–∏–Ω–∞ —Ç–µ–∫—Å—Ç–∞
            mode: –†–µ–∂–∏–º –∞–Ω–∞–ª–∏–∑–∞

        Returns:
            float: –í—Ä–µ–º—è –≤ —Å–µ–∫—É–Ω–¥–∞—Ö
        """
        # LLM –∞–Ω–∞–ª–∏–∑ –∑–∞–Ω–∏–º–∞–µ—Ç –±–æ–ª—å—à–µ –≤—Ä–µ–º–µ–Ω–∏
        return 5.0 + min(text_length / 1000, 4.0) * 0.5
