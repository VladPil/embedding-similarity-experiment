"""
TF-IDF –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä —Ç–µ–∫—Å—Ç–∞
"""
from typing import Optional, Dict, Any, List
import numpy as np
from sklearn.feature_extraction.text import TfidfVectorizer
from loguru import logger

from src.text_domain.entities.base_text import BaseText
from src.text_domain.entities.chunking_strategy import ChunkingStrategy
from src.text_domain.services.chunking_service import ChunkingService
from ..entities.base_analyzer import BaseAnalyzer
from ..entities.analysis_result import AnalysisResult
from src.common.types import AnalysisMode
from src.common.exceptions import AnalysisError


class TfidfAnalyzer(BaseAnalyzer):
    """
    –ê–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä –Ω–∞ –æ—Å–Ω–æ–≤–µ TF-IDF (Term Frequency-Inverse Document Frequency)

    –°—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫–∏–π –º–µ—Ç–æ–¥ –¥–ª—è –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –≤–∞–∂–Ω–æ—Å—Ç–∏ —Å–ª–æ–≤ –≤ —Ç–µ–∫—Å—Ç–µ.
    –ù–µ —Ç—Ä–µ–±—É–µ—Ç LLM, —Ä–∞–±–æ—Ç–∞–µ—Ç –±—ã—Å—Ç—Ä–æ.
    """

    def __init__(self):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä–∞"""
        self.chunking_service = ChunkingService()

    @property
    def name(self) -> str:
        return "tfidf"

    @property
    def display_name(self) -> str:
        return "TF-IDF –∞–Ω–∞–ª–∏–∑"

    @property
    def description(self) -> str:
        return """–°—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫–∏–π –∞–Ω–∞–ª–∏–∑ –≤–∞–∂–Ω–æ—Å—Ç–∏ —Å–ª–æ–≤ –≤ —Ç–µ–∫—Å—Ç–µ.
–í—ã—è–≤–ª—è–µ—Ç –∫–ª—é—á–µ–≤—ã–µ —Ç–µ—Ä–º–∏–Ω—ã –∏ –∏—Ö –∑–Ω–∞—á–∏–º–æ—Å—Ç—å –Ω–∞ –æ—Å–Ω–æ–≤–µ —á–∞—Å—Ç–æ—Ç—ã –≤—Å—Ç—Ä–µ—á–∞–µ–º–æ—Å—Ç–∏."""

    @property
    def requires_llm(self) -> bool:
        return False

    @property
    def requires_embeddings(self) -> bool:
        return False

    async def analyze(
        self,
        text: BaseText,
        mode: AnalysisMode = AnalysisMode.FULL_TEXT,
        chunking_strategy: Optional[ChunkingStrategy] = None,
        context: Optional[Dict[str, Any]] = None
    ) -> AnalysisResult:
        """
        –ê–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å —Ç–µ–∫—Å—Ç —Å –ø–æ–º–æ—â—å—é TF-IDF

        Args:
            text: –¢–µ–∫—Å—Ç –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
            mode: –†–µ–∂–∏–º –∞–Ω–∞–ª–∏–∑–∞
            chunking_strategy: –°—Ç—Ä–∞—Ç–µ–≥–∏—è —á–∞–Ω–∫–æ–≤–∫–∏
            context: –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–π –∫–æ–Ω—Ç–µ–∫—Å—Ç

        Returns:
            AnalysisResult: –†–µ–∑—É–ª—å—Ç–∞—Ç –∞–Ω–∞–ª–∏–∑–∞
        """
        try:
            import time
            start_time = time.time()

            # –í–∞–ª–∏–¥–∞—Ü–∏—è —Ä–µ–∂–∏–º–∞
            self.validate_mode(mode)

            # –ü–æ–ª—É—á–µ–Ω–∏–µ –∫–æ–Ω—Ç–µ–Ω—Ç–∞
            content = await text.get_content()

            if mode == AnalysisMode.FULL_TEXT:
                result_data = await self._analyze_full_text(content)
            else:
                if not chunking_strategy:
                    raise AnalysisError(
                        message="Chunking strategy required for CHUNKED mode",
                        details={"analyzer": self.name}
                    )
                result_data = await self._analyze_chunked(content, chunking_strategy)

            # –í—ã—á–∏—Å–ª–µ–Ω–∏–µ –≤—Ä–µ–º–µ–Ω–∏ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è
            execution_time = (time.time() - start_time) * 1000

            # –°–æ–∑–¥–∞–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞
            result = AnalysisResult(
                text_id=text.id,
                analyzer_name=self.name,
                data=result_data,
                execution_time_ms=execution_time,
                mode=mode.value,
            )

            # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏–∏
            result.interpretation = self.interpret_results(result)

            logger.debug(
                f"TF-IDF –∞–Ω–∞–ª–∏–∑ –∑–∞–≤–µ—Ä—à—ë–Ω –¥–ª—è —Ç–µ–∫—Å—Ç–∞ {text.id} "
                f"–∑–∞ {execution_time:.0f}ms"
            )

            return result

        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ TF-IDF –∞–Ω–∞–ª–∏–∑–∞: {e}")
            raise AnalysisError(
                message=f"TF-IDF analysis failed: {e}",
                details={"text_id": text.id, "error": str(e)}
            )

    async def _analyze_full_text(self, content: str) -> Dict[str, Any]:
        """
        –ê–Ω–∞–ª–∏–∑ –ø–æ–ª–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞

        Args:
            content: –°–æ–¥–µ—Ä–∂–∏–º–æ–µ —Ç–µ–∫—Å—Ç–∞

        Returns:
            Dict: –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –∞–Ω–∞–ª–∏–∑–∞
        """

        # –û–±–æ–≥–∞—â–µ–Ω–Ω—ã–π —Å–ø–∏—Å–æ–∫ —Ä—É—Å—Å–∫–∏—Ö —Å—Ç–æ–ø-—Å–ª–æ–≤
        russian_stop_words = [
            # –ü—Ä–µ–¥–ª–æ–≥–∏
            '–≤', '–≤–æ', '–Ω–∞', '–∑–∞', '–∫', '–∫–æ', '—Å', '—Å–æ', '–ø–æ', '–∏–∑', '–æ—Ç', '–¥–æ', '–¥–ª—è', '–ø—Ä–∏', '–æ', '–æ–±', '–ø–æ–¥', '–Ω–∞–¥', '–º–µ–∂–¥—É', '–ø–µ—Ä–µ–¥',
            # –°–æ—é–∑—ã
            '–∏', '–∞', '–Ω–æ', '–∏–ª–∏', '–¥–∞', '—á—Ç–æ', '—á—Ç–æ–±—ã', '–µ—Å–ª–∏', '–∫–æ–≥–¥–∞', '–∫–∞–∫', '—Ç–∞–∫', '—Ç–æ', '–Ω–∏', '–ª–∏–±–æ', '—Ö–æ—Ç—è', '–ø–æ—Ç–æ–º—É', '–ø–æ—ç—Ç–æ–º—É',
            # –ß–∞—Å—Ç–∏—Ü—ã
            '–Ω–µ', '–Ω–∏', '–∂–µ', '–ª–∏', '–±—ã', '–≤–æ—Ç', '–≤–æ–Ω', '–≤–µ–¥—å', '—É–∂', '–Ω—É', '–¥–∞–∂–µ', '–ª–∏—à—å', '—Ç–æ–ª—å–∫–æ', '–µ—â–µ', '—É–∂–µ', '—Ç–æ–∂–µ', '—Ç–∞–∫–∂–µ',
            # –ú–µ—Å—Ç–æ–∏–º–µ–Ω–∏—è
            '—è', '—Ç—ã', '–æ–Ω', '–æ–Ω–∞', '–æ–Ω–æ', '–º—ã', '–≤—ã', '–æ–Ω–∏', '—Å–µ–±—è', '–º–µ–Ω—è', '—Ç–µ–±—è', '–µ–≥–æ', '–µ—ë', '–Ω–∞—Å', '–≤–∞—Å', '–∏—Ö', '–º–Ω–µ', '—Ç–µ–±–µ', '–µ–º—É', '–µ–π', '–Ω–∞–º', '–≤–∞–º', '–∏–º',
            '–º–æ–π', '—Ç–≤–æ–π', '—Å–≤–æ–π', '–Ω–∞—à', '–≤–∞—à', '—ç—Ç–æ—Ç', '—Ç–æ—Ç', '—Ç–∞–∫–æ–π', '–≤–µ—Å—å', '–≤—Å—è–∫–∏–π', '–∫–∞–∂–¥—ã–π', '–ª—é–±–æ–π', '–¥—Ä—É–≥–æ–π', '–∏–Ω–æ–π', '—Å–∞–º',
            '–∫—Ç–æ', '—á—Ç–æ', '–∫–∞–∫–æ–π', '–∫–æ—Ç–æ—Ä—ã–π', '—á–µ–π', '–≥–¥–µ', '–∫—É–¥–∞', '–æ—Ç–∫—É–¥–∞', '–∫–æ–≥–¥–∞', '–∑–∞—á–µ–º', '–ø–æ—á–µ–º—É', '–∫–∞–∫', '—Å–∫–æ–ª—å–∫–æ',
            # –í—Å–ø–æ–º–æ–≥–∞—Ç–µ–ª—å–Ω—ã–µ –≥–ª–∞–≥–æ–ª—ã –∏ —Å–≤—è–∑–∫–∏
            '–±—ã—Ç—å', '–µ—Å—Ç—å', '–±—ã–ª', '–±—ã–ª–∞', '–±—ã–ª–æ', '–±—ã–ª–∏', '–±—É–¥–µ—Ç', '–±—É–¥—É—Ç', '–±—ã–≤–∞–µ—Ç', '—Å—Ç–∞—Ç—å', '—Å—Ç–∞–ª', '—Å—Ç–∞–ª–∞', '—Å—Ç–∞–ª–æ', '—Å—Ç–∞–ª–∏',
            # –ù–∞—Ä–µ—á–∏—è –æ–±—â–µ–≥–æ –∑–Ω–∞—á–µ–Ω–∏—è
            '–æ—á–µ–Ω—å', '–±–æ–ª–µ–µ', '–º–µ–Ω–µ–µ', '—Å–∞–º—ã–π', '–Ω–∞–∏–±–æ–ª–µ–µ', '–Ω–∞–∏–º–µ–Ω–µ–µ', '—Å–ª–∏—à–∫–æ–º', '–¥–æ–≤–æ–ª—å–Ω–æ', '–≤–µ—Å—å–º–∞', '–ø–æ—á—Ç–∏', '—Å–æ–≤—Å–µ–º', '–≤–ø–æ–ª–Ω–µ',
            '–∑–¥–µ—Å—å', '—Ç–∞–º', '—Ç—É—Ç', '–≤—Å—é–¥—É', '–≤–µ–∑–¥–µ', '–Ω–∏–≥–¥–µ', '–∫—É–¥–∞-—Ç–æ', '–≥–¥–µ-—Ç–æ', '–æ—Ç–∫—É–¥–∞-—Ç–æ', '—Ç—É–¥–∞', '—Å—é–¥–∞', '–æ—Ç—Ç—É–¥–∞', '–æ—Ç—Å—é–¥–∞',
            '—Ç–µ–ø–µ—Ä—å', '—Å–µ–π—á–∞—Å', '—Ç–æ–≥–¥–∞', '–≤—Å–µ–≥–¥–∞', '–Ω–∏–∫–æ–≥–¥–∞', '–∏–Ω–æ–≥–¥–∞', '—á–∞—Å—Ç–æ', '—Ä–µ–¥–∫–æ', '—Å–∫–æ—Ä–æ', '–¥–∞–≤–Ω–æ', '–Ω–µ–¥–∞–≤–Ω–æ', '—Å–µ–≥–æ–¥–Ω—è', '–≤—á–µ—Ä–∞', '–∑–∞–≤—Ç—Ä–∞',
            # –ú–æ–¥–∞–ª—å–Ω—ã–µ —Å–ª–æ–≤–∞
            '–º–æ–∂–µ—Ç', '–º–æ–∂–µ—Ç –±—ã—Ç—å', '–Ω–∞–≤–µ—Ä–Ω–æ–µ', '–≤–µ—Ä–æ—è—Ç–Ω–æ', '–≤–æ–∑–º–æ–∂–Ω–æ', '–∫–æ–Ω–µ—á–Ω–æ', '—Ä–∞–∑—É–º–µ–µ—Ç—Å—è', '–±–µ–∑—É—Å–ª–æ–≤–Ω–æ', '–Ω–µ—Å–æ–º–Ω–µ–Ω–Ω–æ', '–¥–µ–π—Å—Ç–≤–∏—Ç–µ–ª—å–Ω–æ',
            # –í–≤–æ–¥–Ω—ã–µ —Å–ª–æ–≤–∞
            '–∫—Å—Ç–∞—Ç–∏', '–Ω–∞–ø—Ä–∏–º–µ—Ä', '–≤–ø—Ä–æ—á–µ–º', '–æ–¥–Ω–∞–∫–æ', '–Ω–∞–∫–æ–Ω–µ—Ü', '–∏—Ç–∞–∫', '–∑–Ω–∞—á–∏—Ç', '—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ', '—Ç–∞–∫–∏–º –æ–±—Ä–∞–∑–æ–º', '–≤–æ-–ø–µ—Ä–≤—ã—Ö', '–≤–æ-–≤—Ç–æ—Ä—ã—Ö',
            # –ú–µ–∂–¥–æ–º–µ—Ç–∏—è
            '–∞—Ö', '–æ—Ö', '—ç—Ö', '–Ω—É', '–¥–∞', '–Ω–µ—Ç', '–∞–≥–∞', '—É–≥—É', '–æ–π', '–∞–π', '—ç–π',
            # –ß–∏—Å–ª–∏—Ç–µ–ª—å–Ω—ã–µ
            '–æ–¥–∏–Ω', '–æ–¥–Ω–∞', '–æ–¥–Ω–æ', '–¥–≤–∞', '—Ç—Ä–∏', '—á–µ—Ç—ã—Ä–µ', '–ø—è—Ç—å', '—à–µ—Å—Ç—å', '—Å–µ–º—å', '–≤–æ—Å–µ–º—å', '–¥–µ–≤—è—Ç—å', '–¥–µ—Å—è—Ç—å',
            '–ø–µ—Ä–≤—ã–π', '–≤—Ç–æ—Ä–æ–π', '—Ç—Ä–µ—Ç–∏–π', '–ø–æ—Å–ª–µ–¥–Ω–∏–π', '–º–Ω–æ–≥–æ', '–º–∞–ª–æ', '–Ω–µ—Å–∫–æ–ª—å–∫–æ', '—Å—Ç–æ–ª—å–∫–æ', '—Å–∫–æ–ª—å–∫–æ',
            # –°–ª—É–∂–µ–±–Ω—ã–µ –∫–æ–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏
            '—Ç–æ –µ—Å—Ç—å', '–∞ –∏–º–µ–Ω–Ω–æ', '–≤ —Ç–æ–º —á–∏—Å–ª–µ', '–≤ —á–∞—Å—Ç–Ω–æ—Å—Ç–∏', '–≤ –æ—Å–Ω–æ–≤–Ω–æ–º', '–≤ –æ–±—â–µ–º', '–≤ —Ü–µ–ª–æ–º', '–ø–æ –∫—Ä–∞–π–Ω–µ–π –º–µ—Ä–µ',
            '—Ç–∞–∫ –∏–ª–∏ –∏–Ω–∞—á–µ', '—Ç–∞–∫ —Å–∫–∞–∑–∞—Ç—å', '–º–æ–∂–Ω–æ —Å–∫–∞–∑–∞—Ç—å', '–Ω–∞–¥–æ —Å–∫–∞–∑–∞—Ç—å', '—Å—Ç–æ–∏—Ç –æ—Ç–º–µ—Ç–∏—Ç—å',
        ]

        # –°–æ–∑–¥–∞—ë–º TF-IDF –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ç–æ—Ä
        vectorizer = TfidfVectorizer(
            max_features=100,  # –¢–æ–ø-100 —Å–ª–æ–≤
            stop_words=russian_stop_words,  # –û–±–æ–≥–∞—â–µ–Ω–Ω—ã–µ —Ä—É—Å—Å–∫–∏–µ —Å—Ç–æ–ø-—Å–ª–æ–≤–∞
            ngram_range=(1, 2),  # –£–Ω–∏–≥—Ä–∞–º–º—ã –∏ –±–∏–≥—Ä–∞–º–º—ã
            min_df=1,
            max_df=1.0,  # –î–ª—è –æ–¥–Ω–æ–≥–æ –¥–æ–∫—É–º–µ–Ω—Ç–∞ –¥–æ–ª–∂–Ω–æ –±—ã—Ç—å 1.0
            token_pattern=r'\b[–∞-—è—ë]{2,}\b'  # –¢–æ–ª—å–∫–æ —Ä—É—Å—Å–∫–∏–µ —Å–ª–æ–≤–∞ –¥–ª–∏–Ω–æ–π –æ—Ç 2 —Å–∏–º–≤–æ–ª–æ–≤
        )

        # –ü–æ–ª—É—á–∞–µ–º TF-IDF –º–∞—Ç—Ä–∏—Ü—É
        tfidf_matrix = vectorizer.fit_transform([content])
        feature_names = vectorizer.get_feature_names_out()

        # –ü–æ–ª—É—á–∞–µ–º –≤–µ—Å–∞ —Å–ª–æ–≤
        scores = tfidf_matrix.toarray()[0]

        # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ —É–±—ã–≤–∞–Ω–∏—é –≤–∞–∂–Ω–æ—Å—Ç–∏
        top_indices = scores.argsort()[-20:][::-1]
        top_terms = [
            {
                "term": feature_names[i],
                "score": float(scores[i]),
                "rank": rank + 1
            }
            for rank, i in enumerate(top_indices)
            if scores[i] > 0
        ]

        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
        total_terms = len([s for s in scores if s > 0])
        avg_score = float(np.mean(scores[scores > 0])) if total_terms > 0 else 0.0
        max_score = float(np.max(scores)) if len(scores) > 0 else 0.0

        return {
            "top_terms": top_terms,
            "total_terms": total_terms,
            "avg_score": avg_score,
            "max_score": max_score,
            "vocabulary_size": len(feature_names),
        }

    async def _analyze_chunked(
        self,
        content: str,
        strategy: ChunkingStrategy
    ) -> Dict[str, Any]:
        """
        –ê–Ω–∞–ª–∏–∑ –ø–æ —á–∞–Ω–∫–∞–º

        Args:
            content: –°–æ–¥–µ—Ä–∂–∏–º–æ–µ —Ç–µ–∫—Å—Ç–∞
            strategy: –°—Ç—Ä–∞—Ç–µ–≥–∏—è —á–∞–Ω–∫–æ–≤–∫–∏

        Returns:
            Dict: –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –∞–Ω–∞–ª–∏–∑–∞
        """
        # –†–∞–∑–±–∏–≤–∞–µ–º –Ω–∞ —á–∞–Ω–∫–∏
        chunks = await self.chunking_service.chunk_text(content, strategy)

        # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –∫–∞–∂–¥—ã–π —á–∞–Ω–∫
        chunk_results = []
        all_terms = {}

        for chunk in chunks:
            chunk_data = await self._analyze_full_text(chunk.content)
            chunk_results.append({
                "chunk_index": chunk.chunk_index,
                "top_terms": chunk_data["top_terms"][:5],  # –¢–æ–ø-5 –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —á–∞–Ω–∫–∞
            })

            # –ê–≥—Ä–µ–≥–∏—Ä—É–µ–º —Ç–µ—Ä–º–∏–Ω—ã
            for term_data in chunk_data["top_terms"]:
                term = term_data["term"]
                score = term_data["score"]
                if term in all_terms:
                    all_terms[term] += score
                else:
                    all_terms[term] = score

        # –°–æ—Ä—Ç–∏—Ä—É–µ–º –∞–≥—Ä–µ–≥–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ —Ç–µ—Ä–º–∏–Ω—ã
        sorted_terms = sorted(
            [{"term": term, "score": score} for term, score in all_terms.items()],
            key=lambda x: x["score"],
            reverse=True
        )[:20]

        return {
            "chunk_count": len(chunks),
            "chunk_results": chunk_results,
            "aggregated_top_terms": sorted_terms,
            "total_unique_terms": len(all_terms),
        }

    def interpret_results(self, result: AnalysisResult) -> str:
        """
        –ò–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∏—Ä–æ–≤–∞—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã TF-IDF –∞–Ω–∞–ª–∏–∑–∞

        Args:
            result: –†–µ–∑—É–ª—å—Ç–∞—Ç –∞–Ω–∞–ª–∏–∑–∞

        Returns:
            str: –¢–µ–∫—Å—Ç–æ–≤–∞—è –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏—è
        """
        data = result.data

        if result.mode == "full_text":
            top_terms = data.get("top_terms", [])
            if not top_terms:
                return "–ö–ª—é—á–µ–≤—ã–µ —Ç–µ—Ä–º–∏–Ω—ã –Ω–µ –Ω–∞–π–¥–µ–Ω—ã."

            lines = ["üìä –ö–ª—é—á–µ–≤—ã–µ —Ç–µ—Ä–º–∏–Ω—ã –∏ –∏—Ö –∑–Ω–∞—á–∏–º–æ—Å—Ç—å:\n"]

            for i, term_data in enumerate(top_terms[:10], 1):
                term = term_data["term"]
                score = term_data["score"]
                lines.append(f"{i}. '{term}' ‚Äî {score:.3f}")

            lines.append(f"\n–í—Å–µ–≥–æ —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Ç–µ—Ä–º–∏–Ω–æ–≤: {data.get('total_terms', 0)}")

            return "\n".join(lines)

        else:  # chunked mode
            chunk_count = data.get("chunk_count", 0)
            aggregated = data.get("aggregated_top_terms", [])

            if not aggregated:
                return f"–¢–µ–∫—Å—Ç —Ä–∞–∑–±–∏—Ç –Ω–∞ {chunk_count} —á–∞–Ω–∫–æ–≤, –Ω–æ –∫–ª—é—á–µ–≤—ã–µ —Ç–µ—Ä–º–∏–Ω—ã –Ω–µ –Ω–∞–π–¥–µ–Ω—ã."

            lines = [
                f"üìä TF-IDF –∞–Ω–∞–ª–∏–∑ –ø–æ {chunk_count} —á–∞–Ω–∫–∞–º:\n",
                "–¢–æ–ø-10 –∞–≥—Ä–µ–≥–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö —Ç–µ—Ä–º–∏–Ω–æ–≤:\n"
            ]

            for i, term_data in enumerate(aggregated[:10], 1):
                term = term_data["term"]
                score = term_data["score"]
                lines.append(f"{i}. '{term}' ‚Äî {score:.3f}")

            lines.append(f"\n–£–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Ç–µ—Ä–º–∏–Ω–æ–≤: {data.get('total_unique_terms', 0)}")

            return "\n".join(lines)

    def get_estimated_time(
        self,
        text_length: int,
        mode: AnalysisMode = AnalysisMode.FULL_TEXT
    ) -> float:
        """
        –û—Ü–µ–Ω–∫–∞ –≤—Ä–µ–º–µ–Ω–∏ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è

        Args:
            text_length: –î–ª–∏–Ω–∞ —Ç–µ–∫—Å—Ç–∞
            mode: –†–µ–∂–∏–º –∞–Ω–∞–ª–∏–∑–∞

        Returns:
            float: –í—Ä–µ–º—è –≤ —Å–µ–∫—É–Ω–¥–∞—Ö
        """
        # TF-IDF –±—ã—Å—Ç—Ä—ã–π
        base_time = 0.5
        time_per_1k = 0.05
        return base_time + (text_length / 1000) * time_per_1k
