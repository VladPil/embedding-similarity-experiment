"""
TF-IDF –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä —Ç–µ–∫—Å—Ç–∞
"""
from typing import Optional, Dict, Any, List
import numpy as np
from sklearn.feature_extraction.text import TfidfVectorizer
from loguru import logger

from src.text_domain.entities.base_text import BaseText
from src.text_domain.entities.chunking_strategy import ChunkingStrategy
from src.text_domain.services.chunking_service import ChunkingService
from ..entities.base_analyzer import BaseAnalyzer
from ..entities.analysis_result import AnalysisResult
from src.common.types import AnalysisMode
from src.common.exceptions import AnalysisError


class TfidfAnalyzer(BaseAnalyzer):
    """
    –ê–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä –Ω–∞ –æ—Å–Ω–æ–≤–µ TF-IDF (Term Frequency-Inverse Document Frequency)

    –°—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫–∏–π –º–µ—Ç–æ–¥ –¥–ª—è –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –≤–∞–∂–Ω–æ—Å—Ç–∏ —Å–ª–æ–≤ –≤ —Ç–µ–∫—Å—Ç–µ.
    –ù–µ —Ç—Ä–µ–±—É–µ—Ç LLM, —Ä–∞–±–æ—Ç–∞–µ—Ç –±—ã—Å—Ç—Ä–æ.
    """

    def __init__(self):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä–∞"""
        self.chunking_service = ChunkingService()

    @property
    def name(self) -> str:
        return "tfidf"

    @property
    def display_name(self) -> str:
        return "TF-IDF –∞–Ω–∞–ª–∏–∑"

    @property
    def description(self) -> str:
        return """–°—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫–∏–π –∞–Ω–∞–ª–∏–∑ –≤–∞–∂–Ω–æ—Å—Ç–∏ —Å–ª–æ–≤ –≤ —Ç–µ–∫—Å—Ç–µ.
–í—ã—è–≤–ª—è–µ—Ç –∫–ª—é—á–µ–≤—ã–µ —Ç–µ—Ä–º–∏–Ω—ã –∏ –∏—Ö –∑–Ω–∞—á–∏–º–æ—Å—Ç—å –Ω–∞ –æ—Å–Ω–æ–≤–µ —á–∞—Å—Ç–æ—Ç—ã –≤—Å—Ç—Ä–µ—á–∞–µ–º–æ—Å—Ç–∏."""

    @property
    def requires_llm(self) -> bool:
        return False

    @property
    def requires_embeddings(self) -> bool:
        return False

    async def analyze(
        self,
        text: BaseText,
        mode: AnalysisMode = AnalysisMode.FULL_TEXT,
        chunking_strategy: Optional[ChunkingStrategy] = None,
        context: Optional[Dict[str, Any]] = None
    ) -> AnalysisResult:
        """
        –ê–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å —Ç–µ–∫—Å—Ç —Å –ø–æ–º–æ—â—å—é TF-IDF

        Args:
            text: –¢–µ–∫—Å—Ç –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
            mode: –†–µ–∂–∏–º –∞–Ω–∞–ª–∏–∑–∞
            chunking_strategy: –°—Ç—Ä–∞—Ç–µ–≥–∏—è —á–∞–Ω–∫–æ–≤–∫–∏
            context: –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–π –∫–æ–Ω—Ç–µ–∫—Å—Ç

        Returns:
            AnalysisResult: –†–µ–∑—É–ª—å—Ç–∞—Ç –∞–Ω–∞–ª–∏–∑–∞
        """
        try:
            import time
            start_time = time.time()

            # –í–∞–ª–∏–¥–∞—Ü–∏—è —Ä–µ–∂–∏–º–∞
            self.validate_mode(mode)

            # –ü–æ–ª—É—á–µ–Ω–∏–µ –∫–æ–Ω—Ç–µ–Ω—Ç–∞
            content = await text.get_content()

            if mode == AnalysisMode.FULL_TEXT:
                result_data = await self._analyze_full_text(content)
            else:
                if not chunking_strategy:
                    raise AnalysisError(
                        message="Chunking strategy required for CHUNKED mode",
                        details={"analyzer": self.name}
                    )
                result_data = await self._analyze_chunked(content, chunking_strategy)

            # –í—ã—á–∏—Å–ª–µ–Ω–∏–µ –≤—Ä–µ–º–µ–Ω–∏ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è
            execution_time = (time.time() - start_time) * 1000

            # –°–æ–∑–¥–∞–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞
            result = AnalysisResult(
                text_id=text.id,
                analyzer_name=self.name,
                data=result_data,
                execution_time_ms=execution_time,
                mode=mode.value,
            )

            # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏–∏
            result.interpretation = self.interpret_results(result)

            logger.debug(
                f"TF-IDF –∞–Ω–∞–ª–∏–∑ –∑–∞–≤–µ—Ä—à—ë–Ω –¥–ª—è —Ç–µ–∫—Å—Ç–∞ {text.id} "
                f"–∑–∞ {execution_time:.0f}ms"
            )

            return result

        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ TF-IDF –∞–Ω–∞–ª–∏–∑–∞: {e}")
            raise AnalysisError(
                message=f"TF-IDF analysis failed: {e}",
                details={"text_id": text.id, "error": str(e)}
            )

    async def _analyze_full_text(self, content: str) -> Dict[str, Any]:
        """
        –ê–Ω–∞–ª–∏–∑ –ø–æ–ª–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞

        Args:
            content: –°–æ–¥–µ—Ä–∂–∏–º–æ–µ —Ç–µ–∫—Å—Ç–∞

        Returns:
            Dict: –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –∞–Ω–∞–ª–∏–∑–∞
        """
        # –°–æ–∑–¥–∞—ë–º TF-IDF –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ç–æ—Ä
        vectorizer = TfidfVectorizer(
            max_features=100,  # –¢–æ–ø-100 —Å–ª–æ–≤
            stop_words=None,  # –ú–æ–∂–Ω–æ –¥–æ–±–∞–≤–∏—Ç—å —Å—Ç–æ–ø-—Å–ª–æ–≤–∞ –¥–ª—è —Ä—É—Å—Å–∫–æ–≥–æ
            ngram_range=(1, 2),  # –£–Ω–∏–≥—Ä–∞–º–º—ã –∏ –±–∏–≥—Ä–∞–º–º—ã
            min_df=1,
            max_df=0.8,
        )

        # –ü–æ–ª—É—á–∞–µ–º TF-IDF –º–∞—Ç—Ä–∏—Ü—É
        tfidf_matrix = vectorizer.fit_transform([content])
        feature_names = vectorizer.get_feature_names_out()

        # –ü–æ–ª—É—á–∞–µ–º –≤–µ—Å–∞ —Å–ª–æ–≤
        scores = tfidf_matrix.toarray()[0]

        # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ —É–±—ã–≤–∞–Ω–∏—é –≤–∞–∂–Ω–æ—Å—Ç–∏
        top_indices = scores.argsort()[-20:][::-1]
        top_terms = [
            {
                "term": feature_names[i],
                "score": float(scores[i]),
                "rank": rank + 1
            }
            for rank, i in enumerate(top_indices)
            if scores[i] > 0
        ]

        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
        total_terms = len([s for s in scores if s > 0])
        avg_score = float(np.mean(scores[scores > 0])) if total_terms > 0 else 0.0
        max_score = float(np.max(scores)) if len(scores) > 0 else 0.0

        return {
            "top_terms": top_terms,
            "total_terms": total_terms,
            "avg_score": avg_score,
            "max_score": max_score,
            "vocabulary_size": len(feature_names),
        }

    async def _analyze_chunked(
        self,
        content: str,
        strategy: ChunkingStrategy
    ) -> Dict[str, Any]:
        """
        –ê–Ω–∞–ª–∏–∑ –ø–æ —á–∞–Ω–∫–∞–º

        Args:
            content: –°–æ–¥–µ—Ä–∂–∏–º–æ–µ —Ç–µ–∫—Å—Ç–∞
            strategy: –°—Ç—Ä–∞—Ç–µ–≥–∏—è —á–∞–Ω–∫–æ–≤–∫–∏

        Returns:
            Dict: –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –∞–Ω–∞–ª–∏–∑–∞
        """
        # –†–∞–∑–±–∏–≤–∞–µ–º –Ω–∞ —á–∞–Ω–∫–∏
        chunks = await self.chunking_service.chunk_text(content, strategy)

        # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –∫–∞–∂–¥—ã–π —á–∞–Ω–∫
        chunk_results = []
        all_terms = {}

        for chunk in chunks:
            chunk_data = await self._analyze_full_text(chunk.content)
            chunk_results.append({
                "chunk_index": chunk.chunk_index,
                "top_terms": chunk_data["top_terms"][:5],  # –¢–æ–ø-5 –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —á–∞–Ω–∫–∞
            })

            # –ê–≥—Ä–µ–≥–∏—Ä—É–µ–º —Ç–µ—Ä–º–∏–Ω—ã
            for term_data in chunk_data["top_terms"]:
                term = term_data["term"]
                score = term_data["score"]
                if term in all_terms:
                    all_terms[term] += score
                else:
                    all_terms[term] = score

        # –°–æ—Ä—Ç–∏—Ä—É–µ–º –∞–≥—Ä–µ–≥–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ —Ç–µ—Ä–º–∏–Ω—ã
        sorted_terms = sorted(
            [{"term": term, "score": score} for term, score in all_terms.items()],
            key=lambda x: x["score"],
            reverse=True
        )[:20]

        return {
            "chunk_count": len(chunks),
            "chunk_results": chunk_results,
            "aggregated_top_terms": sorted_terms,
            "total_unique_terms": len(all_terms),
        }

    def interpret_results(self, result: AnalysisResult) -> str:
        """
        –ò–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∏—Ä–æ–≤–∞—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã TF-IDF –∞–Ω–∞–ª–∏–∑–∞

        Args:
            result: –†–µ–∑—É–ª—å—Ç–∞—Ç –∞–Ω–∞–ª–∏–∑–∞

        Returns:
            str: –¢–µ–∫—Å—Ç–æ–≤–∞—è –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏—è
        """
        data = result.data

        if result.mode == "full_text":
            top_terms = data.get("top_terms", [])
            if not top_terms:
                return "–ö–ª—é—á–µ–≤—ã–µ —Ç–µ—Ä–º–∏–Ω—ã –Ω–µ –Ω–∞–π–¥–µ–Ω—ã."

            lines = ["üìä –ö–ª—é—á–µ–≤—ã–µ —Ç–µ—Ä–º–∏–Ω—ã –∏ –∏—Ö –∑–Ω–∞—á–∏–º–æ—Å—Ç—å:\n"]

            for i, term_data in enumerate(top_terms[:10], 1):
                term = term_data["term"]
                score = term_data["score"]
                lines.append(f"{i}. '{term}' ‚Äî {score:.3f}")

            lines.append(f"\n–í—Å–µ–≥–æ —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Ç–µ—Ä–º–∏–Ω–æ–≤: {data.get('total_terms', 0)}")

            return "\n".join(lines)

        else:  # chunked mode
            chunk_count = data.get("chunk_count", 0)
            aggregated = data.get("aggregated_top_terms", [])

            if not aggregated:
                return f"–¢–µ–∫—Å—Ç —Ä–∞–∑–±–∏—Ç –Ω–∞ {chunk_count} —á–∞–Ω–∫–æ–≤, –Ω–æ –∫–ª—é—á–µ–≤—ã–µ —Ç–µ—Ä–º–∏–Ω—ã –Ω–µ –Ω–∞–π–¥–µ–Ω—ã."

            lines = [
                f"üìä TF-IDF –∞–Ω–∞–ª–∏–∑ –ø–æ {chunk_count} —á–∞–Ω–∫–∞–º:\n",
                "–¢–æ–ø-10 –∞–≥—Ä–µ–≥–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö —Ç–µ—Ä–º–∏–Ω–æ–≤:\n"
            ]

            for i, term_data in enumerate(aggregated[:10], 1):
                term = term_data["term"]
                score = term_data["score"]
                lines.append(f"{i}. '{term}' ‚Äî {score:.3f}")

            lines.append(f"\n–£–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Ç–µ—Ä–º–∏–Ω–æ–≤: {data.get('total_unique_terms', 0)}")

            return "\n".join(lines)

    def get_estimated_time(
        self,
        text_length: int,
        mode: AnalysisMode = AnalysisMode.FULL_TEXT
    ) -> float:
        """
        –û—Ü–µ–Ω–∫–∞ –≤—Ä–µ–º–µ–Ω–∏ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è

        Args:
            text_length: –î–ª–∏–Ω–∞ —Ç–µ–∫—Å—Ç–∞
            mode: –†–µ–∂–∏–º –∞–Ω–∞–ª–∏–∑–∞

        Returns:
            float: –í—Ä–µ–º—è –≤ —Å–µ–∫—É–Ω–¥–∞—Ö
        """
        # TF-IDF –±—ã—Å—Ç—Ä—ã–π
        base_time = 0.5
        time_per_1k = 0.05
        return base_time + (text_length / 1000) * time_per_1k
