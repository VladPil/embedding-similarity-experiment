"""
–ê–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä —Å—Ç–∏–ª—è —Ç–µ–∫—Å—Ç–∞
"""
import json
import re
from typing import Optional, Dict, Any
from loguru import logger

from src.text_domain.entities.base_text import BaseText
from src.text_domain.entities.chunking_strategy import ChunkingStrategy
from src.text_domain.services.chunking_service import ChunkingService
from ..entities.base_analyzer import BaseAnalyzer
from ..entities.analysis_result import AnalysisResult
from ..entities.prompt_template import PromptTemplate
from src.common.types import AnalysisMode
from src.common.exceptions import AnalysisError


class StyleAnalyzer(BaseAnalyzer):
    """
    –ê–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä —Å—Ç–∏–ª—è —Ç–µ–∫—Å—Ç–∞

    –ö–æ–º–±–∏–Ω–∏—Ä—É–µ—Ç —Å—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫–∏–π –∞–Ω–∞–ª–∏–∑ –∏ LLM –¥–ª—è –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è
    —Å—Ç–∏–ª–∏—Å—Ç–∏—á–µ—Å–∫–∏—Ö –æ—Å–æ–±–µ–Ω–Ω–æ—Å—Ç–µ–π —Ç–µ–∫—Å—Ç–∞
    """

    def __init__(
        self,
        llm_service=None,
        prompt_template: Optional[PromptTemplate] = None
    ):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä–∞"""
        self.llm_service = llm_service
        self.prompt_template = prompt_template or PromptTemplate.create_default_style_prompt()
        self.chunking_service = ChunkingService()

    @property
    def name(self) -> str:
        return "style"

    @property
    def display_name(self) -> str:
        return "–ê–Ω–∞–ª–∏–∑ —Å—Ç–∏–ª—è"

    @property
    def description(self) -> str:
        return """–ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç —Å—Ç–∏–ª–∏—Å—Ç–∏—á–µ—Å–∫–∏–µ –æ—Å–æ–±–µ–Ω–Ω–æ—Å—Ç–∏ —Ç–µ–∫—Å—Ç–∞: –ª–µ–∫—Å–∏–∫—É,
—Å–∏–Ω—Ç–∞–∫—Å–∏—Å, –æ–±—Ä–∞–∑–Ω–æ—Å—Ç—å, —Ä–∏—Ç–º, —ç–º–æ—Ü–∏–æ–Ω–∞–ª—å–Ω–æ—Å—Ç—å."""

    @property
    def requires_llm(self) -> bool:
        return True

    async def analyze(
        self,
        text: BaseText,
        mode: AnalysisMode = AnalysisMode.FULL_TEXT,
        chunking_strategy: Optional[ChunkingStrategy] = None,
        context: Optional[Dict[str, Any]] = None
    ) -> AnalysisResult:
        """–ê–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å —Å—Ç–∏–ª—å —Ç–µ–∫—Å—Ç–∞"""
        try:
            import time
            start_time = time.time()

            self.validate_mode(mode)
            content = await text.get_content()

            # –°—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫–∏–π –∞–Ω–∞–ª–∏–∑
            stats = await self._statistical_analysis(content)

            # LLM –∞–Ω–∞–ª–∏–∑
            if mode == AnalysisMode.FULL_TEXT:
                llm_data = await self._llm_analysis_full(content)
            else:
                if not chunking_strategy:
                    raise AnalysisError("Chunking strategy required for CHUNKED mode")
                llm_data = await self._llm_analysis_chunked(content, chunking_strategy)

            # –û–±—ä–µ–¥–∏–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
            result_data = {
                "statistics": stats,
                "llm_assessment": llm_data,
            }

            execution_time = (time.time() - start_time) * 1000

            result = AnalysisResult(
                text_id=text.id,
                analyzer_name=self.name,
                data=result_data,
                execution_time_ms=execution_time,
                mode=mode.value,
            )

            result.interpretation = self.interpret_results(result)

            logger.debug(f"–ê–Ω–∞–ª–∏–∑ —Å—Ç–∏–ª—è –∑–∞–≤–µ—Ä—à—ë–Ω –¥–ª—è —Ç–µ–∫—Å—Ç–∞ {text.id}")
            return result

        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –∞–Ω–∞–ª–∏–∑–∞ —Å—Ç–∏–ª—è: {e}")
            raise AnalysisError(
                message=f"Style analysis failed: {e}",
                details={"text_id": text.id, "error": str(e)}
            )

    async def _statistical_analysis(self, content: str) -> Dict[str, Any]:
        """
        –°—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫–∏–π –∞–Ω–∞–ª–∏–∑ —Å—Ç–∏–ª—è

        Args:
            content: –¢–µ–∫—Å—Ç

        Returns:
            Dict: –°—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫–∏–µ –º–µ—Ç—Ä–∏–∫–∏
        """
        # –ü—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è
        sentences = re.split(r'[.!?]+', content)
        sentences = [s.strip() for s in sentences if s.strip()]

        # –°–ª–æ–≤–∞
        words = re.findall(r'\b\w+\b', content.lower())

        # –ü–∞—Ä–∞–≥—Ä–∞—Ñ—ã
        paragraphs = content.split('\n\n')
        paragraphs = [p.strip() for p in paragraphs if p.strip()]

        # –ú–µ—Ç—Ä–∏–∫–∏
        avg_sentence_length = sum(len(s.split()) for s in sentences) / len(sentences) if sentences else 0
        avg_word_length = sum(len(w) for w in words) / len(words) if words else 0
        avg_paragraph_length = len(content) / len(paragraphs) if paragraphs else 0

        # –°–ª–æ–∂–Ω—ã–µ —Å–ª–æ–≤–∞ (>7 —Å–∏–º–≤–æ–ª–æ–≤)
        complex_words = [w for w in words if len(w) > 7]
        complex_word_ratio = len(complex_words) / len(words) if words else 0

        # –£–Ω–∏–∫–∞–ª—å–Ω—ã–µ —Å–ª–æ–≤–∞
        unique_words = set(words)
        vocabulary_richness = len(unique_words) / len(words) if words else 0

        return {
            "total_words": len(words),
            "unique_words": len(unique_words),
            "total_sentences": len(sentences),
            "total_paragraphs": len(paragraphs),
            "avg_sentence_length": round(avg_sentence_length, 2),
            "avg_word_length": round(avg_word_length, 2),
            "avg_paragraph_length": round(avg_paragraph_length, 2),
            "complex_word_ratio": round(complex_word_ratio, 3),
            "vocabulary_richness": round(vocabulary_richness, 3),
        }

    async def _llm_analysis_full(self, content: str) -> Dict[str, Any]:
        """LLM –∞–Ω–∞–ª–∏–∑ –ø–æ–ª–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞"""
        # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –¥–ª–∏–Ω—É
        analysis_text = content[:3000]

        user_prompt = self.prompt_template.format_user_prompt(text=analysis_text)

        if self.llm_service:
            response = await self._call_llm(user_prompt)
        else:
            response = self._get_mock_style_response()

        return await self._parse_style_response(response)

    async def _llm_analysis_chunked(
        self,
        content: str,
        strategy: ChunkingStrategy
    ) -> Dict[str, Any]:
        """LLM –∞–Ω–∞–ª–∏–∑ –ø–æ —á–∞–Ω–∫–∞–º"""
        chunks = await self.chunking_service.chunk_text(content, strategy)

        chunk_results = []
        for chunk in chunks[:3]:  # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –ø–µ—Ä–≤—ã–µ 3 —á–∞–Ω–∫–∞
            response = await self._llm_analysis_full(chunk.content)
            chunk_results.append({
                "chunk_index": chunk.chunk_index,
                "analysis": response
            })

        # –ê–≥—Ä–µ–≥–∏—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
        # TODO: –ë–æ–ª–µ–µ —Å–ª–æ–∂–Ω–∞—è –∞–≥—Ä–µ–≥–∞—Ü–∏—è
        return {
            "chunk_count": len(chunks),
            "analyzed_chunks": len(chunk_results),
            "chunk_results": chunk_results,
        }

    async def _call_llm(self, user_prompt: str) -> str:
        """–í—ã–∑–æ–≤ LLM"""
        return self._get_mock_style_response()

    def _get_mock_style_response(self) -> str:
        """–û–±–æ–≥–∞—â–µ–Ω–Ω—ã–π –º–æ–∫-–æ—Ç–≤–µ—Ç –¥–ª—è —Å—Ç–∏–ª—è —Å –¥–µ—Ç–∞–ª—å–Ω—ã–º –∞–Ω–∞–ª–∏–∑–æ–º"""

        # –û–±–æ–≥–∞—â–µ–Ω–Ω—ã–µ —Å—Ç–∏–ª–∏—Å—Ç–∏—á–µ—Å–∫–∏–µ —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∏
        style_characteristics = {
            "lexical_complexity": "—Å—Ä–µ–¥–Ω—è—è",
            "sentence_length": "—Å—Ä–µ–¥–Ω—è—è",
            "imagery": "—Å—Ä–µ–¥–Ω—è—è",
            "emotionality": "–Ω–∏–∑–∫–∞—è",
            "formality": "–Ω–µ–π—Ç—Ä–∞–ª—å–Ω–∞—è",

            # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ —Å—Ç–∏–ª–∏—Å—Ç–∏—á–µ—Å–∫–∏–µ –∞—Å–ø–µ–∫—Ç—ã
            "narrative_perspective": "—Ç—Ä–µ—Ç—å–µ –ª–∏—Ü–æ",  # –ø–µ—Ä–≤–æ–µ/–≤—Ç–æ—Ä–æ–µ/—Ç—Ä–µ—Ç—å–µ/—Å–º–µ—à–∞–Ω–Ω–æ–µ
            "temporal_structure": "–ª–∏–Ω–µ–π–Ω–∞—è",  # –ª–∏–Ω–µ–π–Ω–∞—è/–Ω–µ–ª–∏–Ω–µ–π–Ω–∞—è/—Ü–∏–∫–ª–∏—á–µ—Å–∫–∞—è
            "dialogue_density": "—É–º–µ—Ä–µ–Ω–Ω–∞—è",  # –Ω–∏–∑–∫–∞—è/—É–º–µ—Ä–µ–Ω–Ω–∞—è/–≤—ã—Å–æ–∫–∞—è
            "descriptive_detail": "—Å—Ä–µ–¥–Ω—è—è",  # –º–∏–Ω–∏–º–∞–ª—å–Ω–∞—è/—Å—Ä–µ–¥–Ω—è—è/–±–æ–≥–∞—Ç–∞—è
            "metaphorical_language": "—É–º–µ—Ä–µ–Ω–Ω–∞—è",  # —Ä–µ–¥–∫–∞—è/—É–º–µ—Ä–µ–Ω–Ω–∞—è/–æ–±–∏–ª—å–Ω–∞—è
            "syntax_variety": "—Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–Ω–∞—è",  # –ø—Ä–æ—Å—Ç–∞—è/—Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–Ω–∞—è/—Å–ª–æ–∂–Ω–∞—è
            "rhythm_pattern": "–ø–µ—Ä–µ–º–µ–Ω–Ω—ã–π",  # –º–æ–Ω–æ—Ç–æ–Ω–Ω—ã–π/–ø–µ—Ä–µ–º–µ–Ω–Ω—ã–π/–¥–∏–Ω–∞–º–∏—á–Ω—ã–π
            "tone_consistency": "—Å—Ç–∞–±–∏–ª—å–Ω–∞—è",  # —Å—Ç–∞–±–∏–ª—å–Ω–∞—è/–ø–µ—Ä–µ–º–µ–Ω–Ω–∞—è/–∫–æ–Ω—Ç—Ä–∞—Å—Ç–Ω–∞—è

            # –ñ–∞–Ω—Ä–æ–≤—ã–µ –æ—Å–æ–±–µ–Ω–Ω–æ—Å—Ç–∏
            "literary_devices": [
                "—ç–ø–∏—Ç–µ—Ç—ã", "—Å—Ä–∞–≤–Ω–µ–Ω–∏—è", "–º–µ—Ç–∞—Ñ–æ—Ä—ã", "–æ–ª–∏—Ü–µ—Ç–≤–æ—Ä–µ–Ω–∏–µ", "–≥–∏–ø–µ—Ä–±–æ–ª–∞"
            ],
            "style_markers": [
                "–∞–≤—Ç–æ—Ä—Å–∫–∏–µ –æ—Ç—Å—Ç—É–ø–ª–µ–Ω–∏—è", "–≤–Ω—É—Ç—Ä–µ–Ω–Ω–∏–µ –º–æ–Ω–æ–ª–æ–≥–∏", "–æ–ø–∏—Å–∞–Ω–∏—è –ø—Ä–∏—Ä–æ–¥—ã"
            ],

            # –•–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∏ —è–∑—ã–∫–∞
            "register_level": "–Ω–µ–π—Ç—Ä–∞–ª—å–Ω—ã–π",  # —Ä–∞–∑–≥–æ–≤–æ—Ä–Ω—ã–π/–Ω–µ–π—Ç—Ä–∞–ª—å–Ω—ã–π/–∫–Ω–∏–∂–Ω—ã–π
            "archaic_elements": "–æ—Ç—Å—É—Ç—Å—Ç–≤—É—é—Ç",  # –æ—Ç—Å—É—Ç—Å—Ç–≤—É—é—Ç/—Ä–µ–¥–∫–∏–µ/–∑–∞–º–µ—Ç–Ω—ã–µ
            "colloquial_expressions": "—É–º–µ—Ä–µ–Ω–Ω—ã–µ",  # —Ä–µ–¥–∫–∏–µ/—É–º–µ—Ä–µ–Ω–Ω—ã–µ/—á–∞—Å—Ç—ã–µ
            "technical_vocabulary": "–º–∏–Ω–∏–º–∞–ª—å–Ω–∞—è",  # –º–∏–Ω–∏–º–∞–ª—å–Ω–∞—è/—É–º–µ—Ä–µ–Ω–Ω–∞—è/—Å–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–∞—è

            # –ö–æ–º–ø–æ–∑–∏—Ü–∏–æ–Ω–Ω—ã–µ –æ—Å–æ–±–µ–Ω–Ω–æ—Å—Ç–∏
            "paragraph_structure": "—Ç—Ä–∞–¥–∏—Ü–∏–æ–Ω–Ω–∞—è",  # —Ç—Ä–∞–¥–∏—Ü–∏–æ–Ω–Ω–∞—è/—ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞–ª—å–Ω–∞—è
            "chapter_organization": "—Ö—Ä–æ–Ω–æ–ª–æ–≥–∏—á–µ—Å–∫–∞—è",  # —Ö—Ä–æ–Ω–æ–ª–æ–≥–∏—á–µ—Å–∫–∞—è/—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∞—è/–∞—Å—Å–æ—Ü–∏–∞—Ç–∏–≤–Ω–∞—è
            "pacing_variation": "—Ä–∞–≤–Ω–æ–º–µ—Ä–Ω–∞—è"  # —Ä–∞–≤–Ω–æ–º–µ—Ä–Ω–∞—è/–∫–æ–Ω—Ç—Ä–∞—Å—Ç–Ω–∞—è/–≥—Ä–∞–¥—É–∞–ª—å–Ω–∞—è
        }

        return json.dumps(style_characteristics, ensure_ascii=False)

    async def _parse_style_response(self, response: str) -> Dict[str, Any]:
        """–ü–∞—Ä—Å–∏–Ω–≥ –æ—Ç–≤–µ—Ç–∞ LLM"""
        try:
            return json.loads(response)
        except:
            return {"raw_response": response}

    def interpret_results(self, result: AnalysisResult) -> str:
        """–ò–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤"""
        data = result.data
        stats = data.get("statistics", {})
        llm = data.get("llm_assessment", {})

        lines = ["üìù –°—Ç–∏–ª–∏—Å—Ç–∏—á–µ—Å–∫–∏–π –∞–Ω–∞–ª–∏–∑:\n"]

        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
        lines.append("–°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞:")
        lines.append(f"- –°–ª–æ–≤: {stats.get('total_words', 0)}")
        lines.append(f"- –£–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Å–ª–æ–≤: {stats.get('unique_words', 0)}")
        lines.append(f"- –ü—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π: {stats.get('total_sentences', 0)}")
        lines.append(f"- –°—Ä–µ–¥–Ω–Ω—è—è –¥–ª–∏–Ω–∞ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è: {stats.get('avg_sentence_length', 0)} —Å–ª–æ–≤")
        lines.append(f"- –ë–æ–≥–∞—Ç—Å—Ç–≤–æ —Å–ª–æ–≤–∞—Ä—è: {stats.get('vocabulary_richness', 0):.2%}")

        # LLM –æ—Ü–µ–Ω–∫–∞
        if isinstance(llm, dict) and "lexical_complexity" in llm:
            lines.append("\n–û—Ü–µ–Ω–∫–∞ LLM:")
            lines.append(f"- –°–ª–æ–∂–Ω–æ—Å—Ç—å –ª–µ–∫—Å–∏–∫–∏: {llm.get('lexical_complexity', '–Ω/–¥')}")
            lines.append(f"- –î–ª–∏–Ω–∞ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π: {llm.get('sentence_length', '–Ω/–¥')}")
            lines.append(f"- –û–±—Ä–∞–∑–Ω–æ—Å—Ç—å: {llm.get('imagery', '–Ω/–¥')}")
            lines.append(f"- –≠–º–æ—Ü–∏–æ–Ω–∞–ª—å–Ω–æ—Å—Ç—å: {llm.get('emotionality', '–Ω/–¥')}")

        return "\n".join(lines)

    def get_estimated_time(
        self,
        text_length: int,
        mode: AnalysisMode = AnalysisMode.FULL_TEXT
    ) -> float:
        """–û—Ü–µ–Ω–∫–∞ –≤—Ä–µ–º–µ–Ω–∏"""
        return 3.0 + (text_length / 1000) * 0.3
