"""
–°–µ—Ä–≤–∏—Å –¥–ª—è –æ—Ä–∫–µ—Å—Ç—Ä–∞—Ü–∏–∏ –∞–Ω–∞–ª–∏–∑–∞ —Ç–µ–∫—Å—Ç–æ–≤
"""
from typing import List, Dict, Any, Optional
from loguru import logger
import asyncio
from datetime import datetime

from src.analysis_domain.entities import AnalysisSession, AnalysisResult, BaseAnalyzer
from src.text_domain.entities.base_text import BaseText
from src.text_domain.entities.plain_text import PlainText
from src.text_domain.services.chunking_service import ChunkingService
from src.infrastructure.database.repositories import SessionRepository, TextRepository
from src.infrastructure.queue.progress_broadcaster import ProgressBroadcaster
from src.model_management.services.llm_service import LLMService
from src.model_management.services.embedding_service import EmbeddingService
from src.common.exceptions import AnalysisError
from src.common.utils import now_utc
from sqlalchemy.ext.asyncio import AsyncSession


class AnalysisService:
    """–°–µ—Ä–≤–∏—Å –¥–ª—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è –∞–Ω–∞–ª–∏–∑–∞ —Ç–µ–∫—Å—Ç–æ–≤"""

    def __init__(
        self,
        db_session: AsyncSession,
        llm_service: LLMService,
        embedding_service: EmbeddingService,
        progress_broadcaster: Optional[ProgressBroadcaster] = None
    ):
        """
        Args:
            db_session: –°–µ—Å—Å–∏—è –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö
            llm_service: –°–µ—Ä–≤–∏—Å –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å LLM
            embedding_service: –°–µ—Ä–≤–∏—Å –¥–ª—è embeddings
            progress_broadcaster: Broadcaster –¥–ª—è WebSocket –æ–±–Ω–æ–≤–ª–µ–Ω–∏–π
        """
        self.db_session = db_session
        self.llm_service = llm_service
        self.embedding_service = embedding_service
        self.progress_broadcaster = progress_broadcaster

        self.session_repo = SessionRepository(db_session)
        self.text_repo = TextRepository(db_session)

        # –†–µ–µ—Å—Ç—Ä –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä–æ–≤
        self._analyzer_registry: Dict[str, type] = {}
        self._register_analyzers()

    def _register_analyzers(self):
        """–†–µ–≥–∏—Å—Ç—Ä–∞—Ü–∏—è –≤—Å–µ—Ö –¥–æ—Å—Ç—É–ø–Ω—ã—Ö –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä–æ–≤"""
        from src.analysis_domain.analyzers.genre_analyzer import GenreAnalyzer
        from src.analysis_domain.analyzers.style_analyzer import StyleAnalyzer
        from src.analysis_domain.analyzers.emotion_analyzer import EmotionAnalyzer
        from src.analysis_domain.analyzers.complexity_analyzer import ComplexityAnalyzer
        from src.analysis_domain.analyzers.readability_analyzer import ReadabilityAnalyzer
        from src.analysis_domain.analyzers.character_analyzer import CharacterAnalyzer
        from src.analysis_domain.analyzers.tension_analyzer import TensionAnalyzer
        from src.analysis_domain.analyzers.pace_analyzer import PaceAnalyzer
        from src.analysis_domain.analyzers.water_analyzer import WaterAnalyzer
        from src.analysis_domain.analyzers.theme_analyzer import ThemeAnalyzer
        from src.analysis_domain.analyzers.dialogue_analyzer import DialogueAnalyzer
        from src.analysis_domain.analyzers.description_analyzer import DescriptionAnalyzer
        from src.analysis_domain.analyzers.structure_analyzer import StructureAnalyzer

        self._analyzer_registry = {
            "GenreAnalyzer": GenreAnalyzer,
            "StyleAnalyzer": StyleAnalyzer,
            "EmotionAnalyzer": EmotionAnalyzer,
            "ComplexityAnalyzer": ComplexityAnalyzer,
            "ReadabilityAnalyzer": ReadabilityAnalyzer,
            "CharacterAnalyzer": CharacterAnalyzer,
            "TensionAnalyzer": TensionAnalyzer,
            "PaceAnalyzer": PaceAnalyzer,
            "WaterAnalyzer": WaterAnalyzer,
            "ThemeAnalyzer": ThemeAnalyzer,
            "DialogueAnalyzer": DialogueAnalyzer,
            "DescriptionAnalyzer": DescriptionAnalyzer,
            "StructureAnalyzer": StructureAnalyzer,
        }

    def get_available_analyzers(self) -> List[str]:
        """–ü–æ–ª—É—á–∏—Ç—å —Å–ø–∏—Å–æ–∫ –¥–æ—Å—Ç—É–ø–Ω—ã—Ö –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä–æ–≤"""
        return list(self._analyzer_registry.keys())

    async def run_session(self, session_id: str) -> None:
        """
        –ó–∞–ø—É—Å—Ç–∏—Ç—å —Å–µ—Å—Å–∏—é –∞–Ω–∞–ª–∏–∑–∞

        Args:
            session_id: ID —Å–µ—Å—Å–∏–∏
        """
        try:
            logger.info(f"üöÄ –ù–∞—á–∞–ª–æ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è —Å–µ—Å—Å–∏–∏ {session_id}")

            # –û–±–Ω–æ–≤–ª—è–µ–º —Å—Ç–∞—Ç—É—Å
            await self.session_repo.update_status(
                session_id, "running", progress=0, progress_message="–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è..."
            )
            await self._broadcast_progress(session_id, 0, "–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è...")

            # –ü–æ–ª—É—á–∞–µ–º —Å–µ—Å—Å–∏—é
            session_model = await self.session_repo.get_by_id(session_id, load_relations=True)
            if not session_model:
                raise AnalysisError(f"Session {session_id} not found")

            # –ü–æ–ª—É—á–∞–µ–º —Ç–µ–∫—Å—Ç—ã –∏ –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä—ã
            text_ids = await self.session_repo.get_text_ids(session_id)
            analyzer_names = await self.session_repo.get_analyzer_names(session_id)

            if not text_ids:
                raise AnalysisError("No texts in session")
            if not analyzer_names:
                raise AnalysisError("No analyzers in session")

            logger.info(f"–¢–µ–∫—Å—Ç—ã: {len(text_ids)}, –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä—ã: {len(analyzer_names)}")

            # –û–±—â–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∑–∞–¥–∞—á
            total_tasks = len(text_ids) * len(analyzer_names)
            completed_tasks = 0

            # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –∫–∞–∂–¥—ã–π —Ç–µ–∫—Å—Ç
            for text_idx, text_id in enumerate(text_ids):
                logger.info(f"üìñ –û–±—Ä–∞–±–æ—Ç–∫–∞ —Ç–µ–∫—Å—Ç–∞ {text_idx + 1}/{len(text_ids)}: {text_id}")

                # –ü–æ–ª—É—á–∞–µ–º —Ç–µ–∫—Å—Ç
                content = await self.text_repo.get_content(text_id)
                if not content:
                    logger.warning(f"–¢–µ–∫—Å—Ç {text_id} –Ω–µ –Ω–∞–π–¥–µ–Ω, –ø—Ä–æ–ø—É—Å–∫–∞–µ–º")
                    completed_tasks += len(analyzer_names)
                    continue

                text = PlainText(
                    id=text_id,
                    title=f"Text {text_id}",
                    content=content,
                    storage_type="database"
                )

                # –ß–∞–Ω–∫—É–µ–º –µ—Å–ª–∏ –Ω—É–∂–Ω–æ
                chunks = None
                if session_model.mode == "chunked":
                    await self._broadcast_progress(
                        session_id,
                        int((completed_tasks / total_tasks) * 100),
                        f"–ß–∞–Ω–∫–∏–Ω–≥ —Ç–µ–∫—Å—Ç–∞ {text_idx + 1}/{len(text_ids)}..."
                    )
                    chunks = await self._chunk_text(text)

                # –ó–∞–ø—É—Å–∫–∞–µ–º –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä—ã
                for analyzer_idx, analyzer_name in enumerate(analyzer_names):
                    try:
                        progress_msg = f"–ê–Ω–∞–ª–∏–∑ {analyzer_name} –¥–ª—è —Ç–µ–∫—Å—Ç–∞ {text_idx + 1}/{len(text_ids)}"
                        logger.info(f"üîç {progress_msg}")

                        await self._broadcast_progress(
                            session_id,
                            int((completed_tasks / total_tasks) * 100),
                            progress_msg
                        )

                        # –°–æ–∑–¥–∞–µ–º –∏ –∑–∞–ø—É—Å–∫–∞–µ–º –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä
                        start_time = now_utc()
                        result = await self._run_analyzer(
                            analyzer_name, text, chunks, session_model.mode
                        )
                        execution_time = (now_utc() - start_time).total_seconds() * 1000

                        # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç
                        await self.session_repo.save_result(
                            session_id=session_id,
                            text_id=text_id,
                            analyzer_name=analyzer_name,
                            result_data=result.data,
                            interpretation=self._get_interpretation(analyzer_name, result),
                            execution_time_ms=execution_time
                        )

                        completed_tasks += 1

                    except Exception as e:
                        logger.error(f"–û—à–∏–±–∫–∞ –≤ –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä–µ {analyzer_name}: {e}")
                        completed_tasks += 1
                        # –ü—Ä–æ–¥–æ–ª–∂–∞–µ–º —Å –¥—Ä—É–≥–∏–º–∏ –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä–∞–º–∏

            # –ó–∞–≤–µ—Ä—à–µ–Ω–æ —É—Å–ø–µ—à–Ω–æ
            await self.session_repo.update_status(
                session_id, "completed", progress=100, progress_message="–ê–Ω–∞–ª–∏–∑ –∑–∞–≤–µ—Ä—à—ë–Ω"
            )
            await self._broadcast_progress(session_id, 100, "–ê–Ω–∞–ª–∏–∑ –∑–∞–≤–µ—Ä—à—ë–Ω ‚úÖ")

            logger.info(f"‚úÖ –°–µ—Å—Å–∏—è {session_id} –∑–∞–≤–µ—Ä—à–µ–Ω–∞ —É—Å–ø–µ—à–Ω–æ")

        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è —Å–µ—Å—Å–∏–∏ {session_id}: {e}")
            await self.session_repo.update_status(
                session_id, "failed", error=str(e)
            )
            await self._broadcast_progress(session_id, 0, f"–û—à–∏–±–∫–∞: {str(e)}")
            raise

    async def _chunk_text(self, text: BaseText) -> List[Any]:
        """–†–∞–∑–±–∏—Ç—å —Ç–µ–∫—Å—Ç –Ω–∞ —á–∞–Ω–∫–∏"""
        from src.text_domain.entities import ChunkingStrategy

        strategy = ChunkingStrategy(
            base_chunk_size=2000,
            min_chunk_size=500,
            max_chunk_size=4000,
            overlap_percentage=0.1,
            use_sentence_boundaries=True,
            use_paragraph_boundaries=True
        )

        chunker = ChunkingService(strategy)
        return chunker.chunk_text(text.content)

    async def _run_analyzer(
        self,
        analyzer_name: str,
        text: BaseText,
        chunks: Optional[List[Any]],
        mode: str
    ) -> dict:
        """
        –ó–∞–ø—É—Å—Ç–∏—Ç—å –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä

        Args:
            analyzer_name: –ù–∞–∑–≤–∞–Ω–∏–µ –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä–∞
            text: –¢–µ–∫—Å—Ç
            chunks: –ß–∞–Ω–∫–∏ (–µ—Å–ª–∏ —Ä–µ–∂–∏–º chunked)
            mode: –†–µ–∂–∏–º –∞–Ω–∞–ª–∏–∑–∞

        Returns:
            –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –∞–Ω–∞–ª–∏–∑–∞
        """
        analyzer_class = self._analyzer_registry.get(analyzer_name)
        if not analyzer_class:
            raise AnalysisError(f"Unknown analyzer: {analyzer_name}")

        # –°–æ–∑–¥–∞–µ–º —ç–∫–∑–µ–º–ø–ª—è—Ä –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä–∞
        # TODO: –ü–µ—Ä–µ–¥–∞—Ç—å –ø—Ä–∞–≤–∏–ª—å–Ω—ã–µ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ (–ø—Ä–æ–º–ø—Ç-—à–∞–±–ª–æ–Ω—ã –∏–∑ –ë–î)
        analyzer = analyzer_class(
            llm_service=self.llm_service,
            prompt_template="default"  # –ó–∞–≥–ª—É—à–∫–∞
        )

        # –ó–∞–ø—É—Å–∫–∞–µ–º –∞–Ω–∞–ª–∏–∑
        result = await analyzer.analyze(text, mode, chunks=chunks)

        return result

    def _get_interpretation(self, analyzer_name: str, result: AnalysisResult) -> str:
        """–ü–æ–ª—É—á–∏—Ç—å –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏—é —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞"""
        analyzer_class = self._analyzer_registry.get(analyzer_name)
        if not analyzer_class:
            return str(result)

        try:
            # –°–æ–∑–¥–∞–µ–º –≤—Ä–µ–º–µ–Ω–Ω—ã–π —ç–∫–∑–µ–º–ø–ª—è—Ä –¥–ª—è –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏–∏
            analyzer = analyzer_class(
                llm_service=self.llm_service,
                prompt_template="default"
            )
            return analyzer.interpret_results(result)
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞: {e}")
            return str(result)

    async def _broadcast_progress(
        self,
        session_id: str,
        progress: int,
        message: str
    ) -> None:
        """–û—Ç–ø—Ä–∞–≤–∏—Ç—å –æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ –ø—Ä–æ–≥—Ä–µ—Å—Å–∞ —á–µ—Ä–µ–∑ WebSocket"""
        if self.progress_broadcaster:
            try:
                await self.progress_broadcaster.broadcast_progress(
                    task_id=session_id,
                    status="running",
                    progress=progress,
                    current_step=message
                )
            except Exception as e:
                logger.error(f"–û—à–∏–±–∫–∞ broadcast: {e}")

    async def cancel_session(self, session_id: str) -> bool:
        """
        –û—Ç–º–µ–Ω–∏—Ç—å –≤—ã–ø–æ–ª–Ω–µ–Ω–∏–µ —Å–µ—Å—Å–∏–∏

        Args:
            session_id: ID —Å–µ—Å—Å–∏–∏

        Returns:
            True –µ—Å–ª–∏ –æ—Ç–º–µ–Ω–µ–Ω–∞
        """
        try:
            await self.session_repo.update_status(
                session_id, "cancelled", progress_message="–û—Ç–º–µ–Ω–µ–Ω–æ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–º"
            )
            await self._broadcast_progress(session_id, 0, "–û—Ç–º–µ–Ω–µ–Ω–æ ‚ùå")
            return True
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –æ—Ç–º–µ–Ω—ã —Å–µ—Å—Å–∏–∏: {e}")
            return False
