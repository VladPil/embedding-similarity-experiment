"""
Ð—Ð°Ð³Ñ€ÑƒÐ·Ñ‡Ð¸Ðº Ð¼Ð¾Ð´ÐµÐ»ÐµÐ¹ Ð² Ð¿Ð°Ð¼ÑÑ‚ÑŒ
"""
from typing import Optional, Tuple, Any
import torch
from loguru import logger
from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig
from sentence_transformers import SentenceTransformer

from ..entities.model_config import ModelConfig
from ..entities.model_instance import ModelInstance
from ..scheduler.model_pool import ModelPool
from src.common.exceptions import ModelError, GPUMemoryError
from src.common.types import ModelType, QuantizationType, DeviceType


class ModelLoader:
    """
    Ð—Ð°Ð³Ñ€ÑƒÐ·Ñ‡Ð¸Ðº Ð¼Ð¾Ð´ÐµÐ»ÐµÐ¹ Ð² Ð¿Ð°Ð¼ÑÑ‚ÑŒ Ñ Ð¿Ð¾Ð´Ð´ÐµÑ€Ð¶ÐºÐ¾Ð¹ ÐºÐ²Ð°Ð½Ñ‚Ð¸Ð·Ð°Ñ†Ð¸Ð¸

    Ð—Ð°Ð³Ñ€ÑƒÐ¶Ð°ÐµÑ‚ LLM Ð¸ Embedding Ð¼Ð¾Ð´ÐµÐ»Ð¸ Ð½Ð° GPU/CPU
    """

    def __init__(self, model_pool: ModelPool):
        """
        Ð˜Ð½Ð¸Ñ†Ð¸Ð°Ð»Ð¸Ð·Ð°Ñ†Ð¸Ñ Ð·Ð°Ð³Ñ€ÑƒÐ·Ñ‡Ð¸ÐºÐ°

        Args:
            model_pool: ÐŸÑƒÐ» Ð¼Ð¾Ð´ÐµÐ»ÐµÐ¹
        """
        self.model_pool = model_pool

    async def load_model(self, config: ModelConfig) -> ModelInstance:
        """
        Ð—Ð°Ð³Ñ€ÑƒÐ·Ð¸Ñ‚ÑŒ Ð¼Ð¾Ð´ÐµÐ»ÑŒ Ð² Ð¿Ð°Ð¼ÑÑ‚ÑŒ

        Args:
            config: ÐšÐ¾Ð½Ñ„Ð¸Ð³ÑƒÑ€Ð°Ñ†Ð¸Ñ Ð¼Ð¾Ð´ÐµÐ»Ð¸

        Returns:
            ModelInstance: Ð—Ð°Ð³Ñ€ÑƒÐ¶ÐµÐ½Ð½Ñ‹Ð¹ ÑÐºÐ·ÐµÐ¼Ð¿Ð»ÑÑ€ Ð¼Ð¾Ð´ÐµÐ»Ð¸

        Raises:
            ModelError: Ð•ÑÐ»Ð¸ Ð·Ð°Ð³Ñ€ÑƒÐ·ÐºÐ° Ð½Ðµ ÑƒÐ´Ð°Ð»Ð°ÑÑŒ
            GPUMemoryError: Ð•ÑÐ»Ð¸ Ð½ÐµÐ´Ð¾ÑÑ‚Ð°Ñ‚Ð¾Ñ‡Ð½Ð¾ Ð¿Ð°Ð¼ÑÑ‚Ð¸
        """
        logger.info(f"ðŸ”„ Ð—Ð°Ð³Ñ€ÑƒÐ·ÐºÐ° Ð¼Ð¾Ð´ÐµÐ»Ð¸: {config.model_name} ({config.model_type.value})")

        try:
            if config.is_llm():
                model, tokenizer = await self._load_llm(config)
            elif config.is_embedding():
                model, tokenizer = await self._load_embedding(config), None
            else:
                raise ModelError(
                    message=f"ÐÐµÐ¿Ð¾Ð´Ð´ÐµÑ€Ð¶Ð¸Ð²Ð°ÐµÐ¼Ñ‹Ð¹ Ñ‚Ð¸Ð¿ Ð¼Ð¾Ð´ÐµÐ»Ð¸: {config.model_type}",
                    details={"model_type": config.model_type.value}
                )

            # Ð”Ð¾Ð±Ð°Ð²Ð»ÑÐµÐ¼ Ð² Ð¿ÑƒÐ»
            instance = await self.model_pool.load_model(config, model, tokenizer)

            logger.info(
                f"âœ… ÐœÐ¾Ð´ÐµÐ»ÑŒ Ð·Ð°Ð³Ñ€ÑƒÐ¶ÐµÐ½Ð°: {config.model_name} "
                f"(Ð¿Ð°Ð¼ÑÑ‚ÑŒ: {config.get_memory_estimate_mb():.0f}MB)"
            )

            return instance

        except GPUMemoryError:
            raise

        except Exception as e:
            logger.error(f"ÐžÑˆÐ¸Ð±ÐºÐ° Ð·Ð°Ð³Ñ€ÑƒÐ·ÐºÐ¸ Ð¼Ð¾Ð´ÐµÐ»Ð¸ {config.model_name}: {e}")
            raise ModelError(
                message=f"ÐÐµ ÑƒÐ´Ð°Ð»Ð¾ÑÑŒ Ð·Ð°Ð³Ñ€ÑƒÐ·Ð¸Ñ‚ÑŒ Ð¼Ð¾Ð´ÐµÐ»ÑŒ {config.model_name}",
                details={"error": str(e), "config_id": config.id}
            )

    async def _load_llm(self, config: ModelConfig) -> Tuple[Any, Any]:
        """
        Ð—Ð°Ð³Ñ€ÑƒÐ·Ð¸Ñ‚ÑŒ LLM Ð¼Ð¾Ð´ÐµÐ»ÑŒ

        Args:
            config: ÐšÐ¾Ð½Ñ„Ð¸Ð³ÑƒÑ€Ð°Ñ†Ð¸Ñ Ð¼Ð¾Ð´ÐµÐ»Ð¸

        Returns:
            Tuple[Any, Any]: (model, tokenizer)
        """
        # ÐžÐ¿Ñ€ÐµÐ´ÐµÐ»ÑÐµÐ¼ ÑƒÑÑ‚Ñ€Ð¾Ð¹ÑÑ‚Ð²Ð¾
        device = self._get_device(config)

        # ÐÐ°ÑÑ‚Ñ€Ð¾Ð¹ÐºÐ° ÐºÐ²Ð°Ð½Ñ‚Ð¸Ð·Ð°Ñ†Ð¸Ð¸
        quantization_config = self._get_quantization_config(config)

        # ÐŸÐ°Ñ€Ð°Ð¼ÐµÑ‚Ñ€Ñ‹ Ð·Ð°Ð³Ñ€ÑƒÐ·ÐºÐ¸
        load_kwargs = {
            "pretrained_model_name_or_path": config.model_path or config.model_name,
            "torch_dtype": torch.float16 if device == "cuda" else torch.float32,
            "device_map": "auto" if device == "cuda" else None,
            "trust_remote_code": True,
        }

        if quantization_config:
            load_kwargs["quantization_config"] = quantization_config

        # Ð—Ð°Ð³Ñ€ÑƒÐ¶Ð°ÐµÐ¼ Ð¼Ð¾Ð´ÐµÐ»ÑŒ
        logger.info(f"Ð—Ð°Ð³Ñ€ÑƒÐ·ÐºÐ° LLM Ð½Ð° {device}...")
        model = AutoModelForCausalLM.from_pretrained(**load_kwargs)

        # Ð—Ð°Ð³Ñ€ÑƒÐ¶Ð°ÐµÐ¼ Ñ‚Ð¾ÐºÐµÐ½Ð¸Ð·Ð°Ñ‚Ð¾Ñ€
        tokenizer = AutoTokenizer.from_pretrained(
            config.model_path or config.model_name,
            trust_remote_code=True
        )

        # ÐÐ°ÑÑ‚Ñ€Ð°Ð¸Ð²Ð°ÐµÐ¼ Ñ‚Ð¾ÐºÐµÐ½Ð¸Ð·Ð°Ñ‚Ð¾Ñ€
        if tokenizer.pad_token is None:
            tokenizer.pad_token = tokenizer.eos_token

        return model, tokenizer

    async def _load_embedding(self, config: ModelConfig) -> Any:
        """
        Ð—Ð°Ð³Ñ€ÑƒÐ·Ð¸Ñ‚ÑŒ Embedding Ð¼Ð¾Ð´ÐµÐ»ÑŒ

        Args:
            config: ÐšÐ¾Ð½Ñ„Ð¸Ð³ÑƒÑ€Ð°Ñ†Ð¸Ñ Ð¼Ð¾Ð´ÐµÐ»Ð¸

        Returns:
            Any: ÐœÐ¾Ð´ÐµÐ»ÑŒ SentenceTransformer
        """
        # ÐžÐ¿Ñ€ÐµÐ´ÐµÐ»ÑÐµÐ¼ ÑƒÑÑ‚Ñ€Ð¾Ð¹ÑÑ‚Ð²Ð¾
        device = self._get_device(config)

        logger.info(f"Ð—Ð°Ð³Ñ€ÑƒÐ·ÐºÐ° Embedding Ð¼Ð¾Ð´ÐµÐ»Ð¸ Ð½Ð° {device}...")

        # Ð—Ð°Ð³Ñ€ÑƒÐ¶Ð°ÐµÐ¼ Ñ‡ÐµÑ€ÐµÐ· SentenceTransformer
        model = SentenceTransformer(
            model_name_or_path=config.model_path or config.model_name,
            device=device,
            trust_remote_code=True
        )

        # ÐžÐ±Ð½Ð¾Ð²Ð»ÑÐµÐ¼ Ñ€Ð°Ð·Ð¼ÐµÑ€Ð½Ð¾ÑÑ‚ÑŒ Ð² ÐºÐ¾Ð½Ñ„Ð¸Ð³Ðµ ÐµÑÐ»Ð¸ Ð½Ðµ Ð·Ð°Ð´Ð°Ð½Ð°
        if config.dimensions is None:
            config.dimensions = model.get_sentence_embedding_dimension()

        return model

    def _get_device(self, config: ModelConfig) -> str:
        """
        ÐŸÐ¾Ð»ÑƒÑ‡Ð¸Ñ‚ÑŒ ÑƒÑÑ‚Ñ€Ð¾Ð¹ÑÑ‚Ð²Ð¾ Ð´Ð»Ñ Ð·Ð°Ð³Ñ€ÑƒÐ·ÐºÐ¸

        Args:
            config: ÐšÐ¾Ð½Ñ„Ð¸Ð³ÑƒÑ€Ð°Ñ†Ð¸Ñ Ð¼Ð¾Ð´ÐµÐ»Ð¸

        Returns:
            str: Ð£ÑÑ‚Ñ€Ð¾Ð¹ÑÑ‚Ð²Ð¾ ("cuda:0", "cpu")
        """
        if config.device == DeviceType.CUDA:
            if torch.cuda.is_available():
                return f"cuda:{config.device_id}"
            else:
                logger.warning("CUDA Ð½ÐµÐ´Ð¾ÑÑ‚ÑƒÐ¿Ð½Ð°, Ð¸ÑÐ¿Ð¾Ð»ÑŒÐ·ÑƒÐµÑ‚ÑÑ CPU")
                return "cpu"
        else:
            return "cpu"

    def _get_quantization_config(
        self,
        config: ModelConfig
    ) -> Optional[BitsAndBytesConfig]:
        """
        ÐŸÐ¾Ð»ÑƒÑ‡Ð¸Ñ‚ÑŒ ÐºÐ¾Ð½Ñ„Ð¸Ð³ÑƒÑ€Ð°Ñ†Ð¸ÑŽ ÐºÐ²Ð°Ð½Ñ‚Ð¸Ð·Ð°Ñ†Ð¸Ð¸

        Args:
            config: ÐšÐ¾Ð½Ñ„Ð¸Ð³ÑƒÑ€Ð°Ñ†Ð¸Ñ Ð¼Ð¾Ð´ÐµÐ»Ð¸

        Returns:
            Optional[BitsAndBytesConfig]: ÐšÐ¾Ð½Ñ„Ð¸Ð³ÑƒÑ€Ð°Ñ†Ð¸Ñ Ð¸Ð»Ð¸ None
        """
        if config.quantization == QuantizationType.NONE:
            return None

        if config.quantization == QuantizationType.INT8:
            return BitsAndBytesConfig(
                load_in_8bit=True,
                llm_int8_threshold=6.0,
            )

        elif config.quantization == QuantizationType.INT4:
            return BitsAndBytesConfig(
                load_in_4bit=True,
                bnb_4bit_compute_dtype=torch.float16,
                bnb_4bit_quant_type="nf4",
                bnb_4bit_use_double_quant=True,
            )

        else:
            logger.warning(f"ÐÐµÐ¿Ð¾Ð´Ð´ÐµÑ€Ð¶Ð¸Ð²Ð°ÐµÐ¼Ð°Ñ ÐºÐ²Ð°Ð½Ñ‚Ð¸Ð·Ð°Ñ†Ð¸Ñ: {config.quantization}")
            return None

    async def unload_model(self, config_id: str) -> bool:
        """
        Ð’Ñ‹Ð³Ñ€ÑƒÐ·Ð¸Ñ‚ÑŒ Ð¼Ð¾Ð´ÐµÐ»ÑŒ Ð¸Ð· Ð¿Ð°Ð¼ÑÑ‚Ð¸

        Args:
            config_id: ID ÐºÐ¾Ð½Ñ„Ð¸Ð³ÑƒÑ€Ð°Ñ†Ð¸Ð¸ Ð¼Ð¾Ð´ÐµÐ»Ð¸

        Returns:
            bool: True ÐµÑÐ»Ð¸ ÑƒÑÐ¿ÐµÑˆÐ½Ð¾ Ð²Ñ‹Ð³Ñ€ÑƒÐ¶ÐµÐ½Ð°
        """
        success = await self.model_pool.unload_model(config_id)

        if success:
            # ÐžÑ‡Ð¸Ñ‰Ð°ÐµÐ¼ CUDA ÐºÑÑˆ
            if torch.cuda.is_available():
                torch.cuda.empty_cache()

        return success

    async def reload_model(self, config: ModelConfig) -> ModelInstance:
        """
        ÐŸÐµÑ€ÐµÐ·Ð°Ð³Ñ€ÑƒÐ·Ð¸Ñ‚ÑŒ Ð¼Ð¾Ð´ÐµÐ»ÑŒ (Ð²Ñ‹Ð³Ñ€ÑƒÐ·Ð¸Ñ‚ÑŒ Ð¸ Ð·Ð°Ð³Ñ€ÑƒÐ·Ð¸Ñ‚ÑŒ Ð·Ð°Ð½Ð¾Ð²Ð¾)

        Args:
            config: ÐšÐ¾Ð½Ñ„Ð¸Ð³ÑƒÑ€Ð°Ñ†Ð¸Ñ Ð¼Ð¾Ð´ÐµÐ»Ð¸

        Returns:
            ModelInstance: ÐÐ¾Ð²Ñ‹Ð¹ ÑÐºÐ·ÐµÐ¼Ð¿Ð»ÑÑ€ Ð¼Ð¾Ð´ÐµÐ»Ð¸
        """
        logger.info(f"ðŸ”„ ÐŸÐµÑ€ÐµÐ·Ð°Ð³Ñ€ÑƒÐ·ÐºÐ° Ð¼Ð¾Ð´ÐµÐ»Ð¸: {config.model_name}")

        # Ð’Ñ‹Ð³Ñ€ÑƒÐ¶Ð°ÐµÐ¼ ÐµÑÐ»Ð¸ Ð·Ð°Ð³Ñ€ÑƒÐ¶ÐµÐ½Ð°
        await self.unload_model(config.id)

        # Ð—Ð°Ð³Ñ€ÑƒÐ¶Ð°ÐµÐ¼ Ð·Ð°Ð½Ð¾Ð²Ð¾
        return await self.load_model(config)

    def get_memory_stats(self) -> dict:
        """
        ÐŸÐ¾Ð»ÑƒÑ‡Ð¸Ñ‚ÑŒ ÑÑ‚Ð°Ñ‚Ð¸ÑÑ‚Ð¸ÐºÑƒ Ð¿Ð°Ð¼ÑÑ‚Ð¸

        Returns:
            dict: Ð¡Ñ‚Ð°Ñ‚Ð¸ÑÑ‚Ð¸ÐºÐ° Ð¿Ð¾ GPU/CPU Ð¿Ð°Ð¼ÑÑ‚Ð¸
        """
        stats = {}

        # GPU Ð¿Ð°Ð¼ÑÑ‚ÑŒ
        if torch.cuda.is_available():
            for i in range(torch.cuda.device_count()):
                allocated = torch.cuda.memory_allocated(i) / (1024 ** 3)
                reserved = torch.cuda.memory_reserved(i) / (1024 ** 3)
                total = torch.cuda.get_device_properties(i).total_memory / (1024 ** 3)

                stats[f"gpu_{i}"] = {
                    "allocated_gb": allocated,
                    "reserved_gb": reserved,
                    "total_gb": total,
                    "free_gb": total - allocated,
                    "usage_percent": (allocated / total) * 100
                }

        # CPU Ð¿Ð°Ð¼ÑÑ‚ÑŒ (Ñ‡ÐµÑ€ÐµÐ· psutil ÐµÑÐ»Ð¸ Ð´Ð¾ÑÑ‚ÑƒÐ¿ÐµÐ½)
        try:
            import psutil
            mem = psutil.virtual_memory()
            stats["cpu"] = {
                "used_gb": mem.used / (1024 ** 3),
                "available_gb": mem.available / (1024 ** 3),
                "total_gb": mem.total / (1024 ** 3),
                "usage_percent": mem.percent
            }
        except ImportError:
            pass

        return stats

    def __str__(self) -> str:
        return f"ModelLoader(pool={self.model_pool})"
