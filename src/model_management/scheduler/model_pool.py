"""
–ü—É–ª –º–æ–¥–µ–ª–µ–π –¥–ª—è —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è –∑–∞–≥—Ä—É–∂–µ–Ω–Ω—ã–º–∏ –º–æ–¥–µ–ª—è–º–∏
"""
import asyncio
from typing import Dict, Optional, List
from loguru import logger

from ..entities.model_config import ModelConfig
from ..entities.model_instance import ModelInstance
from src.common.exceptions import ModelError, GPUMemoryError


class ModelPool:
    """
    –ü—É–ª –∑–∞–≥—Ä—É–∂–µ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π —Å –±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∫–æ–π –Ω–∞–≥—Ä—É–∑–∫–∏

    –£–ø—Ä–∞–≤–ª—è–µ—Ç –∑–∞–≥—Ä—É–∑–∫–æ–π, –≤—ã–≥—Ä—É–∑–∫–æ–π –∏ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ–º –∑–∞–¥–∞—á –º–µ–∂–¥—É –º–æ–¥–µ–ª—è–º–∏
    """

    def __init__(
        self,
        max_concurrent_llm: int = 2,
        max_concurrent_embedding: int = 4,
        max_gpu_memory_gb: float = 20.0
    ):
        """
        –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –ø—É–ª–∞

        Args:
            max_concurrent_llm: –ú–∞–∫—Å–∏–º—É–º –æ–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω—ã—Ö LLM –∑–∞–¥–∞—á
            max_concurrent_embedding: –ú–∞–∫—Å–∏–º—É–º –æ–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω—ã—Ö embedding –∑–∞–¥–∞—á
            max_gpu_memory_gb: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –ø–∞–º—è—Ç—å GPU
        """
        self.max_concurrent_llm = max_concurrent_llm
        self.max_concurrent_embedding = max_concurrent_embedding
        self.max_gpu_memory_gb = max_gpu_memory_gb

        # –°–ª–æ–≤–∞—Ä—å –∑–∞–≥—Ä—É–∂–µ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π: config_id -> ModelInstance
        self.instances: Dict[str, ModelInstance] = {}

        # –û—á–µ—Ä–µ–¥–∏ –∑–∞–¥–∞—á
        self.llm_queue: asyncio.Queue = asyncio.Queue()
        self.embedding_queue: asyncio.Queue = asyncio.Queue()

        # Locks –¥–ª—è —Å–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∞—Ü–∏–∏
        self._lock = asyncio.Lock()

    async def load_model(
        self,
        config: ModelConfig,
        model: any,
        tokenizer: Optional[any] = None
    ) -> ModelInstance:
        """
        –ó–∞–≥—Ä—É–∑–∏—Ç—å –º–æ–¥–µ–ª—å –≤ –ø—É–ª

        Args:
            config: –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –º–æ–¥–µ–ª–∏
            model: –û–±—ä–µ–∫—Ç –º–æ–¥–µ–ª–∏
            tokenizer: –¢–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)

        Returns:
            ModelInstance: –≠–∫–∑–µ–º–ø–ª—è—Ä –º–æ–¥–µ–ª–∏

        Raises:
            GPUMemoryError: –ï—Å–ª–∏ –Ω–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –ø–∞–º—è—Ç–∏
        """
        async with self._lock:
            # –ü—Ä–æ–≤–µ—Ä–∫–∞ –ø–∞–º—è—Ç–∏
            current_memory = self.get_total_memory_usage()
            required_memory = config.get_memory_estimate_mb() / 1024  # –í GB

            if current_memory + required_memory > self.max_gpu_memory_gb:
                # –ü–æ–ø—ã—Ç–∫–∞ –æ—Å–≤–æ–±–æ–¥–∏—Ç—å –ø–∞–º—è—Ç—å
                freed = await self._free_memory(required_memory)
                if not freed:
                    raise GPUMemoryError(
                        message=f"Insufficient GPU memory: need {required_memory:.1f}GB, "
                                f"available {self.max_gpu_memory_gb - current_memory:.1f}GB",
                        details={
                            "required_gb": required_memory,
                            "available_gb": self.max_gpu_memory_gb - current_memory,
                            "model_name": config.model_name
                        }
                    )

            # –°–æ–∑–¥–∞—ë–º —ç–∫–∑–µ–º–ø–ª—è—Ä
            instance = ModelInstance(
                config=config,
                model=model,
                tokenizer=tokenizer,
            )

            # –î–æ–±–∞–≤–ª—è–µ–º –≤ –ø—É–ª
            self.instances[config.id] = instance

            logger.info(
                f"‚úÖ –ú–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞ –≤ –ø—É–ª: {config.model_name} "
                f"(–ø–∞–º—è—Ç—å: {required_memory:.1f}GB)"
            )

            return instance

    async def unload_model(self, config_id: str) -> bool:
        """
        –í—ã–≥—Ä—É–∑–∏—Ç—å –º–æ–¥–µ–ª—å –∏–∑ –ø—É–ª–∞

        Args:
            config_id: ID –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ –º–æ–¥–µ–ª–∏

        Returns:
            bool: True –µ—Å–ª–∏ –≤—ã–≥—Ä—É–∂–µ–Ω–∞
        """
        async with self._lock:
            if config_id not in self.instances:
                return False

            instance = self.instances[config_id]

            # –ü—Ä–æ–≤–µ—Ä—è–µ–º —á—Ç–æ –º–æ–¥–µ–ª—å –Ω–µ –∑–∞–Ω—è—Ç–∞
            if instance.is_busy:
                logger.warning(
                    f"–ü–æ–ø—ã—Ç–∫–∞ –≤—ã–≥—Ä—É–∑–∏—Ç—å –∑–∞–Ω—è—Ç—É—é –º–æ–¥–µ–ª—å: {instance.config.model_name}"
                )
                return False

            # –£–¥–∞–ª—è–µ–º –∏–∑ –ø—É–ª–∞
            del self.instances[config_id]

            # –û—Å–≤–æ–±–æ–∂–¥–∞–µ–º –ø–∞–º—è—Ç—å
            del instance.model
            if instance.tokenizer:
                del instance.tokenizer

            logger.info(f"üóëÔ∏è –ú–æ–¥–µ–ª—å –≤—ã–≥—Ä—É–∂–µ–Ω–∞: {instance.config.model_name}")

            return True

    async def get_instance(self, config_id: str) -> Optional[ModelInstance]:
        """
        –ü–æ–ª—É—á–∏—Ç—å —ç–∫–∑–µ–º–ø–ª—è—Ä –º–æ–¥–µ–ª–∏

        Args:
            config_id: ID –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏

        Returns:
            Optional[ModelInstance]: –≠–∫–∑–µ–º–ø–ª—è—Ä –∏–ª–∏ None
        """
        return self.instances.get(config_id)

    async def acquire_model(
        self,
        config_id: str,
        task_id: str,
        timeout: float = 30.0
    ) -> Optional[ModelInstance]:
        """
        –ó–∞—Ö–≤–∞—Ç–∏—Ç—å –º–æ–¥–µ–ª—å –¥–ª—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è –∑–∞–¥–∞—á–∏

        Args:
            config_id: ID –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ –º–æ–¥–µ–ª–∏
            task_id: ID –∑–∞–¥–∞—á–∏
            timeout: –¢–∞–π–º–∞—É—Ç –æ–∂–∏–¥–∞–Ω–∏—è –≤ —Å–µ–∫—É–Ω–¥–∞—Ö

        Returns:
            Optional[ModelInstance]: –ó–∞—Ö–≤–∞—á–µ–Ω–Ω—ã–π —ç–∫–∑–µ–º–ø–ª—è—Ä –∏–ª–∏ None
        """
        start_time = asyncio.get_event_loop().time()

        while True:
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ç–∞–π–º–∞—É—Ç
            if asyncio.get_event_loop().time() - start_time > timeout:
                logger.warning(
                    f"–¢–∞–π–º–∞—É—Ç –æ–∂–∏–¥–∞–Ω–∏—è –º–æ–¥–µ–ª–∏ {config_id} –¥–ª—è –∑–∞–¥–∞—á–∏ {task_id}"
                )
                return None

            # –ü—ã—Ç–∞–µ–º—Å—è –∑–∞—Ö–≤–∞—Ç–∏—Ç—å –º–æ–¥–µ–ª—å
            instance = self.instances.get(config_id)
            if instance and instance.acquire(task_id):
                return instance

            # –ñ–¥—ë–º –Ω–µ–º–Ω–æ–≥–æ –ø–µ—Ä–µ–¥ —Å–ª–µ–¥—É—é—â–µ–π –ø–æ–ø—ã—Ç–∫–æ–π
            await asyncio.sleep(0.1)

    async def release_model(self, config_id: str) -> None:
        """
        –û—Å–≤–æ–±–æ–¥–∏—Ç—å –º–æ–¥–µ–ª—å –ø–æ—Å–ª–µ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è –∑–∞–¥–∞—á–∏

        Args:
            config_id: ID –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ –º–æ–¥–µ–ª–∏
        """
        instance = self.instances.get(config_id)
        if instance:
            instance.release()

    def get_available_models(self, model_type: Optional[str] = None) -> List[ModelInstance]:
        """
        –ü–æ–ª—É—á–∏—Ç—å —Å–ø–∏—Å–æ–∫ –¥–æ—Å—Ç—É–ø–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π

        Args:
            model_type: –¢–∏–ø –º–æ–¥–µ–ª–∏ (llm/embedding) –∏–ª–∏ None –¥–ª—è –≤—Å–µ—Ö

        Returns:
            List[ModelInstance]: –°–ø–∏—Å–æ–∫ –¥–æ—Å—Ç—É–ø–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π
        """
        result = []
        for instance in self.instances.values():
            if not instance.is_busy:
                if model_type is None or instance.config.model_type.value == model_type:
                    result.append(instance)
        return result

    def get_total_memory_usage(self) -> float:
        """
        –ü–æ–ª—É—á–∏—Ç—å –æ–±—â–µ–µ –ø–æ—Ç—Ä–µ–±–ª–µ–Ω–∏–µ –ø–∞–º—è—Ç–∏ –≤ GB

        Returns:
            float: –ü–∞–º—è—Ç—å –≤ GB
        """
        total_mb = sum(
            instance.config.get_memory_estimate_mb()
            for instance in self.instances.values()
        )
        return total_mb / 1024

    async def _free_memory(self, required_gb: float) -> bool:
        """
        –ü–æ–ø—ã—Ç–∫–∞ –æ—Å–≤–æ–±–æ–¥–∏—Ç—å –ø–∞–º—è—Ç—å –≤—ã–≥—Ä—É–∑–∫–æ–π –º–æ–¥–µ–ª–µ–π

        Args:
            required_gb: –¢—Ä–µ–±—É–µ–º–∞—è –ø–∞–º—è—Ç—å –≤ GB

        Returns:
            bool: True –µ—Å–ª–∏ —É–¥–∞–ª–æ—Å—å –æ—Å–≤–æ–±–æ–¥–∏—Ç—å
        """
        # –°–æ—Ä—Ç–∏—Ä—É–µ–º –º–æ–¥–µ–ª–∏ –ø–æ –ø–æ—Å–ª–µ–¥–Ω–µ–º—É –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—é
        sorted_instances = sorted(
            self.instances.values(),
            key=lambda x: x.last_used_at
        )

        freed_gb = 0.0

        for instance in sorted_instances:
            if instance.is_busy:
                continue

            # –í—ã–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å
            config_id = instance.config.id
            if await self.unload_model(config_id):
                freed_gb += instance.config.get_memory_estimate_mb() / 1024
                logger.info(f"–û—Å–≤–æ–±–æ–∂–¥–µ–Ω–æ {freed_gb:.1f}GB –ø–∞–º—è—Ç–∏")

                if freed_gb >= required_gb:
                    return True

        return freed_gb >= required_gb

    def get_stats(self) -> dict:
        """
        –ü–æ–ª—É—á–∏—Ç—å —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –ø—É–ª–∞

        Returns:
            dict: –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
        """
        llm_models = [i for i in self.instances.values() if i.config.is_llm()]
        embedding_models = [i for i in self.instances.values() if i.config.is_embedding()]

        return {
            "total_models": len(self.instances),
            "llm_models": len(llm_models),
            "embedding_models": len(embedding_models),
            "busy_models": sum(1 for i in self.instances.values() if i.is_busy),
            "available_models": sum(1 for i in self.instances.values() if not i.is_busy),
            "total_memory_gb": self.get_total_memory_usage(),
            "max_memory_gb": self.max_gpu_memory_gb,
            "memory_usage_percent": (self.get_total_memory_usage() / self.max_gpu_memory_gb) * 100,
        }

    async def health_check(self) -> dict:
        """
        –ü—Ä–æ–≤–µ—Ä–∫–∞ –∑–¥–æ—Ä–æ–≤—å—è –ø—É–ª–∞

        Returns:
            dict: –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø—Ä–æ–≤–µ—Ä–∫–∏
        """
        stats = self.get_stats()

        # –ü—Ä–æ–≤–µ—Ä–∫–∏
        issues = []

        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –ø–∞–º—è—Ç–∏
        if stats["memory_usage_percent"] > 90:
            issues.append("–ö—Ä–∏—Ç–∏—á–µ—Å–∫–æ–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –ø–∞–º—è—Ç–∏ (>90%)")

        # –ü—Ä–æ–≤–µ—Ä–∫–∞ —É—Å–ø–µ—à–Ω–æ—Å—Ç–∏ –∑–∞–ø—Ä–æ—Å–æ–≤
        for instance in self.instances.values():
            success_rate = instance.get_success_rate()
            if success_rate < 0.8:
                issues.append(
                    f"–ù–∏–∑–∫–∏–π –ø—Ä–æ—Ü–µ–Ω—Ç —É—Å–ø–µ—Ö–∞ –¥–ª—è {instance.config.model_name}: {success_rate:.0%}"
                )

        return {
            "healthy": len(issues) == 0,
            "issues": issues,
            "stats": stats,
        }

    def __str__(self) -> str:
        stats = self.get_stats()
        return (
            f"ModelPool(models={stats['total_models']}, "
            f"memory={stats['total_memory_gb']:.1f}/{stats['max_memory_gb']:.1f}GB)"
        )

    def __repr__(self) -> str:
        return self.__str__()
